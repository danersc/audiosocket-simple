Estrutura do projeto:
README.md
ai_service.py
audio_utils.py
audiosocket_handler.py
code.txt
code_gpt.py
config.json
main.py
microfone_client.py
requirements.txt
speech_service.py
state_machine.py

Código fonte:

ai_service.py:
#!/usr/bin/env python3
# ai_service.py - Serviço para integração com a API de IA

import json
import logging
import httpx
import os
import time
from typing import Dict, Any, Optional

logger = logging.getLogger(__name__)


async def enviar_mensagem_para_ia(texto: str, conversation_id: str) -> Dict[str, Any]:
    """
    Envia uma mensagem para a API de IA e retorna a resposta.

    Args:
        texto: O texto a ser enviado para a API
        conversation_id: ID da conversa para manter contexto entre mensagens

    Returns:
        Dicionário com a resposta da API ou um dicionário com mensagem de erro
    """
    try:
        payload = {
            "id": conversation_id,
            "content": texto
        }

        api_url = os.getenv('AI_API_URL', 'http://localhost:8000/messages')
        if api_url.endswith('/'):
            api_url = api_url.rstrip('/')

        headers = {"Content-Type": "application/json"}

        logger.info("=" * 50)
        logger.info(f"ENVIANDO REQUISIÇÃO PARA API")
        logger.info(f"URL: {api_url}")
        logger.info(f"ID da Conversa: {conversation_id}")
        logger.info(f"Mensagem enviada: {texto}")
        logger.info(f"Payload completo enviado: {json.dumps(payload, ensure_ascii=False, indent=2)}")
        logger.info("=" * 50)

        async with httpx.AsyncClient(follow_redirects=True) as client:
            start_time = time.time()
            response = await client.post(api_url, json=payload, headers=headers, timeout=30.0)
            response_time = time.time() - start_time

            if response.status_code == 200:
                data = response.json()

                # LOG detalhado do retorno da IA:
                logger.info("=" * 50)
                logger.info(f"RESPOSTA COMPLETA RECEBIDA DA API DA IA (tempo: {response_time:.2f}s):")
                logger.info(json.dumps(data, ensure_ascii=False, indent=2))
                logger.info("=" * 50)

                return data
            else:
                logger.error("=" * 50)
                logger.error(f"ERRO NA REQUISIÇÃO PARA API (tempo: {response_time:.2f}s)")
                logger.error(f"Status: {response.status_code}")
                logger.error(f"Resposta da API: {response.text}")
                logger.error("=" * 50)

                return {
                    "content": {
                        "mensagem": "Desculpe, ocorreu um erro ao processar sua solicitação.",
                        "dados": {},
                        "valid_for_action": False
                    },
                    "timestamp": "",
                    "set_call_status": "USER_TURN"
                }

    except Exception as e:
        logger.error("=" * 50)
        logger.error(f"EXCEÇÃO AO COMUNICAR COM API DA IA")
        logger.error(f"ID da Conversa: {conversation_id}")
        logger.error(f"Mensagem que seria enviada: {texto}")
        logger.error(f"Erro: {str(e)}")
        import traceback
        logger.error(f"Traceback: {traceback.format_exc()}")
        logger.error("=" * 50)

        return {
            "content": {
                "mensagem": "Desculpe, não foi possível me conectar ao serviço neste momento.",
                "dados": {},
                "valid_for_action": False
            },
            "timestamp": "",
            "set_call_status": "USER_TURN"
        }

def extrair_mensagem_da_resposta(resposta: Dict[str, Any]) -> str:
    """
    Extrai a mensagem de texto da resposta da API.
    
    Args:
        resposta: Resposta da API
        
    Returns:
        Mensagem de texto para sintetizar
    """
    try:
        return resposta.get("content", {}).get("mensagem", "")
    except Exception as e:
        logger.error(f"Erro ao extrair mensagem da resposta: {e}")
        return "Desculpe, ocorreu um erro ao processar sua solicitação."

def obter_estado_chamada(resposta: Dict[str, Any]) -> Optional[str]:
    """
    Obtém o estado da chamada da resposta da API.

    Args:
        resposta: Resposta da API

    Returns:
        Estado da chamada (USER_TURN, WAITING, IA_TURN) ou None caso não especificado
    """
    try:
        return resposta.get("content", {}).get("set_call_status")
    except Exception as e:
        logger.error(f"Erro ao obter estado da chamada: {e}")
        return None


audiosocket_handler.py:
import asyncio
import logging
import webrtcvad
import struct
from speech_service import transcrever_audio_async, sintetizar_fala_async
from ai_service import enviar_mensagem_para_ia, extrair_mensagem_da_resposta, obter_estado_chamada
from state_machine import State
import os

KIND_SLIN = 0x10
logger = logging.getLogger(__name__)

async def receber_audio(reader, state_machine, audio_queue):
    try:
        while True:
            header = await reader.readexactly(3)
            kind = header[0]
            length = int.from_bytes(header[1:3], 'big')
            audio_chunk = await reader.readexactly(length)
            if kind == KIND_SLIN and len(audio_chunk) == 320 and state_machine.is_user_turn():
                await audio_queue.put(audio_chunk)
    except asyncio.IncompleteReadError:
        logger.info("Cliente desconectado")
    except Exception as e:
        logger.error(f"Erro ao receber áudio: {e}")

async def enviar_audio(writer, dados_audio, origem="desconhecida"):
    logger.info(f"Iniciando envio de áudio sintetizado (origem: {origem}), tamanho total: {len(dados_audio)} bytes")
    chunk_size = 320
    for i in range(0, len(dados_audio), chunk_size):
        chunk = dados_audio[i:i + chunk_size]
        if chunk:
            writer.write(struct.pack('>B H', KIND_SLIN, len(chunk)) + chunk)
            await writer.drain()
            await asyncio.sleep(0.02)
    logger.info(f"Envio de áudio concluído (origem: {origem})")

async def enviar_audio_em_loop(writer, caminho_audio):
    with open(caminho_audio, 'rb') as f:
        dados_audio = f.read()

    chunk_size = 320
    try:
        while True:
            for i in range(0, len(dados_audio), chunk_size):
                chunk = dados_audio[i:i + chunk_size]
                if chunk:
                    writer.write(struct.pack('>B H', KIND_SLIN, len(chunk)) + chunk)
                    await writer.drain()
                    await asyncio.sleep(0.02)
    except asyncio.CancelledError:
        logger.info("Áudio de espera cancelado.")

async def monitorar_waiting(state_machine, writer, caminho_audio):
    waiting_task = None
    while True:
        await state_machine.wait_for_state(State.WAITING)
        logger.info("Estado WAITING detectado, aguardando 3 segundos antes de iniciar áudio de espera.")
        try:
            await asyncio.wait_for(state_machine.wait_for_state_change(), timeout=3.0)
            logger.info("Estado WAITING terminou antes dos 3 segundos, áudio de espera não será iniciado.")
        except asyncio.TimeoutError:
            if state_machine.is_waiting():
                logger.info("Timeout atingido em WAITING, iniciando áudio de espera.")
                waiting_task = asyncio.create_task(enviar_audio_em_loop(writer, caminho_audio))

                await state_machine.wait_for_state_change()
                if waiting_task:
                    waiting_task.cancel()
                    waiting_task = None

async def processar_audio(audio_queue, vad, state_machine, writer):
    frames = []
    is_speaking = False
    silence_start = None

    while True:
        chunk = await audio_queue.get()
        is_voice = vad.is_speech(chunk, 8000)

        if is_voice:
            frames.append(chunk)
            is_speaking = True
            silence_start = None
        elif is_speaking:
            if silence_start is None:
                silence_start = asyncio.get_event_loop().time()
            elif asyncio.get_event_loop().time() - silence_start > 2.0:
                is_speaking = False
                audio_data = b''.join(frames)
                frames.clear()

                if audio_data:
                    state_machine.transition_to(State.WAITING)
                    logger.info("Áudio capturado completo, iniciando transcrição.")

                    texto = await transcrever_audio_async(audio_data)

                    if not texto:
                        logger.warning("Nenhuma transcrição obtida.")
                        state_machine.transition_to(State.USER_TURN)
                        continue

                    logger.info(f"Texto transcrito: {texto}")

                    resposta = await enviar_mensagem_para_ia(texto, state_machine.get_conversation_id())
                    mensagem = extrair_mensagem_da_resposta(resposta)
                    proximo_estado = obter_estado_chamada(resposta)

                    logger.info(f"Mensagem recebida da IA: {mensagem}")
                    logger.info(f"Estado sugerido pela API: {proximo_estado}")

                    if mensagem:
                        audio_resposta = await sintetizar_fala_async(mensagem)
                        if audio_resposta and len(audio_resposta) > 0:
                            state_machine.transition_to(State.IA_TURN)
                            logger.info("Enviando áudio sintetizado ao usuário.")
                            await enviar_audio(writer, audio_resposta, origem="IA Response")
                            await asyncio.sleep(0.5)
                        else:
                            logger.warning("Resposta de áudio vazia.")

                    if proximo_estado:
                        logger.info(f"Aplicando estado '{proximo_estado}' após envio completo do áudio.")
                        state_machine.transition_to(State[proximo_estado])
                    else:
                        state_machine.transition_to(State.USER_TURN)

async def iniciar_servidor_audiosocket(reader, writer, state_machine):
    vad = webrtcvad.Vad(2)
    audio_queue = asyncio.Queue()
    caminho_audio_espera = os.path.join('audio', 'waiting.slin')

    logger.info("Iniciando síntese de áudio para mensagem de saudação.")
    greeting_audio = await sintetizar_fala_async("Condomínio Apoena, em que posso ajudar?")
    if greeting_audio:
        state_machine.transition_to(State.IA_TURN)
        logger.info("Enviando áudio da mensagem inicial ao cliente.")
        await enviar_audio(writer, greeting_audio, origem="Greeting")
        await asyncio.sleep(0.5)

    # ✅ Alteração chave aqui: a transição para USER_TURN ocorre APENAS após envio completo do áudio
    state_machine.transition_to(State.USER_TURN)

    await asyncio.gather(
        receber_audio(reader, state_machine, audio_queue),
        processar_audio(audio_queue, vad, state_machine, writer),
        monitorar_waiting(state_machine, writer, caminho_audio_espera)
    )


requirements.txt:
azure-cognitiveservices-speech==1.41.1
httpx==0.27.2
pydub==0.25.1
python-dotenv==1.0.1
webrtcvad==2.0.10
asyncio
typing_extensions
fastapi
uvicorn[standard]
jinja2
pydantic


config.json:
{
    "greeting": {
        "message": "Condomínio Apoena, em que posso ajudar?",
        "voice": "pt-BR-AntonioNeural",
        "delay_seconds": 2
    },
    "system": {
        "default_state": "STANDBY",
        "silence_threshold_seconds": 2.0,
        "max_transaction_time_seconds": 30
    },
    "audio": {
        "sample_rate": 8000,
        "channels": 1,
        "format": "SLIN",
        "chunk_size": 320
    }
}

code.txt:
Estrutura do projeto:
README.md
ai_service.py
audio_utils.py
audiosocket_handler.py
code_gpt.py
config.json
main.py
microfone_client.py
requirements.txt
speech_service.py
state_machine.py

Código fonte:

ai_service.py:
#!/usr/bin/env python3
# ai_service.py - Serviço para integração com a API de IA

import json
import logging
import httpx
import os
import time
from typing import Dict, Any, Optional

logger = logging.getLogger(__name__)


async def enviar_mensagem_para_ia(texto: str, conversation_id: str) -> Dict[str, Any]:
    """
    Envia uma mensagem para a API de IA e retorna a resposta.

    Args:
        texto: O texto a ser enviado para a API
        conversation_id: ID da conversa para manter contexto entre mensagens

    Returns:
        Dicionário com a resposta da API ou um dicionário com mensagem de erro
    """
    try:
        payload = {
            "id": conversation_id,
            "content": texto
        }

        api_url = os.getenv('AI_API_URL', 'http://localhost:8000/messages')
        if api_url.endswith('/'):
            api_url = api_url.rstrip('/')

        headers = {"Content-Type": "application/json"}

        logger.info("=" * 50)
        logger.info(f"ENVIANDO REQUISIÇÃO PARA API")
        logger.info(f"URL: {api_url}")
        logger.info(f"ID da Conversa: {conversation_id}")
        logger.info(f"Mensagem enviada: {texto}")
        logger.info(f"Payload completo enviado: {json.dumps(payload, ensure_ascii=False, indent=2)}")
        logger.info("=" * 50)

        async with httpx.AsyncClient(follow_redirects=True) as client:
            start_time = time.time()
            response = await client.post(api_url, json=payload, headers=headers, timeout=30.0)
            response_time = time.time() - start_time

            if response.status_code == 200:
                data = response.json()

                # LOG detalhado do retorno da IA:
                logger.info("=" * 50)
                logger.info(f"RESPOSTA COMPLETA RECEBIDA DA API DA IA (tempo: {response_time:.2f}s):")
                logger.info(json.dumps(data, ensure_ascii=False, indent=2))
                logger.info("=" * 50)

                return data
            else:
                logger.error("=" * 50)
                logger.error(f"ERRO NA REQUISIÇÃO PARA API (tempo: {response_time:.2f}s)")
                logger.error(f"Status: {response.status_code}")
                logger.error(f"Resposta da API: {response.text}")
                logger.error("=" * 50)

                return {
                    "content": {
                        "mensagem": "Desculpe, ocorreu um erro ao processar sua solicitação.",
                        "dados": {},
                        "valid_for_action": False
                    },
                    "timestamp": "",
                    "set_call_status": "USER_TURN"
                }

    except Exception as e:
        logger.error("=" * 50)
        logger.error(f"EXCEÇÃO AO COMUNICAR COM API DA IA")
        logger.error(f"ID da Conversa: {conversation_id}")
        logger.error(f"Mensagem que seria enviada: {texto}")
        logger.error(f"Erro: {str(e)}")
        import traceback
        logger.error(f"Traceback: {traceback.format_exc()}")
        logger.error("=" * 50)

        return {
            "content": {
                "mensagem": "Desculpe, não foi possível me conectar ao serviço neste momento.",
                "dados": {},
                "valid_for_action": False
            },
            "timestamp": "",
            "set_call_status": "USER_TURN"
        }

def extrair_mensagem_da_resposta(resposta: Dict[str, Any]) -> str:
    """
    Extrai a mensagem de texto da resposta da API.
    
    Args:
        resposta: Resposta da API
        
    Returns:
        Mensagem de texto para sintetizar
    """
    try:
        return resposta.get("content", {}).get("mensagem", "")
    except Exception as e:
        logger.error(f"Erro ao extrair mensagem da resposta: {e}")
        return "Desculpe, ocorreu um erro ao processar sua solicitação."

def obter_estado_chamada(resposta: Dict[str, Any]) -> Optional[str]:
    """
    Obtém o estado da chamada da resposta da API.

    Args:
        resposta: Resposta da API

    Returns:
        Estado da chamada (USER_TURN, WAITING, IA_TURN) ou None caso não especificado
    """
    try:
        return resposta.get("content", {}).get("set_call_status")
    except Exception as e:
        logger.error(f"Erro ao obter estado da chamada: {e}")
        return None


audiosocket_handler.py:
import asyncio
import logging
import webrtcvad
import struct
from speech_service import transcrever_audio_async, sintetizar_fala_async
from ai_service import enviar_mensagem_para_ia, extrair_mensagem_da_resposta, obter_estado_chamada
from state_machine import State
import os

KIND_SLIN = 0x10
logger = logging.getLogger(__name__)

async def receber_audio(reader, state_machine, audio_queue):
    try:
        while True:
            header = await reader.readexactly(3)
            kind = header[0]
            length = int.from_bytes(header[1:3], 'big')
            audio_chunk = await reader.readexactly(length)
            if kind == KIND_SLIN and len(audio_chunk) == 320 and state_machine.is_user_turn():
                await audio_queue.put(audio_chunk)
    except asyncio.IncompleteReadError:
        logger.info("Cliente desconectado")
    except Exception as e:
        logger.error(f"Erro ao receber áudio: {e}")

async def enviar_audio(writer, dados_audio, origem="desconhecida"):
    logger.info(f"Iniciando envio de áudio sintetizado (origem: {origem}), tamanho total: {len(dados_audio)} bytes")
    chunk_size = 320
    for i in range(0, len(dados_audio), chunk_size):
        chunk = dados_audio[i:i + chunk_size]
        if chunk:
            writer.write(struct.pack('>B H', KIND_SLIN, len(chunk)) + chunk)
            await writer.drain()
            await asyncio.sleep(0.02)
    logger.info(f"Envio de áudio concluído (origem: {origem})")

async def enviar_audio_em_loop(writer, caminho_audio):
    with open(caminho_audio, 'rb') as f:
        dados_audio = f.read()

    chunk_size = 320
    try:
        while True:
            for i in range(0, len(dados_audio), chunk_size):
                chunk = dados_audio[i:i + chunk_size]
                if chunk:
                    writer.write(struct.pack('>B H', KIND_SLIN, len(chunk)) + chunk)
                    await writer.drain()
                    await asyncio.sleep(0.02)
    except asyncio.CancelledError:
        logger.info("Áudio de espera cancelado.")

async def monitorar_waiting(state_machine, writer, caminho_audio):
    waiting_task = None
    while True:
        await state_machine.wait_for_state(State.WAITING)
        logger.info("Estado WAITING detectado, aguardando 3 segundos antes de iniciar áudio de espera.")
        try:
            await asyncio.wait_for(state_machine.wait_for_state_change(), timeout=3.0)
            logger.info("Estado WAITING terminou antes dos 3 segundos, áudio de espera não será iniciado.")
        except asyncio.TimeoutError:
            if state_machine.is_waiting():
                logger.info("Timeout atingido em WAITING, iniciando áudio de espera.")
                waiting_task = asyncio.create_task(enviar_audio_em_loop(writer, caminho_audio))

                await state_machine.wait_for_state_change()
                if waiting_task:
                    waiting_task.cancel()
                    waiting_task = None

async def processar_audio(audio_queue, vad, state_machine, writer):
    frames = []
    is_speaking = False
    silence_start = None

    while True:
        chunk = await audio_queue.get()
        is_voice = vad.is_speech(chunk, 8000)

        if is_voice:
            frames.append(chunk)
            is_speaking = True
            silence_start = None
        elif is_speaking:
            if silence_start is None:
                silence_start = asyncio.get_event_loop().time()
            elif asyncio.get_event_loop().time() - silence_start > 2.0:
                is_speaking = False
                audio_data = b''.join(frames)
                frames.clear()

                if audio_data:
                    state_machine.transition_to(State.WAITING)
                    logger.info("Áudio capturado completo, iniciando transcrição.")

                    texto = await transcrever_audio_async(audio_data)

                    if not texto:
                        logger.warning("Nenhuma transcrição obtida.")
                        state_machine.transition_to(State.USER_TURN)
                        continue

                    logger.info(f"Texto transcrito: {texto}")

                    resposta = await enviar_mensagem_para_ia(texto, state_machine.get_conversation_id())
                    mensagem = extrair_mensagem_da_resposta(resposta)
                    proximo_estado = obter_estado_chamada(resposta)

                    logger.info(f"Mensagem recebida da IA: {mensagem}")
                    logger.info(f"Estado sugerido pela API: {proximo_estado}")

                    if mensagem:
                        audio_resposta = await sintetizar_fala_async(mensagem)
                        if audio_resposta and len(audio_resposta) > 0:
                            state_machine.transition_to(State.IA_TURN)
                            logger.info("Enviando áudio sintetizado ao usuário.")
                            await enviar_audio(writer, audio_resposta, origem="IA Response")
                            await asyncio.sleep(0.5)
                        else:
                            logger.warning("Resposta de áudio vazia.")

                    if proximo_estado:
                        logger.info(f"Aplicando estado '{proximo_estado}' após envio completo do áudio.")
                        state_machine.transition_to(State[proximo_estado])
                    else:
                        state_machine.transition_to(State.USER_TURN)

async def iniciar_servidor_audiosocket(reader, writer, state_machine):
    vad = webrtcvad.Vad(2)
    audio_queue = asyncio.Queue()
    caminho_audio_espera = os.path.join('audio', 'waiting.slin')

    logger.info("Iniciando síntese de áudio para mensagem de saudação.")
    greeting_audio = await sintetizar_fala_async("Condomínio Apoena, em que posso ajudar?")
    if greeting_audio:
        state_machine.transition_to(State.IA_TURN)
        logger.info("Enviando áudio da mensagem inicial ao cliente.")
        await enviar_audio(writer, greeting_audio, origem="Greeting")
        await asyncio.sleep(0.5)

    state_machine.transition_to(State.USER_TURN)

    # Adiciona a tarefa de monitoramento do WAITING independentemente
    await asyncio.gather(
        receber_audio(reader, state_machine, audio_queue),
        processar_audio(audio_queue, vad, state_machine, writer),
        monitorar_waiting(state_machine, writer, caminho_audio_espera)
    )


requirements.txt:
azure-cognitiveservices-speech==1.41.1
httpx==0.27.2
pydub==0.25.1
python-dotenv==1.0.1
webrtcvad==2.0.10
asyncio
typing_extensions
fastapi
uvicorn[standard]
jinja2
pydantic


config.json:
{
    "greeting": {
        "message": "Condomínio Apoena, em que posso ajudar?",
        "voice": "pt-BR-AntonioNeural",
        "delay_seconds": 2
    },
    "system": {
        "default_state": "STANDBY",
        "silence_threshold_seconds": 2.0,
        "max_transaction_time_seconds": 30
    },
    "audio": {
        "sample_rate": 8000,
        "channels": 1,
        "format": "SLIN",
        "chunk_size": 320
    }
}

state_machine.py:
#!/usr/bin/env python3
# state_machine.py - Máquina de estado simplificada para gerenciar turnos de conversa

import enum
import logging
import uuid
from datetime import datetime
from typing import Callable, List, Dict, Optional
import asyncio  # <-- necessário para as funções assíncronas adicionadas

logger = logging.getLogger(__name__)


class State(enum.Enum):
    """Estados simplificados para a conversa entre usuário e IA."""
    STANDBY = "STANDBY"  # Estado inicial, aguardando nova chamada
    USER_TURN = "USER_TURN"  # Turno do usuário (sistema está ouvindo)
    WAITING = "WAITING"  # Estado intermediário de processamento
    IA_TURN = "IA_TURN"  # Turno da IA (sistema está respondendo)


class StateMachine:
    """
    Máquina de estados simplificada para controlar o fluxo de comunicação
    entre o usuário e a IA em uma chamada.
    """

    def __init__(self):
        self.current_state = State.STANDBY
        self.conversation_id = None
        self.state_change_callbacks: Dict[State, List[Callable]] = {
            state: [] for state in State
        }
        self.transcricoes = []
        self.ultima_resposta = None
        logger.info(f"Máquina de estados inicializada em {self.current_state}")

    def get_state(self) -> State:
        return self.current_state

    def transition_to(self, new_state: State) -> None:
        if new_state == self.current_state:
            logger.debug(f"Ignorando transição redundante para {new_state}")
            return

        old_state = self.current_state
        self.current_state = new_state
        logger.info(f"Transição de estado: {old_state} -> {new_state}")

        if old_state == State.IA_TURN and new_state == State.USER_TURN:
            logger.info("*** IMPORTANTE: Transição de IA_TURN para USER_TURN - ativando escuta ***")
            self.registrar_transcricao_sistema("Sistema ativou escuta - aguardando fala do usuário")
        elif old_state == State.WAITING:
            logger.info(f"Saindo do estado WAITING para {new_state}")
            self.registrar_transcricao_sistema(f"Estado alterado: {old_state.value} -> {new_state.value}")
        if new_state == State.USER_TURN:
            logger.info("*** Sistema pronto para ouvir o usuário ***")

        for callback in self.state_change_callbacks.get(new_state, []):
            try:
                callback()
            except Exception as e:
                logger.error(f"Erro no callback de mudança de estado: {e}")

    def on_state_change(self, state: State, callback: Callable) -> None:
        if state in self.state_change_callbacks:
            self.state_change_callbacks[state].append(callback)

    def is_user_turn(self) -> bool:
        return self.current_state == State.USER_TURN

    def is_ai_turn(self) -> bool:
        return self.current_state == State.IA_TURN

    def is_waiting(self) -> bool:
        return self.current_state == State.WAITING

    def is_standby(self) -> bool:
        return self.current_state == State.STANDBY

    def start_new_conversation(self, standby=False) -> str:
        self.conversation_id = str(uuid.uuid4())
        self.transcricoes = []
        self.ultima_resposta = None

        if not standby:
            self.transition_to(State.USER_TURN)
        else:
            self.transition_to(State.STANDBY)

        logger.info(f"Nova conversa iniciada com ID: {self.conversation_id}")
        return self.conversation_id

    def registrar_transcricao_usuario(self, texto: str) -> None:
        self.transcricoes.append({
            "timestamp": datetime.now().isoformat(),
            "origem": "usuario",
            "texto": texto
        })
        logger.info(f"Transcrição do usuário registrada: {texto}")

    def registrar_transcricao_ia(self, texto: str, resposta_completa: Optional[Dict] = None) -> None:
        self.transcricoes.append({
            "timestamp": datetime.now().isoformat(),
            "origem": "ia",
            "texto": texto
        })
        self.ultima_resposta = resposta_completa
        logger.info(f"Resposta da IA registrada: {texto}")

    def registrar_transcricao_sistema(self, texto: str) -> None:
        self.transcricoes.append({
            "timestamp": datetime.now().isoformat(),
            "origem": "sistema",
            "texto": texto
        })
        logger.info(f"Mensagem do sistema registrada: {texto}")

    def obter_historico_transcricoes(self) -> List[Dict]:
        return self.transcricoes

    def get_conversation_id(self) -> Optional[str]:
        return self.conversation_id

    def end_conversation(self) -> None:
        logger.info(f"Conversa {self.conversation_id} finalizada")
        self.conversation_id = None
        self.transition_to(State.STANDBY)

    # Novas funções adicionadas:
    async def wait_for_state(self, state: State):
        """Aguarda até que o estado especificado seja atingido."""
        while self.current_state != state:
            await asyncio.sleep(0.1)

    async def wait_for_state_change(self):
        """Aguarda até que ocorra uma mudança do estado atual."""
        current_state = self.current_state
        while self.current_state == current_state:
            await asyncio.sleep(0.1)


README.md:
# AudioSocket Simple

Aplicação simplificada para atendimento por IA em condomínios utilizando o protocolo AudioSocket do Asterisk.

## Descrição

Esta aplicação é responsável por gerenciar chamadas VoIP através do protocolo AudioSocket, realizando:

1. Detecção de voz usando VAD (Voice Activity Detection)
2. Transcrição do áudio usando Azure Speech Services
3. Processamento de intenções via API de IA
4. Síntese de voz para respostas
5. Gerenciamento de turnos de conversa entre usuário e IA

## Funcionalidades principais

- Socket TCP para comunicação com o Asterisk via protocolo AudioSocket
- Máquina de estados simplificada com 4 estados (STANDBY, USER_TURN, WAITING, IA_TURN)
- Gerenciamento de IDs de conversa para manter o contexto entre mensagens
- Mensagem de saudação automática configurável
- Detecção de voz e silêncio usando webrtcvad
- Transcrição de áudio através do Azure Speech Services
- Comunicação com API de IA para processar mensagens do usuário
- Síntese de voz usando Azure Speech Services
- Interface web simples para debug e monitoramento

## Requisitos

- Python 3.8+
- Biblioteca webrtcvad para detecção de voz
- Azure Speech Services para transcrição e síntese de voz
- API de IA para processamento de mensagens

## Configuração

1. Crie um arquivo `.env` com as seguintes variáveis de ambiente:
   ```
   AZURE_SPEECH_KEY=sua_chave_do_azure
   AZURE_SPEECH_REGION=sua_regiao_do_azure
   SILENCE_THRESHOLD_SECONDS=2.0
   AI_API_URL=http://localhost:8000/messages
   ```

2. Instale as dependências:
   ```
   pip install -r requirements.txt
   ```

3. Personalize a mensagem de saudação no arquivo `config.json`:
   ```json
   {
     "greeting": {
       "message": "Condomínio Apoena, em que posso ajudar?",
       "voice": "pt-BR-AntonioNeural",
       "delay_seconds": 2
     },
     ...
   }
   ```

## Execução

### Servidor principal
Inicie o servidor principal que lida com a chamada e inclui a interface web de debug:
```
python main.py
```

O servidor AudioSocket irá escutar em 127.0.0.1:8080 e a interface web de debug estará disponível em http://127.0.0.1:8081.

### Cliente de teste com microfone
Para testar o sistema com seu microfone local:
```
python microfone_client.py
```

Este cliente captura áudio do microfone do seu computador e o envia para o servidor AudioSocket, permitindo testar toda a funcionalidade sem precisar do Asterisk.

Opções disponíveis:
```
python microfone_client.py --host 127.0.0.1 --port 8080
```

## Fluxo de conversação

1. **Estado STANDBY**: 
   - Sistema aguarda uma conexão
   - Ao receber uma conexão, registra o ID da chamada
   - Após um breve delay, envia a mensagem de saudação
   
2. **Estado USER_TURN**:
   - Sistema ativa após a saudação
   - Detecta quando o usuário começa a falar
   - Captura o áudio até identificar silêncio
   - Transcreve o áudio usando Azure Speech
   
3. **Estado WAITING**:
   - Processa o texto transcrito
   - Envia para a API de IA
   
4. **Estado IA_TURN**:
   - Recebe a resposta da IA
   - Sintetiza a fala usando Azure Speech
   - Envia o áudio de resposta
   - Retorna para USER_TURN ou STANDBY dependendo da resposta da IA

## Interface de Debug

A interface web de debug mostra:
- Estado atual da chamada
- ID da conversa ativa
- Histórico de transcrições (usuário, IA e sistema)

A página atualiza automaticamente a cada 2 segundos para mostrar o estado atual.

## API de IA

A aplicação se comunica com uma API de IA pela URL `http://localhost:8000/messages` que deve retornar uma resposta no formato:

```json
{
    "content": {
        "mensagem": "Texto da resposta da IA",
        "dados": {
            "intent_type": "tipo_intencao",
            "outros_dados": "valores_relevantes"
        },
        "valid_for_action": false
    },
    "timestamp": "2025-04-15T19:17:59.052067",
    "set_call_status": "USER_TURN"
}
```

O campo `set_call_status` pode ter os valores: "USER_TURN", "WAITING", "IA_TURN" ou "STANDBY" para controlar o fluxo da conversa.

audio_utils.py:
#!/usr/bin/env python3
# audio_utils.py - Utilitários para manipulação de áudio

from pydub import AudioSegment
from io import BytesIO
import logging

logger = logging.getLogger(__name__)

def converter_bytes_para_wav(dados_slin, sample_rate=8000):
    """
    Converte bytes no formato SLIN para WAV.
    
    Args:
        dados_slin: Bytes de áudio no formato SLIN
        sample_rate: Taxa de amostragem (padrão: 8000 Hz)
        
    Returns:
        Bytes WAV do áudio convertido
    """
    try:
        audio_segment = AudioSegment(
            data=bytes(dados_slin),
            sample_width=2,  # 16 bits = 2 bytes
            frame_rate=sample_rate,
            channels=1
        )
        buffer = BytesIO()
        audio_segment.export(buffer, format='wav')
        return buffer.getvalue()
    except Exception as e:
        logger.error(f"Erro ao converter bytes para WAV: {e}")
        return None

def converter_wav_para_slin(dados_wav, sample_rate=8000):
    """
    Converte bytes WAV para o formato SLIN.
    
    Args:
        dados_wav: Bytes de áudio no formato WAV
        sample_rate: Taxa de amostragem desejada (padrão: 8000 Hz)
        
    Returns:
        Bytes SLIN do áudio convertido
    """
    try:
        audio_segment = AudioSegment.from_file(BytesIO(dados_wav), format="wav")
        audio_segment = audio_segment.set_frame_rate(sample_rate).set_channels(1).set_sample_width(2)
        return audio_segment.raw_data
    except Exception as e:
        logger.error(f"Erro ao converter WAV para SLIN: {e}")
        return None

microfone_client.py:
#!/usr/bin/env python3
import socket, struct, threading, pyaudio, logging, uuid

logging.basicConfig(level=logging.INFO, format='%(asctime)s %(levelname)s %(message)s')

KIND_ID, KIND_SLIN, KIND_HANGUP = 0x01, 0x10, 0x00

class AudioSocketClient:
    def __init__(self, host='127.0.0.1', port=8080):
        self.host, self.port = host, port
        self.call_id = uuid.uuid4().bytes
        self.sample_rate, self.channels, self.chunk_size = 8000, 1, 320
        self.format = pyaudio.paInt16
        self.running = False

    def connect(self):
        self.socket = socket.socket(socket.AF_INET, socket.SOCK_STREAM)
        self.socket.connect((self.host, self.port))
        self.socket.sendall(struct.pack('>B H', KIND_ID, len(self.call_id)) + self.call_id)
        logging.info("Conectado ao servidor.")
        self.running = True
        threading.Thread(target=self.send_audio).start()
        threading.Thread(target=self.receive_audio).start()

    def send_audio(self):
        p = pyaudio.PyAudio()
        stream = p.open(format=self.format, channels=self.channels, rate=self.sample_rate, input=True, frames_per_buffer=self.chunk_size//2)
        try:
            while self.running:
                data = stream.read(self.chunk_size//2, exception_on_overflow=False)
                self.socket.sendall(struct.pack('>B H', KIND_SLIN, len(data)) + data)
        except Exception as e:
            logging.error(f"Erro no envio de áudio: {e}")
            self.running = False
        stream.stop_stream()
        stream.close()
        p.terminate()

    def receive_audio(self):
        p = pyaudio.PyAudio()
        stream = p.open(format=self.format, channels=self.channels, rate=self.sample_rate, output=True)
        try:
            while self.running:
                header = self.socket.recv(3)
                if not header: break
                kind, length = header[0], struct.unpack('>H', header[1:3])[0]
                payload = self.socket.recv(length)
                if kind == KIND_SLIN:
                    stream.write(payload)
        except Exception as e:
            logging.error(f"Erro no recebimento de áudio: {e}")
            self.running = False
        stream.stop_stream()
        stream.close()
        p.terminate()

    def disconnect(self):
        self.running = False
        self.socket.sendall(struct.pack('>B H', KIND_HANGUP, 0))
        self.socket.close()
        logging.info("Desconectado do servidor.")

if __name__ == "__main__":
    client = AudioSocketClient()
    try:
        client.connect()
        while client.running: pass
    except KeyboardInterrupt:
        client.disconnect()


code_gpt.py:
import os

IGNORE = {'__pycache__', '.env', '.git', '.idea', 'audio'}

def list_structure(base_path, prefix=""):
    tree = ""
    items = sorted(os.listdir(base_path))
    for item in items:
        if item in IGNORE:
            continue
        path = os.path.join(base_path, item)
        if os.path.isdir(path):
            tree += f"{prefix}{item}/\n"
            tree += list_structure(path, prefix + "    ")
        else:
            tree += f"{prefix}{item}\n"
    return tree

def list_code_files(base_path):
    code = ""
    for root, dirs, files in os.walk(base_path):
        dirs[:] = [d for d in dirs if d not in IGNORE]
        for file in files:
            if file in IGNORE:
                continue
            file_path = os.path.join(root, file)
            rel_path = os.path.relpath(file_path, base_path)
            code += f"\n{rel_path}:\n"
            try:
                with open(file_path, "r", encoding="utf-8") as f:
                    code += f.read() + "\n"
            except Exception as e:
                code += f"Erro ao ler arquivo: {e}\n"
    return code

def main():
    base_path = "."
    structure = list_structure(base_path)
    code_files = list_code_files(base_path)

    with open("code.txt", "w", encoding="utf-8") as f:
        f.write("Estrutura do projeto:\n")
        f.write(structure)
        f.write("\nCódigo fonte:\n")
        f.write(code_files)

    print("Arquivo 'code.txt' criado com sucesso.")

if __name__ == "__main__":
    main()


main.py:
import asyncio
import logging
from dotenv import load_dotenv
from state_machine import StateMachine
from audiosocket_handler import iniciar_servidor_audiosocket

load_dotenv()
logging.basicConfig(level=logging.INFO)

async def handle_client(reader, writer):
    state_machine = StateMachine()
    state_machine.start_new_conversation()

    # Lendo a mensagem inicial KIND_ID (esperado pelo cliente)
    try:
        header = await reader.readexactly(3)
        kind = header[0]
        length = int.from_bytes(header[1:3], "big")
        call_id = await reader.readexactly(length)

        if kind != 0x01:  # KIND_ID
            logging.error("Mensagem inicial não é KIND_ID. Fechando conexão.")
            writer.close()
            await writer.wait_closed()
            return

        logging.info(f"Recebido Call ID: {call_id.hex()}")

        # Agora iniciar o servidor audio socket normalmente
        await iniciar_servidor_audiosocket(reader, writer, state_machine)

    except asyncio.IncompleteReadError:
        logging.error("Cliente desconectado inesperadamente.")
    except Exception as e:
        logging.error(f"Erro inesperado: {e}")
    finally:
        writer.close()
        await writer.wait_closed()

async def main():
    server = await asyncio.start_server(handle_client, '127.0.0.1', 8080)
    logging.info("Servidor AudioSocket rodando em 127.0.0.1:8080")
    async with server:
        try:
            await server.serve_forever()
        except asyncio.CancelledError:
            logging.info("Servidor está sendo encerrado.")

if __name__ == "__main__":
    try:
        asyncio.run(main())
    except KeyboardInterrupt:
        print("Servidor encerrado manualmente.")


speech_service.py:
import asyncio
from io import BytesIO

from audio_utils import converter_bytes_para_wav, converter_wav_para_slin
import azure.cognitiveservices.speech as speechsdk
import os
from pydub import AudioSegment

async def transcrever_audio_async(dados_audio_slin):
    loop = asyncio.get_event_loop()
    return await loop.run_in_executor(None, transcrever_audio, dados_audio_slin)

async def sintetizar_fala_async(texto):
    loop = asyncio.get_event_loop()
    return await loop.run_in_executor(None, sintetizar_fala, texto)

def transcrever_audio(dados_audio_slin):
    audio_wav = converter_bytes_para_wav(dados_audio_slin, 8000)
    if not audio_wav:
        return None
    audio_segment = AudioSegment.from_file(BytesIO(audio_wav), format="wav")
    audio_segment = audio_segment.set_frame_rate(16000)
    buffer = BytesIO()
    audio_segment.export(buffer, format='wav')
    audio_wav_16k = buffer.getvalue()
    speech_config = speechsdk.SpeechConfig(subscription=os.getenv('AZURE_SPEECH_KEY'), region=os.getenv('AZURE_SPEECH_REGION'))
    speech_config.speech_recognition_language = 'pt-BR'
    audio_stream = speechsdk.audio.PushAudioInputStream()
    audio_stream.write(audio_wav_16k)
    audio_stream.close()
    audio_config = speechsdk.audio.AudioConfig(stream=audio_stream)
    recognizer = speechsdk.SpeechRecognizer(speech_config=speech_config, audio_config=audio_config)
    result = recognizer.recognize_once()
    return result.text if result.reason == speechsdk.ResultReason.RecognizedSpeech else None

def sintetizar_fala(texto):
    speech_config = speechsdk.SpeechConfig(subscription=os.getenv('AZURE_SPEECH_KEY'), region=os.getenv('AZURE_SPEECH_REGION'))
    speech_config.speech_synthesis_language = 'pt-BR'
    synthesizer = speechsdk.SpeechSynthesizer(speech_config=speech_config, audio_config=None)
    result = synthesizer.speak_text_async(texto).get()
    if result.reason == speechsdk.ResultReason.SynthesizingAudioCompleted:
        return converter_wav_para_slin(result.audio_data, 8000)
    return None



state_machine.py:
#!/usr/bin/env python3
# state_machine.py - Máquina de estado simplificada para gerenciar turnos de conversa

import enum
import logging
import uuid
from datetime import datetime
from typing import Callable, List, Dict, Optional
import asyncio  # <-- necessário para as funções assíncronas adicionadas

logger = logging.getLogger(__name__)


class State(enum.Enum):
    """Estados simplificados para a conversa entre usuário e IA."""
    STANDBY = "STANDBY"  # Estado inicial, aguardando nova chamada
    USER_TURN = "USER_TURN"  # Turno do usuário (sistema está ouvindo)
    WAITING = "WAITING"  # Estado intermediário de processamento
    IA_TURN = "IA_TURN"  # Turno da IA (sistema está respondendo)


class StateMachine:
    """
    Máquina de estados simplificada para controlar o fluxo de comunicação
    entre o usuário e a IA em uma chamada.
    """

    def __init__(self):
        self.current_state = State.STANDBY
        self.conversation_id = None
        self.state_change_callbacks: Dict[State, List[Callable]] = {
            state: [] for state in State
        }
        self.transcricoes = []
        self.ultima_resposta = None
        logger.info(f"Máquina de estados inicializada em {self.current_state}")

    def get_state(self) -> State:
        return self.current_state

    def transition_to(self, new_state: State) -> None:
        if new_state == self.current_state:
            logger.debug(f"Ignorando transição redundante para {new_state}")
            return

        old_state = self.current_state
        self.current_state = new_state
        logger.info(f"Transição de estado: {old_state} -> {new_state}")

        if old_state == State.IA_TURN and new_state == State.USER_TURN:
            logger.info("*** IMPORTANTE: Transição de IA_TURN para USER_TURN - ativando escuta ***")
            self.registrar_transcricao_sistema("Sistema ativou escuta - aguardando fala do usuário")
        elif old_state == State.WAITING:
            logger.info(f"Saindo do estado WAITING para {new_state}")
            self.registrar_transcricao_sistema(f"Estado alterado: {old_state.value} -> {new_state.value}")
        if new_state == State.USER_TURN:
            logger.info("*** Sistema pronto para ouvir o usuário ***")

        for callback in self.state_change_callbacks.get(new_state, []):
            try:
                callback()
            except Exception as e:
                logger.error(f"Erro no callback de mudança de estado: {e}")

    def on_state_change(self, state: State, callback: Callable) -> None:
        if state in self.state_change_callbacks:
            self.state_change_callbacks[state].append(callback)

    def is_user_turn(self) -> bool:
        return self.current_state == State.USER_TURN

    def is_ai_turn(self) -> bool:
        return self.current_state == State.IA_TURN

    def is_waiting(self) -> bool:
        return self.current_state == State.WAITING

    def is_standby(self) -> bool:
        return self.current_state == State.STANDBY

    def start_new_conversation(self, standby=False) -> str:
        self.conversation_id = str(uuid.uuid4())
        self.transcricoes = []
        self.ultima_resposta = None

        if not standby:
            self.transition_to(State.USER_TURN)
        else:
            self.transition_to(State.STANDBY)

        logger.info(f"Nova conversa iniciada com ID: {self.conversation_id}")
        return self.conversation_id

    def registrar_transcricao_usuario(self, texto: str) -> None:
        self.transcricoes.append({
            "timestamp": datetime.now().isoformat(),
            "origem": "usuario",
            "texto": texto
        })
        logger.info(f"Transcrição do usuário registrada: {texto}")

    def registrar_transcricao_ia(self, texto: str, resposta_completa: Optional[Dict] = None) -> None:
        self.transcricoes.append({
            "timestamp": datetime.now().isoformat(),
            "origem": "ia",
            "texto": texto
        })
        self.ultima_resposta = resposta_completa
        logger.info(f"Resposta da IA registrada: {texto}")

    def registrar_transcricao_sistema(self, texto: str) -> None:
        self.transcricoes.append({
            "timestamp": datetime.now().isoformat(),
            "origem": "sistema",
            "texto": texto
        })
        logger.info(f"Mensagem do sistema registrada: {texto}")

    def obter_historico_transcricoes(self) -> List[Dict]:
        return self.transcricoes

    def get_conversation_id(self) -> Optional[str]:
        return self.conversation_id

    def end_conversation(self) -> None:
        logger.info(f"Conversa {self.conversation_id} finalizada")
        self.conversation_id = None
        self.transition_to(State.STANDBY)

    # Novas funções adicionadas:
    async def wait_for_state(self, state: State):
        """Aguarda até que o estado especificado seja atingido."""
        while self.current_state != state:
            await asyncio.sleep(0.1)

    async def wait_for_state_change(self):
        """Aguarda até que ocorra uma mudança do estado atual."""
        current_state = self.current_state
        while self.current_state == current_state:
            await asyncio.sleep(0.1)


README.md:
# AudioSocket Simple

Aplicação simplificada para atendimento por IA em condomínios utilizando o protocolo AudioSocket do Asterisk.

## Descrição

Esta aplicação é responsável por gerenciar chamadas VoIP através do protocolo AudioSocket, realizando:

1. Detecção de voz usando VAD (Voice Activity Detection)
2. Transcrição do áudio usando Azure Speech Services
3. Processamento de intenções via API de IA
4. Síntese de voz para respostas
5. Gerenciamento de turnos de conversa entre usuário e IA

## Funcionalidades principais

- Socket TCP para comunicação com o Asterisk via protocolo AudioSocket
- Máquina de estados simplificada com 4 estados (STANDBY, USER_TURN, WAITING, IA_TURN)
- Gerenciamento de IDs de conversa para manter o contexto entre mensagens
- Mensagem de saudação automática configurável
- Detecção de voz e silêncio usando webrtcvad
- Transcrição de áudio através do Azure Speech Services
- Comunicação com API de IA para processar mensagens do usuário
- Síntese de voz usando Azure Speech Services
- Interface web simples para debug e monitoramento

## Requisitos

- Python 3.8+
- Biblioteca webrtcvad para detecção de voz
- Azure Speech Services para transcrição e síntese de voz
- API de IA para processamento de mensagens

## Configuração

1. Crie um arquivo `.env` com as seguintes variáveis de ambiente:
   ```
   AZURE_SPEECH_KEY=sua_chave_do_azure
   AZURE_SPEECH_REGION=sua_regiao_do_azure
   SILENCE_THRESHOLD_SECONDS=2.0
   AI_API_URL=http://localhost:8000/messages
   ```

2. Instale as dependências:
   ```
   pip install -r requirements.txt
   ```

3. Personalize a mensagem de saudação no arquivo `config.json`:
   ```json
   {
     "greeting": {
       "message": "Condomínio Apoena, em que posso ajudar?",
       "voice": "pt-BR-AntonioNeural",
       "delay_seconds": 2
     },
     ...
   }
   ```

## Execução

### Servidor principal
Inicie o servidor principal que lida com a chamada e inclui a interface web de debug:
```
python main.py
```

O servidor AudioSocket irá escutar em 127.0.0.1:8080 e a interface web de debug estará disponível em http://127.0.0.1:8081.

### Cliente de teste com microfone
Para testar o sistema com seu microfone local:
```
python microfone_client.py
```

Este cliente captura áudio do microfone do seu computador e o envia para o servidor AudioSocket, permitindo testar toda a funcionalidade sem precisar do Asterisk.

Opções disponíveis:
```
python microfone_client.py --host 127.0.0.1 --port 8080
```

## Fluxo de conversação

1. **Estado STANDBY**: 
   - Sistema aguarda uma conexão
   - Ao receber uma conexão, registra o ID da chamada
   - Após um breve delay, envia a mensagem de saudação
   
2. **Estado USER_TURN**:
   - Sistema ativa após a saudação
   - Detecta quando o usuário começa a falar
   - Captura o áudio até identificar silêncio
   - Transcreve o áudio usando Azure Speech
   
3. **Estado WAITING**:
   - Processa o texto transcrito
   - Envia para a API de IA
   
4. **Estado IA_TURN**:
   - Recebe a resposta da IA
   - Sintetiza a fala usando Azure Speech
   - Envia o áudio de resposta
   - Retorna para USER_TURN ou STANDBY dependendo da resposta da IA

## Interface de Debug

A interface web de debug mostra:
- Estado atual da chamada
- ID da conversa ativa
- Histórico de transcrições (usuário, IA e sistema)

A página atualiza automaticamente a cada 2 segundos para mostrar o estado atual.

## API de IA

A aplicação se comunica com uma API de IA pela URL `http://localhost:8000/messages` que deve retornar uma resposta no formato:

```json
{
    "content": {
        "mensagem": "Texto da resposta da IA",
        "dados": {
            "intent_type": "tipo_intencao",
            "outros_dados": "valores_relevantes"
        },
        "valid_for_action": false
    },
    "timestamp": "2025-04-15T19:17:59.052067",
    "set_call_status": "USER_TURN"
}
```

O campo `set_call_status` pode ter os valores: "USER_TURN", "WAITING", "IA_TURN" ou "STANDBY" para controlar o fluxo da conversa.

audio_utils.py:
#!/usr/bin/env python3
# audio_utils.py - Utilitários para manipulação de áudio

from pydub import AudioSegment
from io import BytesIO
import logging

logger = logging.getLogger(__name__)

def converter_bytes_para_wav(dados_slin, sample_rate=8000):
    """
    Converte bytes no formato SLIN para WAV.
    
    Args:
        dados_slin: Bytes de áudio no formato SLIN
        sample_rate: Taxa de amostragem (padrão: 8000 Hz)
        
    Returns:
        Bytes WAV do áudio convertido
    """
    try:
        audio_segment = AudioSegment(
            data=bytes(dados_slin),
            sample_width=2,  # 16 bits = 2 bytes
            frame_rate=sample_rate,
            channels=1
        )
        buffer = BytesIO()
        audio_segment.export(buffer, format='wav')
        return buffer.getvalue()
    except Exception as e:
        logger.error(f"Erro ao converter bytes para WAV: {e}")
        return None

def converter_wav_para_slin(dados_wav, sample_rate=8000):
    """
    Converte bytes WAV para o formato SLIN.
    
    Args:
        dados_wav: Bytes de áudio no formato WAV
        sample_rate: Taxa de amostragem desejada (padrão: 8000 Hz)
        
    Returns:
        Bytes SLIN do áudio convertido
    """
    try:
        audio_segment = AudioSegment.from_file(BytesIO(dados_wav), format="wav")
        audio_segment = audio_segment.set_frame_rate(sample_rate).set_channels(1).set_sample_width(2)
        return audio_segment.raw_data
    except Exception as e:
        logger.error(f"Erro ao converter WAV para SLIN: {e}")
        return None

microfone_client.py:
#!/usr/bin/env python3
import socket, struct, threading, pyaudio, logging, uuid

logging.basicConfig(level=logging.INFO, format='%(asctime)s %(levelname)s %(message)s')

KIND_ID, KIND_SLIN, KIND_HANGUP = 0x01, 0x10, 0x00

class AudioSocketClient:
    def __init__(self, host='127.0.0.1', port=8080):
        self.host, self.port = host, port
        self.call_id = uuid.uuid4().bytes
        self.sample_rate, self.channels, self.chunk_size = 8000, 1, 320
        self.format = pyaudio.paInt16
        self.running = False

    def connect(self):
        self.socket = socket.socket(socket.AF_INET, socket.SOCK_STREAM)
        self.socket.connect((self.host, self.port))
        self.socket.sendall(struct.pack('>B H', KIND_ID, len(self.call_id)) + self.call_id)
        logging.info("Conectado ao servidor.")
        self.running = True
        threading.Thread(target=self.send_audio).start()
        threading.Thread(target=self.receive_audio).start()

    def send_audio(self):
        p = pyaudio.PyAudio()
        stream = p.open(format=self.format, channels=self.channels, rate=self.sample_rate, input=True, frames_per_buffer=self.chunk_size//2)
        try:
            while self.running:
                data = stream.read(self.chunk_size//2, exception_on_overflow=False)
                self.socket.sendall(struct.pack('>B H', KIND_SLIN, len(data)) + data)
        except Exception as e:
            logging.error(f"Erro no envio de áudio: {e}")
            self.running = False
        stream.stop_stream()
        stream.close()
        p.terminate()

    def receive_audio(self):
        p = pyaudio.PyAudio()
        stream = p.open(format=self.format, channels=self.channels, rate=self.sample_rate, output=True)
        try:
            while self.running:
                header = self.socket.recv(3)
                if not header: break
                kind, length = header[0], struct.unpack('>H', header[1:3])[0]
                payload = self.socket.recv(length)
                if kind == KIND_SLIN:
                    stream.write(payload)
        except Exception as e:
            logging.error(f"Erro no recebimento de áudio: {e}")
            self.running = False
        stream.stop_stream()
        stream.close()
        p.terminate()

    def disconnect(self):
        self.running = False
        self.socket.sendall(struct.pack('>B H', KIND_HANGUP, 0))
        self.socket.close()
        logging.info("Desconectado do servidor.")

if __name__ == "__main__":
    client = AudioSocketClient()
    try:
        client.connect()
        while client.running: pass
    except KeyboardInterrupt:
        client.disconnect()


code_gpt.py:
import os

IGNORE = {'__pycache__', '.env', '.git', '.idea', 'audio'}

def list_structure(base_path, prefix=""):
    tree = ""
    items = sorted(os.listdir(base_path))
    for item in items:
        if item in IGNORE:
            continue
        path = os.path.join(base_path, item)
        if os.path.isdir(path):
            tree += f"{prefix}{item}/\n"
            tree += list_structure(path, prefix + "    ")
        else:
            tree += f"{prefix}{item}\n"
    return tree

def list_code_files(base_path):
    code = ""
    for root, dirs, files in os.walk(base_path):
        dirs[:] = [d for d in dirs if d not in IGNORE]
        for file in files:
            if file in IGNORE:
                continue
            file_path = os.path.join(root, file)
            rel_path = os.path.relpath(file_path, base_path)
            code += f"\n{rel_path}:\n"
            try:
                with open(file_path, "r", encoding="utf-8") as f:
                    code += f.read() + "\n"
            except Exception as e:
                code += f"Erro ao ler arquivo: {e}\n"
    return code

def main():
    base_path = "."
    structure = list_structure(base_path)
    code_files = list_code_files(base_path)

    with open("code.txt", "w", encoding="utf-8") as f:
        f.write("Estrutura do projeto:\n")
        f.write(structure)
        f.write("\nCódigo fonte:\n")
        f.write(code_files)

    print("Arquivo 'code.txt' criado com sucesso.")

if __name__ == "__main__":
    main()


main.py:
import asyncio
import logging
from dotenv import load_dotenv
from state_machine import StateMachine
from audiosocket_handler import iniciar_servidor_audiosocket

load_dotenv()
logging.basicConfig(level=logging.INFO)

async def handle_client(reader, writer):
    state_machine = StateMachine()
    state_machine.start_new_conversation()

    # Lendo a mensagem inicial KIND_ID (esperado pelo cliente)
    try:
        header = await reader.readexactly(3)
        kind = header[0]
        length = int.from_bytes(header[1:3], "big")
        call_id = await reader.readexactly(length)

        if kind != 0x01:  # KIND_ID
            logging.error("Mensagem inicial não é KIND_ID. Fechando conexão.")
            writer.close()
            await writer.wait_closed()
            return

        logging.info(f"Recebido Call ID: {call_id.hex()}")

        # Agora iniciar o servidor audio socket normalmente
        await iniciar_servidor_audiosocket(reader, writer, state_machine)

    except asyncio.IncompleteReadError:
        logging.error("Cliente desconectado inesperadamente.")
    except Exception as e:
        logging.error(f"Erro inesperado: {e}")
    finally:
        writer.close()
        await writer.wait_closed()

async def main():
    server = await asyncio.start_server(handle_client, '127.0.0.1', 8080)
    logging.info("Servidor AudioSocket rodando em 127.0.0.1:8080")
    async with server:
        try:
            await server.serve_forever()
        except asyncio.CancelledError:
            logging.info("Servidor está sendo encerrado.")

if __name__ == "__main__":
    try:
        asyncio.run(main())
    except KeyboardInterrupt:
        print("Servidor encerrado manualmente.")


speech_service.py:
import asyncio
from io import BytesIO

from audio_utils import converter_bytes_para_wav, converter_wav_para_slin
import azure.cognitiveservices.speech as speechsdk
import os
from pydub import AudioSegment

async def transcrever_audio_async(dados_audio_slin):
    loop = asyncio.get_event_loop()
    return await loop.run_in_executor(None, transcrever_audio, dados_audio_slin)

async def sintetizar_fala_async(texto):
    loop = asyncio.get_event_loop()
    return await loop.run_in_executor(None, sintetizar_fala, texto)

def transcrever_audio(dados_audio_slin):
    audio_wav = converter_bytes_para_wav(dados_audio_slin, 8000)
    if not audio_wav:
        return None
    audio_segment = AudioSegment.from_file(BytesIO(audio_wav), format="wav")
    audio_segment = audio_segment.set_frame_rate(16000)
    buffer = BytesIO()
    audio_segment.export(buffer, format='wav')
    audio_wav_16k = buffer.getvalue()
    speech_config = speechsdk.SpeechConfig(subscription=os.getenv('AZURE_SPEECH_KEY'), region=os.getenv('AZURE_SPEECH_REGION'))
    speech_config.speech_recognition_language = 'pt-BR'
    audio_stream = speechsdk.audio.PushAudioInputStream()
    audio_stream.write(audio_wav_16k)
    audio_stream.close()
    audio_config = speechsdk.audio.AudioConfig(stream=audio_stream)
    recognizer = speechsdk.SpeechRecognizer(speech_config=speech_config, audio_config=audio_config)
    result = recognizer.recognize_once()
    return result.text if result.reason == speechsdk.ResultReason.RecognizedSpeech else None

def sintetizar_fala(texto):
    speech_config = speechsdk.SpeechConfig(subscription=os.getenv('AZURE_SPEECH_KEY'), region=os.getenv('AZURE_SPEECH_REGION'))
    speech_config.speech_synthesis_language = 'pt-BR'
    synthesizer = speechsdk.SpeechSynthesizer(speech_config=speech_config, audio_config=None)
    result = synthesizer.speak_text_async(texto).get()
    if result.reason == speechsdk.ResultReason.SynthesizingAudioCompleted:
        return converter_wav_para_slin(result.audio_data, 8000)
    return None

