Estrutura do projeto:
.DS_Store
.env.example
.gitignore
ai/
    __init__.py
    agents.py
    crew.py
    main.py
    models/
        __init__.py
        intent.py
    state_manager.py
    tasks.py
    tools.py
    utils/
        __init__.py
        intent_extractor.py
ai_service.py
audio_utils.py
audiosocket_handler.py
azure_speech_callbacks.py
code_gpt.py
config.json
conversation_flow.py
data/
    apartamentos.json
    ramais_config.json
docs/
    01-introducao.md
    02-arquitetura.md
    03-configuracao.md
    04-fluxos-comunicacao.md
    05-interacao-morador.md
    06-encerramento-chamadas.md
    07-processamento-ai.md
    08-otimizacoes.md
    09-testes.md
    10-desenvolvimento-futuro.md
    11-deteccao-voz-avancada.md
    azure_speech_optimization.md
    deteccao-voz-azure.md
extensions/
    __init__.py
    api_server.py
    config_persistence.py
    db_connector.py
    db_listener.py
    extension_manager.py
    mock_db_connector.py
    resource_manager.py
    server_manager.py
main.py
microfone_client.py
record_slin_audio.py
requirements.txt
services/
    __init__.py
    amqp_service.py
session_manager.py
setup_system.py
speech_service.py
state_machine.py
tools/
    play_slin.py
utils/
    README.md
    __init__.py
    call_logger.py
    log_analyzer.py
    unused/
        uuid_generator.py

Código fonte:

ai_service.py:
import logging
from ai.main import process_message  # Importação direta do CrewAI
from typing import Dict, Any, Optional

logger = logging.getLogger(__name__)

async def enviar_mensagem_para_ia(texto: str, conversation_id: str) -> Dict[str, Any]:
    """
    Agora chama diretamente o CrewAI ao invés da API externa.
    """
    try:
        logger.info("=" * 50)
        logger.info(f"Chamando CrewAI diretamente com mensagem: {texto}")
        resposta = process_message(content=texto, id=conversation_id)

        logger.info("=" * 50)
        logger.info(f"Resposta recebida do CrewAI:")
        logger.info(resposta)
        logger.info("=" * 50)

        # Adiciona o timestamp que antes era retornado pela API
        resposta_com_timestamp = {
            "content": resposta,
            "timestamp": ""
        }

        return resposta_com_timestamp

    except Exception as e:
        logger.error(f"Erro ao comunicar com CrewAI diretamente: {e}")
        return {
            "content": {
                "mensagem": "Desculpe, não consegui processar sua solicitação no momento.",
                "dados": {},
                "valid_for_action": False,
                "set_call_status": "USER_TURN"
            },
            "timestamp": ""
        }

def extrair_mensagem_da_resposta(resposta: Dict[str, Any]) -> str:
    return resposta.get("content", {}).get("mensagem", "")

def obter_estado_chamada(resposta: Dict[str, Any]) -> Optional[str]:
    return resposta.get("content", {}).get("set_call_status")


audiosocket_handler.py:
# audiosocket_handler.py

import asyncio
import logging
import struct
import os
import time
import json
import socket
from typing import Optional
from enum import Enum

import webrtcvad
import azure.cognitiveservices.speech as speechsdk

# Importar nossa classe de callbacks para Azure Speech
from azure_speech_callbacks import SpeechCallbacks

from speech_service import transcrever_audio_async, sintetizar_fala_async
from session_manager import SessionManager  # Importamos o SessionManager
from utils.call_logger import CallLoggerManager  # Importamos o CallLoggerManager
from extensions.resource_manager import resource_manager  # Importamos o ResourceManager

# Constante para o timeout de verificação de terminação
TERMINATE_CHECK_INTERVAL = 0.5  # segundos

# Enum para os tipos de detecção de voz
class VoiceDetectionType(Enum):
    WEBRTCVAD = "webrtcvad"
    AZURE_SPEECH = "azure_speech"

logger = logging.getLogger(__name__)

# Identificador do formato SLIN
KIND_SLIN = 0x10

# Podemos instanciar um SessionManager aqui como singleton/global.
# Se preferir criar em outro lugar, adapte.
session_manager = SessionManager()

# Variável global para armazenar o extension_manager
extension_manager = None

def set_extension_manager(manager):
    """
    Define o extension_manager global para ser usado pelo handler.
    """
    global extension_manager
    extension_manager = manager

# Carregar configurações do config.json
try:
    with open('config.json', 'r') as f:
        config = json.load(f)
        SILENCE_THRESHOLD_SECONDS = config['system'].get('silence_threshold_seconds', 2.0)
        RESIDENT_MAX_SILENCE_SECONDS = config['system'].get('resident_max_silence_seconds', 45.0)
        TRANSMISSION_DELAY_MS = config['audio'].get('transmission_delay_ms', 20) / 1000  # Convertido para segundos
        POST_AUDIO_DELAY_SECONDS = config['audio'].get('post_audio_delay_seconds', 0.5)
        DISCARD_BUFFER_FRAMES = config['audio'].get('discard_buffer_frames', 25)
        GOODBYE_DELAY_SECONDS = config['system'].get('goodbye_delay_seconds', 3.0)  # Tempo para ouvir mensagem de despedida
        
        # Configuração de detecção de voz (webrtcvad ou azure_speech)
        VOICE_DETECTION_TYPE = VoiceDetectionType(config['system'].get('voice_detection_type', 'webrtcvad'))
        # Configurações específicas para Azure Speech
        AZURE_SPEECH_SEGMENT_TIMEOUT_MS = config['system'].get('azure_speech_segment_timeout_ms', 800)
        
        logger.info(f"Configurações carregadas: silence={SILENCE_THRESHOLD_SECONDS}s, resident_max_silence={RESIDENT_MAX_SILENCE_SECONDS}s, transmission_delay={TRANSMISSION_DELAY_MS}s, post_audio_delay={POST_AUDIO_DELAY_SECONDS}s, discard_buffer={DISCARD_BUFFER_FRAMES} frames, goodbye_delay={GOODBYE_DELAY_SECONDS}s, voice_detection={VOICE_DETECTION_TYPE.value}")
except Exception as e:
    logger.warning(f"Erro ao carregar config.json, usando valores padrão: {e}")
    SILENCE_THRESHOLD_SECONDS = 2.0
    RESIDENT_MAX_SILENCE_SECONDS = 45.0
    TRANSMISSION_DELAY_MS = 0.02
    POST_AUDIO_DELAY_SECONDS = 0.5
    DISCARD_BUFFER_FRAMES = 25
    GOODBYE_DELAY_SECONDS = 3.0
    VOICE_DETECTION_TYPE = VoiceDetectionType.WEBRTCVAD
    AZURE_SPEECH_SEGMENT_TIMEOUT_MS = 800


# Função auxiliar para obter a porta local de uma conexão
def get_local_port(writer) -> Optional[int]:
    """
    Obtém a porta local de uma conexão.
    """
    try:
        sock = writer.get_extra_info('socket')
        if sock:
            _, port = sock.getsockname()
            return port
    except:
        pass
    return None


async def check_terminate_flag(session, call_id, role, call_logger=None):
    """
    Tarefa auxiliar que monitora periodicamente se uma sessão deve ser encerrada.
    Retorna True se a terminação foi solicitada.
    """
    event = session.terminate_visitor_event if role == "visitante" else session.terminate_resident_event
    
    try:
        await asyncio.wait_for(event.wait(), timeout=TERMINATE_CHECK_INTERVAL)
        logger.info(f"[{call_id}] Sinal de terminação detectado para {role}")
        if call_logger:
            call_logger.log_event("TERMINATION_SIGNAL_DETECTED", {
                "role": role,
                "timestamp": time.time()
            })
        return True
    except asyncio.TimeoutError:
        # Timeout normal, continua verificando
        return False


async def _encerrar_apos_delay(call_id, session_manager, delay_seconds=5.0):
    """
    Função auxiliar para encerrar a sessão após um delay, permitindo que
    as mensagens enfileiradas sejam processadas e enviadas.
    """
    logger.info(f"[{call_id}] Agendando encerramento de sessão após {delay_seconds} segundos")
    await asyncio.sleep(delay_seconds)
    logger.info(f"[{call_id}] Delay concluído, iniciando encerramento da sessão")
    
    # Verificar se a sessão ainda existe antes de tentar encerrá-la
    if session_manager.get_session(call_id):
        logger.info(f"[{call_id}] Sinalizando encerramento via end_session após delay de {delay_seconds}s")
        session_manager.end_session(call_id)
        
        # Aguarda mais um momento para garantir que as tasks de envio de mensagens
        # tiveram chance de processar o evento de encerramento
        await asyncio.sleep(3.0)  # Tempo mais longo para garantir processamento
        
        # Verificar se a sessão ainda existe após o encerramento inicial
        if session_manager.get_session(call_id):
            logger.warning(f"[{call_id}] Sessão ainda existe após tempo adicional, força bruta para remoção")
            # Remove diretamente a sessão como último recurso
            session_manager._complete_session_termination(call_id)
            
            # Loga para confirmar que a sessão foi encerrada
            if not session_manager.get_session(call_id):
                logger.info(f"[{call_id}] Sessão removida com sucesso após tentativa de força bruta")
            else:
                logger.error(f"[{call_id}] FALHA na remoção completa da sessão mesmo após tentativa de força bruta!")
    else:
        logger.info(f"[{call_id}] Sessão já foi encerrada naturalmente, nenhuma ação necessária")


async def send_goodbye_and_terminate(writer, session, call_id, role, call_logger=None):
    """
    Envia uma mensagem de despedida final e encerra a conexão.
    """
    try:
        # Obter mensagem de despedida baseada na configuração
        # Decisão baseada no papel e no estado da conversa
        if role == "visitante":
            # Verificar se estamos no teste específico com a mensagem de finalização
            if session.intent_data.get("test_hangup") == True:
                goodbye_msg = "A chamada com o morador foi finalizada. Obrigado por utilizar nosso sistema."
            elif session.intent_data.get("authorization_result") == "authorized":
                goodbye_msg = config.get('call_termination', {}).get('goodbye_messages', {}).get('visitor', {}).get(
                    'authorized', "Sua entrada foi autorizada. Obrigado por utilizar nossa portaria inteligente.")
            elif session.intent_data.get("authorization_result") == "denied":
                goodbye_msg = config.get('call_termination', {}).get('goodbye_messages', {}).get('visitor', {}).get(
                    'denied', "Sua entrada não foi autorizada. Obrigado por utilizar nossa portaria inteligente.")
            else:
                goodbye_msg = config.get('call_termination', {}).get('goodbye_messages', {}).get('visitor', {}).get(
                    'default', "Obrigado por utilizar nossa portaria inteligente. Até a próxima!")
        else:
            goodbye_msg = config.get('call_termination', {}).get('goodbye_messages', {}).get('resident', {}).get(
                'default', "Obrigado pela sua resposta. Encerrando a chamada.")
        
        # Registrar evento de envio de despedida
        if call_logger:
            call_logger.log_event("SENDING_GOODBYE", {
                "role": role,
                "message": goodbye_msg
            })
        
        # Sintetizar a mensagem de despedida e enviar diretamente (sem enfileirar)
        logger.info(f"[{call_id}] Enviando mensagem de despedida diretamente para {role}: {goodbye_msg}")
        audio_resposta = await sintetizar_fala_async(goodbye_msg)
        
        if audio_resposta:
            # Enviar o áudio diretamente
            await enviar_audio(writer, audio_resposta, call_id=call_id, origem=role.capitalize())
            
            # Registrar evento de envio bem-sucedido
            if call_logger:
                call_logger.log_event("GOODBYE_SENT_SUCCESSFULLY", {
                    "role": role,
                    "message": goodbye_msg,
                    "audio_size": len(audio_resposta)
                })
                
            # Aguardar um tempo para que a mensagem seja ouvida
            logger.info(f"[{call_id}] Aguardando {GOODBYE_DELAY_SECONDS}s para o {role} ouvir a despedida")
            await asyncio.sleep(GOODBYE_DELAY_SECONDS)
            
            # Verificar se é o teste específico com a mensagem de finalização
            if role == "visitante" and session.intent_data.get("test_hangup") == True:
                # Enviar KIND_HANGUP explicitamente para finalizar a conexão
                logger.info(f"[{call_id}] Enviando KIND_HANGUP para finalizar a conexão ativamente")
                try:
                    # Enviar KIND_HANGUP (0x00) com payload length 0
                    writer.write(struct.pack('>B H', 0x00, 0))
                    await writer.drain()
                    if call_logger:
                        call_logger.log_event("HANGUP_SENT", {
                            "role": role,
                            "reason": "active_termination_test"
                        })
                except Exception as hangup_error:
                    logger.error(f"[{call_id}] Erro ao enviar KIND_HANGUP: {hangup_error}")
                    if call_logger:
                        call_logger.log_error("HANGUP_SEND_FAILED", 
                                        f"Erro ao enviar KIND_HANGUP", 
                                        {"error": str(hangup_error)})
        else:
            logger.error(f"[{call_id}] Falha ao sintetizar mensagem de despedida para {role}")
            if call_logger:
                call_logger.log_error("GOODBYE_SYNTHESIS_FAILED", 
                                    f"Falha ao sintetizar mensagem de despedida para {role}", 
                                    {"message": goodbye_msg})
        
        # Fechar a conexão
        if call_logger:
            call_logger.log_event("CONNECTION_CLOSING", {
                "role": role,
                "reason": "controlled_termination"
            })
        
        writer.close()
        await writer.wait_closed()
        logger.info(f"[{call_id}] Conexão com {role} encerrada com sucesso")
        
    except Exception as e:
        logger.error(f"[{call_id}] Erro ao enviar despedida para {role}: {e}")
        # Ainda assim, tentar fechar a conexão em caso de erro
        try:
            writer.close()
            await writer.wait_closed()
        except:
            pass


async def enviar_audio(writer: asyncio.StreamWriter, dados_audio: bytes, call_id: str = None, origem="desconhecida"):
    """
    Envia dados de áudio (SLIN) ao cliente via 'writer'.
    """
    logger.info(f"[{origem}] Enviando áudio de {len(dados_audio)} bytes.")
    
    # Registrar no log específico da chamada
    if call_id:
        is_visitor = (origem == "Visitante")
        call_logger = CallLoggerManager.get_logger(call_id)
        call_logger.log_event("AUDIO_SEND_START", {
            "target": "visitor" if is_visitor else "resident",
            "audio_size_bytes": len(dados_audio)
        })
        
        # Registrar sessão no ResourceManager se ainda não estiver registrada
        if call_id not in resource_manager.active_sessions:
            porta = get_local_port(writer)
            resource_manager.register_session(call_id, porta)
    
    start_time = time.time()
    chunk_size = 320
    
    # Verificar se precisamos aplicar throttling baseado na carga do sistema
    should_throttle = resource_manager.should_throttle_audio()
    transmission_delay = TRANSMISSION_DELAY_MS * 1.5 if should_throttle else TRANSMISSION_DELAY_MS
    
    if should_throttle:
        logger.warning(f"[{call_id}] Aplicando throttling na transmissão de áudio devido à alta carga do sistema")
    
    for i in range(0, len(dados_audio), chunk_size):
        chunk = dados_audio[i : i + chunk_size]
        header = struct.pack(">B H", KIND_SLIN, len(chunk))
        writer.write(header + chunk)
        await writer.drain()
        # Pequeno atraso para não encher o buffer do lado do Asterisk
        # Se sistema estiver sobrecarregado, aumentamos o delay
        await asyncio.sleep(transmission_delay)
    
    # Registrar conclusão
    if call_id:
        duration_ms = (time.time() - start_time) * 1000
        call_logger.log_event("AUDIO_SEND_COMPLETE", {
            "target": "visitor" if is_visitor else "resident",
            "duration_ms": round(duration_ms, 2)
        })


async def receber_audio_visitante(reader: asyncio.StreamReader, call_id: str):
    """
    Função que redireciona para a implementação apropriada com base na configuração.
    """
    # Escolher a implementação com base na configuração
    if VOICE_DETECTION_TYPE == VoiceDetectionType.WEBRTCVAD:
        return await receber_audio_visitante_vad(reader, call_id)
    elif VOICE_DETECTION_TYPE == VoiceDetectionType.AZURE_SPEECH:
        return await receber_audio_visitante_azure_speech(reader, call_id)
    else:
        # Fallback para webrtcvad se o tipo não for reconhecido
        logger.warning(f"[{call_id}] Tipo de detecção de voz '{VOICE_DETECTION_TYPE}' não reconhecido, usando webrtcvad")
        return await receber_audio_visitante_vad(reader, call_id)

async def receber_audio_visitante_vad(reader: asyncio.StreamReader, call_id: str):
    """
    Implementação usando webrtcvad para detecção de voz e silêncio.
    
    Tarefa que fica lendo o áudio do visitante, detecta quando ele fala (usando VAD)
    e chama `session_manager.process_visitor_text(...)` ao fim de cada frase.
    
    Agora com controle de estado para evitar retroalimentação durante a fala da IA
    e suporte para encerramento gracioso da conexão.
    """
    call_logger = CallLoggerManager.get_logger(call_id)
    vad = webrtcvad.Vad(2)  # Agressivo 0-3
    frames = []
    is_speaking = False
    silence_start = None
    speech_start = None
    
    # Para controlar se estamos no modo de escuta ativa
    is_listening_mode = True
    
    # Acessar a sessão para verificar o estado
    session = session_manager.get_session(call_id)
    if not session:
        logger.error(f"[{call_id}] Sessão não encontrada para iniciar recebimento de áudio")
        return
    
    # Flag de buffer para descartar áudio residual após IA falar
    discard_buffer_frames = 0
    
    while True:
        # Verificar sinal de terminação
        if session.terminate_visitor_event.is_set():
            logger.info(f"[{call_id}] Detectado sinal para encerrar recebimento de áudio do visitante")
            call_logger.log_event("TERMINATE_VISITOR_AUDIO", {
                "reason": "session_terminated",
                "timestamp": time.time()
            })
            break
        
        try:
            # Uso de wait_for com timeout para permitir verificação de terminação
            header = await asyncio.wait_for(reader.readexactly(3), timeout=0.5)
        except asyncio.TimeoutError:
            # Timeout apenas para verificação de terminação, continuamos normalmente
            continue
        except asyncio.IncompleteReadError:
            logger.info(f"[{call_id}] Visitante desconectou (EOF).")
            call_logger.log_call_ended("visitor_disconnected")
            break

        if not header:
            logger.info(f"[{call_id}] Nenhum dado de header, encerrando.")
            call_logger.log_call_ended("invalid_header")
            break

        kind = header[0]
        length = int.from_bytes(header[1:3], "big")

        audio_chunk = await reader.readexactly(length)
        
        # Verificar o estado atual da sessão
        current_state = session.visitor_state
        
        # Se estamos em IA_TURN, significa que a IA está falando - não devemos processar VAD
        if current_state == "IA_TURN":
            is_listening_mode = False
            continue  # Pula processamento durante fala da IA
            
        # Período de transição: após IA falar, descartamos alguns frames para evitar eco
        if discard_buffer_frames > 0:
            discard_buffer_frames -= 1
            continue
            
        # Se acabamos de transitar de IA_TURN para USER_TURN, ativamos modo de escuta e descartamos frames iniciais
        if not is_listening_mode and current_state == "USER_TURN":
            is_listening_mode = True
            discard_buffer_frames = DISCARD_BUFFER_FRAMES  # Descartar quadros para evitar eco
            logger.debug(f"[{call_id}] Ativando modo de escuta de visitante")
            call_logger.log_event("LISTENING_MODE_ACTIVATED", {"timestamp": time.time()})
            
            # Limpar quaisquer frames acumulados anteriormente
            frames = []
            is_speaking = False
            silence_start = None
            speech_start = None
            continue

        # Processamos VAD apenas quando estamos em modo de escuta
        if is_listening_mode and kind == KIND_SLIN and (len(audio_chunk) == 320 or len(audio_chunk) == 640):
            # Se o chunk for de 640 bytes (320 amostras de 16 bits), precisamos garantir que seja processado corretamente
            chunk_to_process = audio_chunk
            if len(audio_chunk) == 640:
                # O WebRTCVAD espera PCM de 16 bits em 320 bytes, então estamos recebendo o dobro do tamanho esperado
                logger.debug(f"[{call_id}] Recebido chunk de 640 bytes do cliente - formato PCM 16-bit")
                
            # Avalia VAD
            is_voice = vad.is_speech(chunk_to_process, 8000)
            if is_voice:
                frames.append(audio_chunk)
                if not is_speaking:
                    is_speaking = True
                    speech_start = asyncio.get_event_loop().time()
                    logger.debug(f"[{call_id}] Visitante começou a falar.")
                    call_logger.log_speech_detected(is_visitor=True)
                silence_start = None
            else:
                if is_speaking:
                    # Já estava falando e agora está em silêncio
                    if silence_start is None:
                        silence_start = asyncio.get_event_loop().time()
                    else:
                        # Se passou 2s em silêncio, considera que a fala terminou
                        silence_duration = asyncio.get_event_loop().time() - silence_start
                        if silence_duration > SILENCE_THRESHOLD_SECONDS:
                            is_speaking = False
                            
                            # Se não temos frames suficientes (< 1s), provavelmente é ruído
                            if len(frames) < 50:  # ~1 segundo de áudio (50 frames de 20ms)
                                logger.debug(f"[{call_id}] Descartando fala curta demais ({len(frames)} frames)")
                                frames = []
                                continue
                            
                            # Calcular duração total da fala
                            speech_duration = (asyncio.get_event_loop().time() - speech_start) * 1000
                            logger.debug(f"[{call_id}] Visitante parou de falar após {speech_duration:.0f}ms.")
                            call_logger.log_speech_ended(speech_duration, is_visitor=True)
                            call_logger.log_silence_detected(silence_duration * 1000, is_visitor=True)
                            
                            audio_data = b"".join(frames)
                            frames.clear()

                            # Desativar escuta durante processamento para evitar retroalimentação
                            is_listening_mode = False
                            
                            # Log antes da transcrição
                            call_logger.log_transcription_start(len(audio_data), is_visitor=True)
                            
                            # Mudar estado para WAITING durante processamento
                            session.visitor_state = "WAITING"
                            
                            # Transcrever com medição de tempo e monitoramento de recursos
                            start_time = time.time()
                            texto = await transcrever_audio_async(audio_data, call_id=call_id)
                            transcription_time = (time.time() - start_time) * 1000
                            
                            if texto:
                                call_logger.log_transcription_complete(texto, transcription_time, is_visitor=True)
                                
                                # Medição do tempo de processamento da IA
                                start_time = time.time()
                                session_manager.process_visitor_text(call_id, texto)
                                ai_processing_time = (time.time() - start_time) * 1000
                                
                                call_logger.log_event("VISITOR_PROCESSING_COMPLETE", {
                                    "text": texto,
                                    "processing_time_ms": round(ai_processing_time, 2)
                                })
                                
                                # Agora, mesmo que o estado tenha mudado para IA_TURN durante o processamento,
                                # vamos respeitar isso (is_listening_mode já está False)
                            else:
                                call_logger.log_error("TRANSCRIPTION_FAILED", 
                                                    "Falha ao transcrever áudio do visitante", 
                                                    {"audio_size": len(audio_data)})
                                # Voltar ao modo de escuta, já que não conseguimos processar o áudio
                                is_listening_mode = True
        elif kind != KIND_SLIN or (len(audio_chunk) != 320 and len(audio_chunk) != 640):
            logger.warning(f"[{call_id}] Chunk inválido do visitante. kind={kind}, len={len(audio_chunk)}")
            call_logger.log_error("INVALID_CHUNK", 
                                "Chunk de áudio inválido recebido do visitante", 
                                {"kind": kind, "length": len(audio_chunk)})

    # Ao sair, encerrou a conexão
    logger.info(f"[{call_id}] receber_audio_visitante_vad terminou.")

async def receber_audio_visitante_azure_speech(reader: asyncio.StreamReader, call_id: str):
    """
    Implementação simplificada usando Azure Speech SDK para detecção de voz e silêncio.
    
    Esta implementação confia mais nos recursos nativos do Azure Speech SDK
    para detecção de fala em ambientes ruidosos, com mínima interferência local.
    """
    call_logger = CallLoggerManager.get_logger(call_id)
    
    # Acessar a sessão para verificar o estado
    session = session_manager.get_session(call_id)
    if not session:
        logger.error(f"[{call_id}] Sessão não encontrada para iniciar recebimento de áudio")
        return
    
    # Flag para descartar frames iniciais após a IA falar (anti-eco)
    discard_buffer_frames = 0
    
    # Verificar se as variáveis de ambiente necessárias estão definidas
    azure_key = os.getenv('AZURE_SPEECH_KEY')
    azure_region = os.getenv('AZURE_SPEECH_REGION')
    
    if not azure_key or not azure_region:
        logger.error(f"[{call_id}] Credenciais do Azure Speech não configuradas. Verifique as variáveis de ambiente.")
        return
    
    # Configurações do Azure Speech SDK
    speech_config = speechsdk.SpeechConfig(
        subscription=azure_key,
        region=azure_region
    )
    speech_config.speech_recognition_language = 'pt-BR'
    
    # Configurações otimizadas para detecção em ambientes ruidosos
    visitor_timeout_ms = config["system"].get("azure_speech_visitor_timeout_ms", 1000)
    speech_config.set_property(speechsdk.PropertyId.Speech_SegmentationSilenceTimeoutMs, str(visitor_timeout_ms))
    speech_config.set_property(speechsdk.PropertyId.SpeechServiceConnection_InitialSilenceTimeoutMs, "10000")
    speech_config.set_property(speechsdk.PropertyId.SpeechServiceConnection_EndSilenceTimeoutMs, str(visitor_timeout_ms))
    speech_config.set_property(speechsdk.PropertyId.SpeechServiceResponse_PostProcessingOption, "TrueText")
    # Speech_VoiceDetectionSensitivity não existe, vamos remover essa linha
    
    # Configurações para melhor qualidade e precisão
    # Verificar se o método existe antes de chamar para evitar erros
    if hasattr(speech_config, 'enable_audio_logging'):
        speech_config.enable_audio_logging()
    if hasattr(speech_config, 'enable_dictation'):
        speech_config.enable_dictation()
    if hasattr(speechsdk, 'ProfanityOption'):
        speech_config.set_profanity(speechsdk.ProfanityOption.Raw)
    
    # Criar o stream de áudio (PCM 16-bit a 8kHz) - SLIN formato
    # Configuração explícita e completa para garantir que o Azure Speech entenda corretamente o formato
    audio_format = speechsdk.audio.AudioStreamFormat(
        samples_per_second=8000,  # Crucialmente importante: 8kHz para SLIN
        bits_per_sample=16,       # 16-bit PCM
        channels=1                # mono
    )
    
    # Log detalhado do formato para debug
    logger.info(f"[{call_id}] Configurando formato de áudio: 8kHz, 16-bit, mono (SLIN)")
    
    # Criar o stream com o formato explícito
    push_stream = speechsdk.audio.PushAudioInputStream(stream_format=audio_format)
    audio_config = speechsdk.audio.AudioConfig(stream=push_stream)
    
    # Adicionar parâmetros extras de segmentação
    if hasattr(speechsdk.PropertyId, "Speech_SegmentationStrategy"):
        # Configurar estratégia de segmentação manual para melhor controle
        speech_config.set_property(speechsdk.PropertyId.Speech_SegmentationStrategy, "ManualOnly")
        logger.info(f"[{call_id}] Estratégia de segmentação configurada para: ManualOnly")
    
    # Criar o reconhecedor com o formato de áudio explícito configurado
    recognizer = speechsdk.SpeechRecognizer(speech_config=speech_config, audio_config=audio_config)
    logger.info(f"[{call_id}] Reconhecedor de fala iniciado com formato SLIN 8kHz")
    
    # Controle simplificado de reconhecimento
    is_recognition_started = False
    
    # Função para processar texto reconhecido pelo Azure
    async def process_recognized_text(text, audio_data):
        # Verificar se temos dados válidos para processar
        if not audio_data or len(audio_data) == 0:
            logger.warning(f"[{call_id}] Recebido texto sem áudio para processar")
            return
        
        # Mudar estado para WAITING durante processamento
        session.visitor_state = "WAITING"
        
        # Log antes da transcrição/processamento
        audio_size = len(audio_data)
        call_logger.log_event("PROCESSING_VISITOR_AUDIO", {
            "audio_size": audio_size,
            "text_from_azure": text if text else "(sem texto)"
        })
        
        # Se o Azure Speech transcreveu com sucesso, usar o texto diretamente
        if text and text.strip():
            logger.info(f"[{call_id}] Texto reconhecido pelo Azure: '{text}'")
            
            # Processar o texto reconhecido
            start_time = time.time()
            session_manager.process_visitor_text(call_id, text)
            processing_time = (time.time() - start_time) * 1000
            
            call_logger.log_event("VISITOR_PROCESSING_COMPLETE", {
                "text": text,
                "processing_time_ms": round(processing_time, 2),
                "source": "azure_speech"
            })
        else:
            # Se não temos texto, tentar transcrição com nosso método
            logger.info(f"[{call_id}] Azure não retornou texto, tentando transcrição alternativa para {audio_size} bytes")
            
            start_time = time.time()
            texto = await transcrever_audio_async(audio_data, call_id=call_id)
            transcription_time = (time.time() - start_time) * 1000
            
            if texto:
                logger.info(f"[{call_id}] Áudio transcrito: '{texto}' em {transcription_time:.1f}ms")
                
                # Processar o texto transcrito
                start_time = time.time()
                session_manager.process_visitor_text(call_id, texto)
                processing_time = (time.time() - start_time) * 1000
                
                call_logger.log_event("VISITOR_PROCESSING_COMPLETE", {
                    "text": texto,
                    "processing_time_ms": round(processing_time, 2),
                    "source": "fallback_transcription"
                })
            else:
                logger.error(f"[{call_id}] Falha na transcrição do áudio ({audio_size} bytes)")
                call_logger.log_error("TRANSCRIPTION_FAILED", 
                                      "Falha ao transcrever áudio do visitante", 
                                      {"audio_size": audio_size})
    
    # Criar e configurar o gerenciador de callbacks
    speech_callbacks = SpeechCallbacks(call_id, is_visitor=True, call_logger=call_logger)
    speech_callbacks.set_process_callback(process_recognized_text)
    speech_callbacks.register_callbacks(recognizer, speech_config)
    
    # Iniciar o reconhecimento contínuo
    recognizer.start_continuous_recognition_async()
    logger.info(f"[{call_id}] Iniciado reconhecimento contínuo com Azure Speech")
    
    try:
        while True:
            # Verificar sinal de terminação
            if session.terminate_visitor_event.is_set():
                logger.info(f"[{call_id}] Detectado sinal para encerrar recebimento de áudio do visitante")
                call_logger.log_event("TERMINATE_VISITOR_AUDIO", {"reason": "session_terminated"})
                break
            
            try:
                # Uso de wait_for com timeout para permitir verificação periódica de terminação
                header = await asyncio.wait_for(reader.readexactly(3), timeout=0.5)
            except asyncio.TimeoutError:
                # Timeout apenas para verificação, continuamos normalmente
                continue
            except asyncio.IncompleteReadError:
                logger.info(f"[{call_id}] Visitante desconectou (EOF)")
                call_logger.log_call_ended("visitor_disconnected")
                break

            if not header:
                logger.info(f"[{call_id}] Nenhum dado de header, encerrando")
                call_logger.log_call_ended("invalid_header")
                break

            kind = header[0]
            length = int.from_bytes(header[1:3], "big")

            audio_chunk = await reader.readexactly(length)
            
            # Verificar o estado atual da sessão
            current_state = session.visitor_state
            
            # Se a IA está falando, não processamos o áudio
            if current_state == "IA_TURN":
                # Marcar que a IA acabou de enviar áudio para evitar eco
                if hasattr(speech_callbacks, 'mark_ia_audio_sent'):
                    speech_callbacks.mark_ia_audio_sent()
                continue
                
            # Período de transição: após IA falar, descartamos alguns frames para evitar eco
            if discard_buffer_frames > 0:
                discard_buffer_frames -= 1
                continue
                
            # Se acabamos de transitar de IA_TURN para USER_TURN, preparamos para escuta
            if current_state == "USER_TURN" and not is_recognition_started:
                discard_buffer_frames = DISCARD_BUFFER_FRAMES
                is_recognition_started = True
                logger.debug(f"[{call_id}] Ativando escuta do visitante")
                continue

            # Processar apenas chunks de áudio válidos
            if kind == KIND_SLIN and (len(audio_chunk) == 320 or len(audio_chunk) == 640):
                # Iniciar reconhecimento se ainda não começou
                if not is_recognition_started:
                    is_recognition_started = True
                
                # Sempre enviar o áudio para o Azure Speech e também para o buffer local
                try:
                    # Enviar para Azure Speech primeiro
                    push_stream.write(audio_chunk)
                    
                    # Adicionar ao buffer local através do callback
                    # Importante: Isso garante que o áudio seja armazenado mesmo se o Azure não ativar a coleta
                    speech_callbacks.add_audio_chunk(audio_chunk)
                    
                    # Log para verificar tamanho do buffer a cada 20 chunks
                    if hasattr(speech_callbacks, 'audio_buffer') and len(speech_callbacks.audio_buffer) % 20 == 0:
                        logger.info(f"[{call_id}] Buffer de áudio: {len(speech_callbacks.audio_buffer)} chunks (~{len(speech_callbacks.audio_buffer)*20}ms)")
                        
                    # Verificar status de detecção e coleta
                    if hasattr(speech_callbacks, 'speech_detected') and speech_callbacks.speech_detected and not speech_callbacks.collecting_audio:
                        # Forçar coleta se detecção de fala está ativa mas coleta não
                        speech_callbacks.collecting_audio = True
                        logger.info(f"[{call_id}] Forçando coleta de áudio com speech_detected={speech_callbacks.speech_detected}")
                except Exception as e:
                    logger.error(f"[{call_id}] Erro ao processar áudio: {e}")
            
            elif kind != KIND_SLIN or (len(audio_chunk) != 320 and len(audio_chunk) != 640):
                logger.warning(f"[{call_id}] Chunk inválido do visitante. kind={kind}, len={len(audio_chunk)}")
                call_logger.log_error("INVALID_CHUNK", 
                                     "Chunk de áudio inválido recebido do visitante", 
                                     {"kind": kind, "length": len(audio_chunk)})
    
    finally:
        # Finalizar o reconhecedor
        logger.info(f"[{call_id}] Parando reconhecimento contínuo com Azure Speech")
        recognizer.stop_continuous_recognition_async()
        
    # Ao sair, encerrou a conexão
    logger.info(f"[{call_id}] receber_audio_visitante_azure_speech terminou.")


async def enviar_mensagens_visitante(writer: asyncio.StreamWriter, call_id: str):
    """
    Tarefa que periodicamente verifica se há mensagens pendentes
    para o visitante no SessionManager, sintetiza e envia via áudio.
    
    Atualiza o estado da sessão durante a fala da IA para evitar retroalimentação.
    Suporte para encerramento gracioso.
    """
    call_logger = CallLoggerManager.get_logger(call_id)
    
    # Verificar se a sessão existe
    session = session_manager.get_session(call_id)
    if not session:
        logger.error(f"[{call_id}] Sessão não encontrada para enviar mensagens")
        return
    
    # Flag para controlar encerramento com mensagem final
    final_message_sent = False
    
    while True:
        # Verificar sinal de terminação
        if session.terminate_visitor_event.is_set() and not final_message_sent:
            # Enviar mensagem de despedida e encerrar
            logger.info(f"[{call_id}] Iniciando despedida para visitante")
            await send_goodbye_and_terminate(writer, session, call_id, "visitante", call_logger)
            
            # Marcar que a mensagem de despedida foi enviada
            final_message_sent = True
            
            # Forçar encerramento imediato após despedida do visitante
            logger.info(f"[{call_id}] Forçando encerramento da sessão após despedida do visitante")
            session_manager._complete_session_termination(call_id)
            break
            
        await asyncio.sleep(0.2)  # Ajuste conforme sua necessidade

        # Se já está em modo de encerramento, não processa novas mensagens
        if session.terminate_visitor_event.is_set():
            continue

        # Tenta buscar uma mensagem
        msg = session_manager.get_message_for_visitor(call_id)
        if msg is not None:
            logger.info(f"[{call_id}] Enviando mensagem ao visitante: {msg}")
            
            # IMPORTANTE: Mudar estado para IA_TURN antes de começar a falar
            # Isso sinaliza para o VAD parar de processar durante a fala
            old_state = session.visitor_state
            session.visitor_state = "IA_TURN"
            
            call_logger.log_event("STATE_CHANGE", {
                "from": old_state,
                "to": "IA_TURN",
                "reason": "ia_speaking"
            })
            
            call_logger.log_synthesis_start(msg, is_visitor=True)
            
            # Medir tempo de síntese
            start_time = time.time()
            audio_resposta = await sintetizar_fala_async(msg)
            synthesis_time = (time.time() - start_time) * 1000
            
            # Se falhou na síntese, voltamos ao estado anterior
            if not audio_resposta:
                call_logger.log_error("SYNTHESIS_FAILED", 
                                     "Falha ao sintetizar mensagem para o visitante", 
                                     {"message": msg})
                
                # Voltar ao estado anterior
                session.visitor_state = old_state
                call_logger.log_event("STATE_CHANGE", {
                    "from": "IA_TURN",
                    "to": old_state,
                    "reason": "synthesis_failed"
                })
                continue
                
            # Síntese bem-sucedida, enviar áudio
            call_logger.log_synthesis_complete(len(audio_resposta), synthesis_time, is_visitor=True)
            
            # Tempo estimado para reprodução (baseado no tamanho do áudio)
            # A taxa de amostragem é 8000Hz com 16 bits por amostra
            # Aproximadamente (len(audio_resposta) / 16000) segundos de áudio
            playback_duration_ms = (len(audio_resposta) / 16) * 1000
            call_logger.log_event("ESTIMATED_PLAYBACK_DURATION", {
                "duration_ms": playback_duration_ms,
                "audio_size_bytes": len(audio_resposta)
            })
            
            # Enviar o áudio (isso já registra logs de envio)
            await enviar_audio(writer, audio_resposta, call_id=call_id, origem="Visitante")
            
            # Adicionar um atraso maior após o envio do áudio para garantir
            # que o áudio seja totalmente reproduzido E evitar eco/retroalimentação
            # Aumentamos para 2.0 segundos mínimo para dar mais margem de segurança
            safe_delay = max(POST_AUDIO_DELAY_SECONDS, 2.0)
            logger.info(f"[{call_id}] Aguardando {safe_delay}s após envio do áudio para evitar eco")
            await asyncio.sleep(safe_delay)
            
            # PROTEÇÃO ANTI-ECO ADICIONAL: Limpar quaisquer dados coletados durante o período
            # em que a IA estava falando - isso evita processamento de eco
            if 'speech_callbacks' in locals() or hasattr(session, 'speech_callbacks'):
                speech_callbacks_obj = speech_callbacks if 'speech_callbacks' in locals() else session.speech_callbacks
                
                if hasattr(speech_callbacks_obj, 'audio_buffer'):
                    buffer_size = len(speech_callbacks_obj.audio_buffer)
                    if buffer_size > 0:
                        logger.info(f"[{call_id}] Limpando buffer de {buffer_size} frames coletados durante fala da IA")
                        speech_callbacks_obj.audio_buffer = []
                        
                    # Resetar outros estados de detecção
                    speech_callbacks_obj.collecting_audio = False  # Será ativado novamente quando necessário
                    speech_callbacks_obj.speech_detected = False
                    
                    # Limpar quaisquer flags pendentes
                    if hasattr(speech_callbacks_obj, 'pending_processing_flag'):
                        speech_callbacks_obj.pending_processing_flag = False
                    if hasattr(speech_callbacks_obj, 'pending_audio_for_processing'):
                        speech_callbacks_obj.pending_audio_for_processing = None
                        
                    # Resetar contadores de frames após silêncio
                    if hasattr(speech_callbacks_obj, 'frames_after_silence'):
                        speech_callbacks_obj.frames_after_silence = 0
            
            # Mudar de volta para USER_TURN para que o sistema possa escutar o usuário
            session.visitor_state = "USER_TURN"
            
            # PROTEÇÃO ANTI-LOOP: Registrar timestamp de quando a IA terminou de falar
            # Isto será usado para ignorar detecções de fala muito próximas ao fim da fala da IA
            if not hasattr(session, 'last_ai_speech_end_time'):
                session.last_ai_speech_end_time = {}
            # Marcar o momento exato em que o áudio terminou de ser enviado
            session.last_ai_speech_end_time['visitor'] = time.time()
            logger.info(f"[{call_id}] Marcando timestamp de fim de fala da IA: {session.last_ai_speech_end_time['visitor']}")
            
            call_logger.log_event("STATE_CHANGE", {
                "from": "IA_TURN",
                "to": "USER_TURN",
                "reason": "ia_finished_speaking"
            })


async def iniciar_servidor_audiosocket_visitante(reader, writer):
    """
    Versão modificada que registra a porta local usada pela conexão.
    """
    # Recuperar porta local
    local_port = get_local_port(writer)
    
    # Resto do código atual
    header = await reader.readexactly(3)
    kind = header[0]
    length = int.from_bytes(header[1:3], "big")
    call_id_bytes = await reader.readexactly(length)
    
    # Converter para UUID com formato de traços
    import uuid
    call_id = str(uuid.UUID(bytes=call_id_bytes))
    
    # Registrar porta usada para este call_id
    if extension_manager and local_port:
        ext_info = extension_manager.get_extension_info(porta=local_port)
        logger.info(f"[VISITANTE] Call ID: {call_id} na porta {local_port}, ramal: {ext_info.get('ramal_ia', 'desconhecido')}")
    else:
        logger.info(f"[VISITANTE] Call ID: {call_id}")
    
    # Inicializar logger específico para esta chamada
    call_logger = CallLoggerManager.get_logger(call_id)
    call_logger.log_event("CALL_SETUP", {
        "type": "visitor",
        "call_id": call_id,
        "local_port": local_port,
        "voice_detection": VOICE_DETECTION_TYPE.value
    })

    session_manager.create_session(call_id)

    # Registrar a conexão ativa no ResourceManager para permitir KIND_HANGUP
    resource_manager.register_connection(call_id, "visitor", reader, writer)
    logger.info(f"[{call_id}] Conexão do visitante registrada no ResourceManager")

    # SAUDAÇÃO:
    welcome_msg = "Olá, seja bem-vindo! Em que posso ajudar?"
    call_logger.log_event("GREETING", {"message": welcome_msg})
    
    session_manager.enfileirar_visitor(
        call_id,
        welcome_msg
    )

    # Iniciar as tarefas de recebimento e envio de áudio
    task1 = asyncio.create_task(receber_audio_visitante(reader, call_id))
    task2 = asyncio.create_task(enviar_mensagens_visitante(writer, call_id))

    # Espera até que alguma das tarefas termine (em geral, quando visitante desconecta).
    start_time = time.time()
    done, pending = await asyncio.wait([task1, task2], return_when=asyncio.FIRST_COMPLETED)
    call_duration = (time.time() - start_time) * 1000
    
    logger.info(f"[{call_id}] Alguma tarefa finalizou, vamos encerrar as duas...")
    call_logger.log_event("TASKS_ENDING", {
        "done_tasks": len(done),
        "pending_tasks": len(pending),
        "call_duration_ms": round(call_duration, 2)
    })

    # Cancela a outra
    for t in pending:
        t.cancel()
    await asyncio.gather(*pending, return_exceptions=True)

    logger.info(f"[{call_id}] Encerrando conexão do visitante.")
    call_logger.log_call_ended("visitor_connection_closed", call_duration)
    
    # Remover conexão do ResourceManager
    resource_manager.unregister_connection(call_id, "visitor")
    
    # Remover logger para liberar recursos
    CallLoggerManager.remove_logger(call_id)
    
    # Tratar fechamento do socket com robustez para lidar com desconexões abruptas
    try:
        writer.close()
        # Usar um timeout para wait_closed para evitar bloqueio indefinido 
        # em caso de desconexão súbita (Connection reset by peer)
        await asyncio.wait_for(writer.wait_closed(), timeout=2.0)
    except asyncio.TimeoutError:
        logger.info(f"[{call_id}] Timeout ao aguardar fechamento do socket - provavelmente já foi fechado pelo cliente")
    except ConnectionResetError:
        # Isso é esperado se o cliente desconectar abruptamente após receber KIND_HANGUP
        logger.info(f"[{call_id}] Conexão resetada pelo cliente após KIND_HANGUP - comportamento normal")
    except Exception as e:
        # Capturar qualquer outro erro durante o fechamento da conexão
        logger.warning(f"[{call_id}] Erro ao fechar conexão: {str(e)}")
    
    logger.info(f"[{call_id}] Socket encerrado e liberado para novas conexões")


# ------------------------
# MORADOR
# ------------------------

async def receber_audio_morador(reader: asyncio.StreamReader, call_id: str):
    """
    Função que redireciona para a implementação apropriada com base na configuração.
    """
    # Escolher a implementação com base na configuração
    if VOICE_DETECTION_TYPE == VoiceDetectionType.WEBRTCVAD:
        return await receber_audio_morador_vad(reader, call_id)
    elif VOICE_DETECTION_TYPE == VoiceDetectionType.AZURE_SPEECH:
        return await receber_audio_morador_azure_speech(reader, call_id)
    else:
        # Fallback para webrtcvad se o tipo não for reconhecido
        logger.warning(f"[{call_id}] Tipo de detecção de voz '{VOICE_DETECTION_TYPE}' não reconhecido, usando webrtcvad")
        return await receber_audio_morador_vad(reader, call_id)

async def receber_audio_morador_vad(reader: asyncio.StreamReader, call_id: str):
    """
    Implementação usando webrtcvad para detecção de voz e silêncio do morador.
    """
    call_logger = CallLoggerManager.get_logger(call_id)
    vad = webrtcvad.Vad(3)  # Aumentando a sensibilidade do VAD para detectar falas curtas
    frames = []
    is_speaking = False
    silence_start = None
    speech_start = None
    
    # Para controlar se estamos no modo de escuta ativa
    is_listening_mode = True
    
    # Acessar a sessão para verificar o estado
    session = session_manager.get_session(call_id)
    if not session:
        logger.error(f"[{call_id}] Sessão não encontrada para iniciar recebimento de áudio do morador")
        return
    
    # Flag de buffer para descartar áudio residual após IA falar
    discard_buffer_frames = 0

    while True:
        # Verificar sinal de terminação
        if session.terminate_resident_event.is_set():
            logger.info(f"[{call_id}] Detectado sinal para encerrar recebimento de áudio do morador")
            call_logger.log_event("TERMINATE_RESIDENT_AUDIO", {
                "reason": "session_terminated",
                "timestamp": time.time()
            })
            break
        
        try:
            # Uso de wait_for com timeout para permitir verificação de terminação
            header = await asyncio.wait_for(reader.readexactly(3), timeout=0.5)
        except asyncio.TimeoutError:
            # Timeout apenas para verificação de terminação, continuamos normalmente
            continue
        except asyncio.IncompleteReadError:
            logger.info(f"[{call_id}] Morador desconectou (EOF).")
            call_logger.log_call_ended("resident_disconnected")
            break

        if not header:
            logger.info(f"[{call_id}] Nenhum dado de header, encerrando (morador).")
            call_logger.log_call_ended("invalid_header_resident")
            break

        kind = header[0]
        length = int.from_bytes(header[1:3], "big")

        audio_chunk = await reader.readexactly(length)
        
        # Verificar o estado atual da sessão
        current_state = session.resident_state
        
        # Se estamos em IA_TURN, significa que a IA está falando com o morador - não processamos
        if current_state == "IA_TURN":
            is_listening_mode = False
            continue  # Pula processamento durante fala da IA
            
        # Período de transição: após IA falar, descartamos alguns frames para evitar eco
        if discard_buffer_frames > 0:
            discard_buffer_frames -= 1
            continue
            
        # Se acabamos de transitar de IA_TURN para USER_TURN, ativamos modo de escuta
        if not is_listening_mode and current_state == "USER_TURN":
            is_listening_mode = True
            discard_buffer_frames = DISCARD_BUFFER_FRAMES  # Descartar quadros para evitar eco
            logger.debug(f"[{call_id}] Ativando modo de escuta de morador")
            call_logger.log_event("RESIDENT_LISTENING_MODE_ACTIVATED", {"timestamp": time.time()})
            
            # Limpar quaisquer frames acumulados anteriormente
            frames = []
            is_speaking = False
            silence_start = None
            speech_start = None
            continue
        
        # Processamos VAD apenas quando estamos em modo de escuta
        if is_listening_mode and kind == KIND_SLIN and (len(audio_chunk) == 320 or len(audio_chunk) == 640):
            # Para morador, usamos uma detecção mais agressiva para palavras curtas como "Sim"
            # Se o chunk for de 640 bytes (320 amostras de 16 bits), precisamos garantir que seja processado corretamente
            chunk_to_process = audio_chunk
            if len(audio_chunk) == 640:
                # O WebRTCVAD espera PCM de 16 bits em 320 bytes, então estamos recebendo o dobro do tamanho esperado
                logger.debug(f"[{call_id}] Recebido chunk de 640 bytes do morador - formato PCM 16-bit")
                
            is_voice = vad.is_speech(chunk_to_process, 8000)
            if is_voice:
                frames.append(audio_chunk)
                if not is_speaking:
                    is_speaking = True
                    speech_start = asyncio.get_event_loop().time()
                    logger.info(f"[{call_id}] Morador começou a falar.")  # Aumentado para INFO para melhor visibilidade
                    call_logger.log_speech_detected(is_visitor=False)
                silence_start = None
            else:
                if is_speaking:
                    if silence_start is None:
                        silence_start = asyncio.get_event_loop().time()
                    else:
                        silence_duration = asyncio.get_event_loop().time() - silence_start
                        # Usar um tempo de silêncio muito mais curto para respostas rápidas do morador
                        if silence_duration > SILENCE_THRESHOLD_SECONDS:
                            is_speaking = False
                            logger.info(f"[{call_id}] Silêncio de {silence_duration:.2f}s detectado após fala do morador")
                            
                            # Mesmo com fala muito curta, processamos, pois pode ser um "Sim" rápido
                            if len(frames) < 20:  # ~0.4 segundo de áudio (20 frames de 20ms)
                                logger.info(f"[{call_id}] Fala CURTA do morador detectada: {len(frames)} frames (~{len(frames)*20}ms) - Processando mesmo assim")
                                # NÃO descartamos frames curtos para capturar "Sim" rápidos
                            
                            # Calcular duração total da fala
                            speech_duration = (asyncio.get_event_loop().time() - speech_start) * 1000
                            logger.debug(f"[{call_id}] Morador parou de falar após {speech_duration:.0f}ms.")
                            call_logger.log_speech_ended(speech_duration, is_visitor=False)
                            call_logger.log_silence_detected(silence_duration * 1000, is_visitor=False)
                            
                            audio_data = b"".join(frames)
                            frames.clear()
                            
                            # Desativar escuta durante processamento
                            is_listening_mode = False

                            # Log antes da transcrição
                            call_logger.log_transcription_start(len(audio_data), is_visitor=False)
                            
                            # Mudar estado para WAITING durante processamento
                            session.resident_state = "WAITING"
                            
                            # Transcrever com medição de tempo e monitoramento de recursos
                            start_time = time.time()
                            texto = await transcrever_audio_async(audio_data, call_id=call_id)
                            transcription_time = (time.time() - start_time) * 1000
                            
                            if texto:
                                call_logger.log_transcription_complete(texto, transcription_time, is_visitor=False)
                                
                                # Medição do tempo de processamento
                                start_time = time.time()
                                session_manager.process_resident_text(call_id, texto)
                                processing_time = (time.time() - start_time) * 1000
                                
                                call_logger.log_event("RESIDENT_PROCESSING_COMPLETE", {
                                    "text": texto,
                                    "processing_time_ms": round(processing_time, 2)
                                })
                                
                                logger.info(f"[{call_id}] Resposta do morador processada: '{texto}'")
                                
                                # Verificar se a sessão tem flag de finalização após processamento
                                session = session_manager.get_session(call_id)
                                if session and hasattr(session.flow, 'state'):
                                    flow_state = session.flow.state
                                    if str(flow_state) == 'FlowState.FINALIZADO':
                                        logger.info(f"[{call_id}] Flow detectado como FINALIZADO após resposta do morador")
                                        
                                        # Garantir que o visitor receba notificação da autorização
                                        intent_data = session.flow.intent_data if hasattr(session.flow, 'intent_data') else {}
                                        authorization_result = intent_data.get("authorization_result", "")
                                        intent_type = intent_data.get("intent_type", "entrada")
                                        
                                        if authorization_result == "authorized":
                                            # Enviar mensagem explícita ao visitante sobre autorização
                                            if intent_type == "entrega":
                                                visitor_msg = "Ótima notícia! O morador autorizou sua entrega."
                                            elif intent_type == "visita":
                                                visitor_msg = "Ótima notícia! O morador autorizou sua visita."
                                            else:
                                                visitor_msg = "Ótima notícia! O morador autorizou sua entrada."
                                            
                                            logger.info(f"[{call_id}] Notificando visitante explicitamente da autorização: {visitor_msg}")
                                            session_manager.enfileirar_visitor(call_id, visitor_msg)
                                            
                                            # Forçar mensagem final - essencial para fechar o ciclo
                                            final_msg = f"Sua {intent_type} foi autorizada pelo morador. Obrigado por utilizar nossa portaria inteligente."
                                            session_manager.enfileirar_visitor(call_id, final_msg)
                                            
                                            # Não finalizar a sessão imediatamente, permitir que as mensagens sejam enviadas
                                            # Agendamos um encerramento após um delay longo para garantir que todas as mensagens sejam ouvidas
                                            logger.info(f"[{call_id}] Agendando encerramento da sessão em 10 segundos após autorização do morador")
                                            asyncio.create_task(_encerrar_apos_delay(call_id, session_manager, 10.0))  # Delay mais longo para garantir processamento completo
                                        elif authorization_result == "denied":
                                            # Enviar mensagem explícita ao visitante sobre negação
                                            visitor_msg = f"Infelizmente o morador não autorizou sua {intent_type if intent_type else 'entrada'} neste momento."
                                            logger.info(f"[{call_id}] Notificando visitante explicitamente da negação: {visitor_msg}")
                                            session_manager.enfileirar_visitor(call_id, visitor_msg)
                                            
                                            # Forçar mensagem final - essencial para fechar o ciclo
                                            final_msg = f"Sua {intent_type} NÃO foi autorizada pelo morador. Obrigado por utilizar nossa portaria inteligente."
                                            session_manager.enfileirar_visitor(call_id, final_msg)
                                            
                                            # Não finalizar a sessão imediatamente, permitir que as mensagens sejam enviadas
                                            # Agendamos um encerramento após um delay longo para garantir que todas as mensagens sejam ouvidas
                                            logger.info(f"[{call_id}] Agendando encerramento da sessão em 10 segundos após negação do morador")
                                            asyncio.create_task(_encerrar_apos_delay(call_id, session_manager, 10.0))  # Delay mais longo para garantir processamento completo
                            else:
                                call_logger.log_error("TRANSCRIPTION_FAILED", 
                                                    "Falha ao transcrever áudio do morador", 
                                                    {"audio_size": len(audio_data)})
                                # Voltar ao modo de escuta, já que não foi possível processar
                                is_listening_mode = True
        elif kind != KIND_SLIN or (len(audio_chunk) != 320 and len(audio_chunk) != 640):
            logger.warning(f"[{call_id}] Chunk inválido do morador. kind={kind}, len={len(audio_chunk)}")
            call_logger.log_error("INVALID_CHUNK", 
                                "Chunk de áudio inválido recebido do morador", 
                                {"kind": kind, "length": len(audio_chunk)})

    logger.info(f"[{call_id}] receber_audio_morador_vad terminou.")

async def receber_audio_morador_azure_speech(reader: asyncio.StreamReader, call_id: str):
    """
    Implementação simplificada usando Azure Speech SDK para detecção de voz do morador.
    
    Otimizada para respostas curtas como "sim" e "não", esta implementação confia mais
    nos recursos nativos do Azure Speech para detecção de fala com mínima interferência local.
    """
    call_logger = CallLoggerManager.get_logger(call_id)
    
    # Acessar a sessão para verificar o estado
    session = session_manager.get_session(call_id)
    if not session:
        logger.error(f"[{call_id}] Sessão não encontrada para iniciar recebimento de áudio do morador")
        return
    
    # Flag para descartar frames iniciais após a IA falar (anti-eco)
    discard_buffer_frames = 0
    
    # Verificar se as variáveis de ambiente necessárias estão definidas
    azure_key = os.getenv('AZURE_SPEECH_KEY')
    azure_region = os.getenv('AZURE_SPEECH_REGION')
    
    if not azure_key or not azure_region:
        logger.error(f"[{call_id}] Credenciais do Azure Speech não configuradas. Verifique as variáveis de ambiente.")
        return
    
    # Configurações do Azure Speech SDK
    speech_config = speechsdk.SpeechConfig(
        subscription=azure_key,
        region=azure_region
    )
    speech_config.speech_recognition_language = 'pt-BR'
    
    # Configurações otimizadas para detecção de respostas curtas do morador
    resident_timeout_ms = config["system"].get("azure_speech_resident_timeout_ms", 600)
    speech_config.set_property(speechsdk.PropertyId.Speech_SegmentationSilenceTimeoutMs, str(resident_timeout_ms))
    speech_config.set_property(speechsdk.PropertyId.SpeechServiceConnection_InitialSilenceTimeoutMs, "10000")
    speech_config.set_property(speechsdk.PropertyId.SpeechServiceConnection_EndSilenceTimeoutMs, str(resident_timeout_ms))
    speech_config.set_property(speechsdk.PropertyId.SpeechServiceResponse_PostProcessingOption, "TrueText")
    
    # A propriedade Speech_VoiceDetectionSensitivity não existe, não vamos configurá-la
    # Ajuste para detectar respostas curtas
    speech_config.set_property(speechsdk.PropertyId.Speech_SegmentationSilenceTimeoutMs, str(resident_timeout_ms))
    speech_config.set_property(speechsdk.PropertyId.SpeechServiceConnection_EndSilenceTimeoutMs, str(resident_timeout_ms))
    
    # Configurações para melhor qualidade e precisão
    # Verificar se o método existe antes de chamar para evitar erros
    if hasattr(speech_config, 'enable_audio_logging'):
        speech_config.enable_audio_logging()
    if hasattr(speech_config, 'enable_dictation'):
        speech_config.enable_dictation()
    if hasattr(speechsdk, 'ProfanityOption'):
        speech_config.set_profanity(speechsdk.ProfanityOption.Raw)
    
    # Criar o stream de áudio (PCM 16-bit a 8kHz) - SLIN formato
    # Configuração explícita e completa para garantir que o Azure Speech entenda corretamente o formato
    audio_format = speechsdk.audio.AudioStreamFormat(
        samples_per_second=8000,  # Crucialmente importante: 8kHz para SLIN
        bits_per_sample=16,       # 16-bit PCM
        channels=1                # mono
    )
    
    # Log detalhado do formato para debug
    logger.info(f"[{call_id}] Configurando formato de áudio: 8kHz, 16-bit, mono (SLIN)")
    
    # Criar o stream com o formato explícito
    push_stream = speechsdk.audio.PushAudioInputStream(stream_format=audio_format)
    audio_config = speechsdk.audio.AudioConfig(stream=push_stream)
    
    # Adicionar parâmetros extras de segmentação
    if hasattr(speechsdk.PropertyId, "Speech_SegmentationStrategy"):
        # Configurar estratégia de segmentação manual para melhor controle
        speech_config.set_property(speechsdk.PropertyId.Speech_SegmentationStrategy, "ManualOnly")
        logger.info(f"[{call_id}] Estratégia de segmentação configurada para: ManualOnly")
    
    # Criar o reconhecedor com o formato de áudio explícito configurado
    recognizer = speechsdk.SpeechRecognizer(speech_config=speech_config, audio_config=audio_config)
    logger.info(f"[{call_id}] Reconhecedor de fala iniciado com formato SLIN 8kHz")
    
    # Controle simplificado de reconhecimento
    is_recognition_started = False
    
    # Função para processar texto reconhecido pelo Azure
    async def process_recognized_text(text, audio_data):
        # Verificar se temos dados válidos para processar
        if not audio_data or len(audio_data) == 0:
            logger.warning(f"[{call_id}] Recebido texto sem áudio do morador para processar")
            return
        
        # Mudar estado para WAITING durante processamento
        session.resident_state = "WAITING"
        
        # Log antes da transcrição/processamento
        audio_size = len(audio_data)
        call_logger.log_event("PROCESSING_RESIDENT_AUDIO", {
            "audio_size": audio_size,
            "text_from_azure": text if text else "(sem texto)"
        })
        
        # Otimização para respostas curtas do morador
        is_short_audio = audio_size < 8000  # ~0.5 segundo
        
        # Se o Azure Speech transcreveu com sucesso, usar o texto diretamente
        if text and text.strip():
            text = text.lower()  # Normalizar para comparação
            logger.info(f"[{call_id}] Texto reconhecido do morador: '{text}'")
            
            # Verificar se é uma resposta comum ("sim", "não", etc.)
            common_responses = ["sim", "não", "nao", "ok", "pode", "liberado", "autorizado"]
            if any(resp in text for resp in common_responses):
                logger.info(f"[{call_id}] Resposta comum do morador detectada: '{text}'")
            
            # Processar o texto reconhecido
            start_time = time.time()
            session_manager.process_resident_text(call_id, text)
            processing_time = (time.time() - start_time) * 1000
            
            call_logger.log_event("RESIDENT_PROCESSING_COMPLETE", {
                "text": text,
                "processing_time_ms": round(processing_time, 2),
                "source": "azure_speech"
            })
        else:
            # Se não temos texto mas temos áudio curto, provavelmente é uma resposta rápida como "sim"
            if is_short_audio:
                logger.info(f"[{call_id}] Áudio curto do morador ({audio_size} bytes) - considerando como 'sim'")
                texto = "sim"  # Resposta padrão para áudios curtos do morador
                
                # Processar o texto presumido
                start_time = time.time()
                session_manager.process_resident_text(call_id, texto)
                processing_time = (time.time() - start_time) * 1000
                
                call_logger.log_event("RESIDENT_PROCESSING_COMPLETE", {
                    "text": texto,
                    "processing_time_ms": round(processing_time, 2),
                    "source": "short_audio_fallback"
                })
            else:
                # Para áudios mais longos, tentar transcrição alternativa
                logger.info(f"[{call_id}] Tentando transcrição alternativa para {audio_size} bytes de áudio do morador")
                
                start_time = time.time()
                texto = await transcrever_audio_async(audio_data, call_id=call_id)
                transcription_time = (time.time() - start_time) * 1000
                
                if texto:
                    logger.info(f"[{call_id}] Áudio do morador transcrito: '{texto}' em {transcription_time:.1f}ms")
                    
                    # Processar o texto transcrito
                    start_time = time.time()
                    session_manager.process_resident_text(call_id, texto)
                    processing_time = (time.time() - start_time) * 1000
                    
                    call_logger.log_event("RESIDENT_PROCESSING_COMPLETE", {
                        "text": texto,
                        "processing_time_ms": round(processing_time, 2),
                        "source": "fallback_transcription"
                    })
                else:
                    logger.error(f"[{call_id}] Falha na transcrição do áudio do morador ({audio_size} bytes)")
                    call_logger.log_error("TRANSCRIPTION_FAILED", 
                                         "Falha ao transcrever áudio do morador", 
                                         {"audio_size": audio_size})
        
        # Verificar se a sessão tem flag de finalização após processamento
        session = session_manager.get_session(call_id)
        if session and hasattr(session.flow, 'state'):
            flow_state = session.flow.state
            if str(flow_state) == 'FlowState.FINALIZADO':
                logger.info(f"[{call_id}] Flow detectado como FINALIZADO após resposta do morador")
                
                # Garantir que o visitor receba notificação da autorização
                intent_data = session.flow.intent_data if hasattr(session.flow, 'intent_data') else {}
                authorization_result = intent_data.get("authorization_result", "")
                intent_type = intent_data.get("intent_type", "entrada")
                
                if authorization_result == "authorized":
                    # Enviar mensagem explícita ao visitante sobre autorização
                    if intent_type == "entrega":
                        visitor_msg = "Ótima notícia! O morador autorizou sua entrega."
                    elif intent_type == "visita":
                        visitor_msg = "Ótima notícia! O morador autorizou sua visita."
                    else:
                        visitor_msg = "Ótima notícia! O morador autorizou sua entrada."
                    
                    logger.info(f"[{call_id}] Notificando visitante sobre autorização: {visitor_msg}")
                    session_manager.enfileirar_visitor(call_id, visitor_msg)
                    
                    # Forçar mensagem final
                    final_msg = f"Sua {intent_type} foi autorizada pelo morador. Obrigado por utilizar nossa portaria inteligente."
                    session_manager.enfileirar_visitor(call_id, final_msg)
                    
                    # Agendar encerramento após um delay para garantir que mensagens sejam ouvidas
                    logger.info(f"[{call_id}] Agendando encerramento da sessão após autorização")
                    asyncio.create_task(_encerrar_apos_delay(call_id, session_manager, 10.0))
                elif authorization_result == "denied":
                    # Enviar mensagem explícita ao visitante sobre negação
                    visitor_msg = f"Infelizmente o morador não autorizou sua {intent_type if intent_type else 'entrada'}."
                    logger.info(f"[{call_id}] Notificando visitante sobre negação: {visitor_msg}")
                    session_manager.enfileirar_visitor(call_id, visitor_msg)
                    
                    # Forçar mensagem final
                    final_msg = f"Sua {intent_type} não foi autorizada. Obrigado por utilizar nossa portaria inteligente."
                    session_manager.enfileirar_visitor(call_id, final_msg)
                    
                    # Agendar encerramento após um delay
                    logger.info(f"[{call_id}] Agendando encerramento da sessão após negação")
                    asyncio.create_task(_encerrar_apos_delay(call_id, session_manager, 10.0))
    
    # Criar e configurar o gerenciador de callbacks
    resident_speech_callbacks = SpeechCallbacks(call_id, is_visitor=False, call_logger=call_logger)
    resident_speech_callbacks.set_process_callback(process_recognized_text)
    resident_speech_callbacks.register_callbacks(recognizer, speech_config)
    
    # Salvar na sessão para acessar de outras partes do código
    session.resident_speech_callbacks = resident_speech_callbacks
    
    # Iniciar o reconhecimento contínuo
    recognizer.start_continuous_recognition_async()
    logger.info(f"[{call_id}] Iniciado reconhecimento contínuo com Azure Speech para morador")
    
    try:
        while True:
            # Verificar sinal de terminação
            if session.terminate_resident_event.is_set():
                logger.info(f"[{call_id}] Detectado sinal para encerrar recebimento de áudio do morador")
                call_logger.log_event("TERMINATE_RESIDENT_AUDIO", {"reason": "session_terminated"})
                break
            
            try:
                # Uso de wait_for com timeout para permitir verificação periódica de terminação
                header = await asyncio.wait_for(reader.readexactly(3), timeout=0.5)
            except asyncio.TimeoutError:
                # Timeout apenas para verificação, continuamos normalmente
                continue
            except asyncio.IncompleteReadError:
                logger.info(f"[{call_id}] Morador desconectou (EOF)")
                call_logger.log_call_ended("resident_disconnected")
                break

            if not header:
                logger.info(f"[{call_id}] Nenhum dado de header do morador, encerrando")
                call_logger.log_call_ended("invalid_header_resident")
                break

            kind = header[0]
            length = int.from_bytes(header[1:3], "big")

            audio_chunk = await reader.readexactly(length)
            
            # Verificar o estado atual da sessão
            current_state = session.resident_state
            
            # Se a IA está falando, não processamos o áudio
            if current_state == "IA_TURN":
                # Marcar que a IA acabou de enviar áudio para evitar eco
                if hasattr(resident_speech_callbacks, 'mark_ia_audio_sent'):
                    resident_speech_callbacks.mark_ia_audio_sent()
                continue
                
            # Período de transição: após IA falar, descartamos alguns frames para evitar eco
            if discard_buffer_frames > 0:
                discard_buffer_frames -= 1
                continue
                
            # Se acabamos de transitar de IA_TURN para USER_TURN, preparamos para escuta
            if current_state == "USER_TURN" and not is_recognition_started:
                discard_buffer_frames = DISCARD_BUFFER_FRAMES
                is_recognition_started = True
                logger.debug(f"[{call_id}] Ativando escuta do morador")
                continue

            # Processar apenas chunks de áudio válidos
            if kind == KIND_SLIN and (len(audio_chunk) == 320 or len(audio_chunk) == 640):
                # Iniciar reconhecimento se ainda não começou
                if not is_recognition_started:
                    is_recognition_started = True
                
                # Sempre enviar o áudio para o Azure Speech
                try:
                    push_stream.write(audio_chunk)
                except Exception as e:
                    logger.error(f"[{call_id}] Erro ao enviar áudio do morador para Azure Speech: {e}")
            
            elif kind != KIND_SLIN or (len(audio_chunk) != 320 and len(audio_chunk) != 640):
                logger.warning(f"[{call_id}] Chunk inválido do morador. kind={kind}, len={len(audio_chunk)}")
                call_logger.log_error("INVALID_CHUNK", 
                                     "Chunk de áudio inválido recebido do morador", 
                                     {"kind": kind, "length": len(audio_chunk)})
    
    finally:
        # Finalizar o reconhecedor
        logger.info(f"[{call_id}] Parando reconhecimento contínuo com Azure Speech para morador")
        recognizer.stop_continuous_recognition_async()
        
    # Ao sair, encerrou a conexão
    logger.info(f"[{call_id}] receber_audio_morador_azure_speech terminou.")


async def enviar_mensagens_morador(writer: asyncio.StreamWriter, call_id: str):
    """
    Fica buscando mensagens para o morador, sintetiza e envia via áudio.
    
    Atualiza o estado da sessão durante a fala da IA para evitar retroalimentação.
    Suporte para encerramento gracioso.
    """
    call_logger = CallLoggerManager.get_logger(call_id)
    
    # Verificar se a sessão existe
    session = session_manager.get_session(call_id)
    if not session:
        logger.error(f"[{call_id}] Sessão não encontrada para enviar mensagens ao morador")
        return
    
    # Flag para controlar encerramento com mensagem final
    final_message_sent = False
    
    while True:
        # Verificar sinal de terminação
        if session.terminate_resident_event.is_set() and not final_message_sent:
            # Enviar mensagem de despedida e encerrar
            logger.info(f"[{call_id}] Iniciando despedida para morador")
            await send_goodbye_and_terminate(writer, session, call_id, "morador", call_logger)
            
            # Marcar que a mensagem de despedida foi enviada
            final_message_sent = True
            
            # NÃO forçar encerramento completo aqui
            # Apenas registrar que o morador foi desconectado, 
            # deixar que o visitante receba suas mensagens e encerre a sessão
            logger.info(f"[{call_id}] Conexão do morador encerrada, aguardando ciclo do visitante")
            
            break
        
        await asyncio.sleep(0.2)
        
        # Se já está em modo de encerramento, não processa novas mensagens
        if session.terminate_resident_event.is_set():
            continue
            
        msg = session_manager.get_message_for_resident(call_id)
        if msg is not None:
            logger.info(f"[{call_id}] Enviando mensagem ao morador: {msg}")
            
            # IMPORTANTE: Mudar estado para IA_TURN antes de começar a falar
            # Isso sinaliza para o VAD parar de processar durante a fala
            old_state = session.resident_state
            session.resident_state = "IA_TURN"
            
            call_logger.log_event("RESIDENT_STATE_CHANGE", {
                "from": old_state,
                "to": "IA_TURN",
                "reason": "ia_speaking_to_resident"
            })
            
            call_logger.log_synthesis_start(msg, is_visitor=False)
            
            # Medir tempo de síntese
            start_time = time.time()
            audio_resposta = await sintetizar_fala_async(msg)
            synthesis_time = (time.time() - start_time) * 1000
            
            # Se falhou na síntese, voltamos ao estado anterior
            if not audio_resposta:
                call_logger.log_error("SYNTHESIS_FAILED", 
                                     "Falha ao sintetizar mensagem para o morador", 
                                     {"message": msg})
                
                # Voltar ao estado anterior
                session.resident_state = old_state
                call_logger.log_event("RESIDENT_STATE_CHANGE", {
                    "from": "IA_TURN",
                    "to": old_state,
                    "reason": "synthesis_failed"
                })
                continue
                
            # Síntese bem-sucedida, enviar áudio
            call_logger.log_synthesis_complete(len(audio_resposta), synthesis_time, is_visitor=False)
            
            # Tempo estimado para reprodução (baseado no tamanho do áudio)
            # A taxa de amostragem é 8000Hz com 16 bits por amostra
            # Aproximadamente (len(audio_resposta) / 16000) segundos de áudio
            playback_duration_ms = (len(audio_resposta) / 16) * 1000
            call_logger.log_event("RESIDENT_ESTIMATED_PLAYBACK_DURATION", {
                "duration_ms": playback_duration_ms,
                "audio_size_bytes": len(audio_resposta)
            })
            
            # Enviar o áudio (isso já registra logs de envio)
            await enviar_audio(writer, audio_resposta, call_id=call_id, origem="Morador")
            
            # Adicionar um atraso maior após o envio do áudio para garantir
            # que o áudio seja totalmente reproduzido E evitar eco/retroalimentação
            # Aumentamos para 2.0 segundos mínimo para dar mais margem de segurança
            safe_delay = max(POST_AUDIO_DELAY_SECONDS, 2.0)
            logger.info(f"[{call_id}] Aguardando {safe_delay}s após envio do áudio para evitar eco")
            await asyncio.sleep(safe_delay)
            
            # PROTEÇÃO ANTI-ECO ADICIONAL: Limpar quaisquer dados coletados durante o período
            # em que a IA estava falando - isso evita processamento de eco
            if 'resident_speech_callbacks' in locals() or hasattr(session, 'resident_speech_callbacks'):
                speech_callbacks_obj = resident_speech_callbacks if 'resident_speech_callbacks' in locals() else session.resident_speech_callbacks
                
                if hasattr(speech_callbacks_obj, 'audio_buffer'):
                    buffer_size = len(speech_callbacks_obj.audio_buffer)
                    if buffer_size > 0:
                        logger.info(f"[{call_id}] Limpando buffer de {buffer_size} frames coletados durante fala da IA para o morador")
                        speech_callbacks_obj.audio_buffer = []
                        
                    # Resetar outros estados de detecção
                    speech_callbacks_obj.collecting_audio = False  # Será ativado novamente quando necessário
                    speech_callbacks_obj.speech_detected = False
                    
                    # Limpar quaisquer flags pendentes
                    if hasattr(speech_callbacks_obj, 'pending_processing_flag'):
                        speech_callbacks_obj.pending_processing_flag = False
                    if hasattr(speech_callbacks_obj, 'pending_audio_for_processing'):
                        speech_callbacks_obj.pending_audio_for_processing = None
                        
                    # Resetar contadores de frames após silêncio
                    if hasattr(speech_callbacks_obj, 'frames_after_silence'):
                        speech_callbacks_obj.frames_after_silence = 0
            
            # Mudar de volta para USER_TURN para que o sistema possa escutar o morador
            session.resident_state = "USER_TURN"
            call_logger.log_event("RESIDENT_STATE_CHANGE", {
                "from": "IA_TURN",
                "to": "USER_TURN",
                "reason": "ia_finished_speaking_to_resident"
            })


async def iniciar_servidor_audiosocket_morador(reader, writer):
    """
    Versão modificada que registra a porta local usada pela conexão.
    """
    # Recuperar porta local
    local_port = get_local_port(writer)
    
    # Resto do código atual
    header = await reader.readexactly(3)
    kind = header[0]
    length = int.from_bytes(header[1:3], "big")
    call_id_bytes = await reader.readexactly(length)
    
    # Converter para UUID com formato de traços
    import uuid
    call_id = str(uuid.UUID(bytes=call_id_bytes))
    
    # Registrar porta usada para este call_id
    if extension_manager and local_port:
        ext_info = extension_manager.get_extension_info(porta=local_port)
        logger.info(f"[MORADOR] Call ID: {call_id} na porta {local_port}, ramal: {ext_info.get('ramal_retorno', 'desconhecido')}")
    else:
        logger.info(f"[MORADOR] Call ID: {call_id}")
    
    # Inicializar logger específico para esta chamada
    call_logger = CallLoggerManager.get_logger(call_id)
    call_logger.log_event("CALL_SETUP", {
        "type": "resident",
        "call_id": call_id,
        "local_port": local_port,
        "voice_detection": VOICE_DETECTION_TYPE.value
    })

    # Registrar a conexão ativa no ResourceManager para permitir KIND_HANGUP
    resource_manager.register_connection(call_id, "resident", reader, writer)
    logger.info(f"[{call_id}] Conexão do morador registrada no ResourceManager")

    # Verificar se sessão já existe (deve existir se o fluxo estiver correto)
    existing_session = session_manager.get_session(call_id)
    if not existing_session:
        logger.warning(f"[MORADOR] Call ID {call_id} não encontrado como sessão existente. Criando nova sessão.")
        
        # Criar uma nova sessão - isto não deveria acontecer em circunstâncias normais
        session = session_manager.create_session(call_id)
        
        # SAUDAÇÃO MORADOR para nova sessão:
        welcome_msg = "Olá, morador! Você está em ligação com a portaria inteligente."
        call_logger.log_event("GREETING_RESIDENT", {"message": welcome_msg})
        session_manager.enfileirar_resident(call_id, welcome_msg)
        
        # Atualizar estado do morador para USER_TURN para permitir interação inicial
        session.resident_state = "USER_TURN"
    else:
        logger.info(f"[MORADOR] Sessão existente encontrada para Call ID: {call_id}. Conectando morador ao fluxo existente.")
        
        # Transferir intent_data para a sessão do morador se necessário
        if hasattr(existing_session.flow, 'intent_data') and existing_session.flow.intent_data:
            existing_session.intent_data = existing_session.flow.intent_data
            logger.info(f"[MORADOR] Intent data transferido para sessão: {existing_session.intent_data}")
            
        # Atualizar estado do morador para USER_TURN para permitir interação
        existing_session.resident_state = "USER_TURN"
        
        # IMPORTANTE: Indicar ao conversation_flow que o morador atendeu
        try:
            flow = existing_session.flow
            previous_state = flow.state
            
            # Este é o trigger que indica que o morador atendeu
            call_logger.log_event("MORADOR_CONNECTED", {
                "voip_number": flow.voip_number_morador if hasattr(flow, 'voip_number_morador') else "unknown",
                "previous_state": previous_state.name if hasattr(previous_state, 'name') else str(previous_state)
            })
            
            # Simular primeira mensagem do morador para acionar o fluxo
            session_manager.process_resident_text(call_id, "AUDIO_CONNECTION_ESTABLISHED")
            
            logger.info(f"[MORADOR] Conexão de áudio estabelecida, notificado o flow para iniciar comunicação")
        except Exception as e:
            logger.error(f"[MORADOR] Erro ao notificar atendimento: {e}", exc_info=True)

    # Iniciar as tarefas de recebimento e envio de áudio
    task1 = asyncio.create_task(receber_audio_morador(reader, call_id))
    task2 = asyncio.create_task(enviar_mensagens_morador(writer, call_id))

    start_time = time.time()
    done, pending = await asyncio.wait([task1, task2], return_when=asyncio.FIRST_COMPLETED)
    call_duration = (time.time() - start_time) * 1000
    
    logger.info(f"[{call_id}] Alguma tarefa (morador) finalizou, encerrar.")
    call_logger.log_event("RESIDENT_TASKS_ENDING", {
        "done_tasks": len(done),
        "pending_tasks": len(pending),
        "call_duration_ms": round(call_duration, 2)
    })

    for t in pending:
        t.cancel()
    await asyncio.gather(*pending, return_exceptions=True)

    # Remover conexão do ResourceManager
    resource_manager.unregister_connection(call_id, "resident")

    # Tratar fechamento do socket com robustez para lidar com desconexões abruptas
    try:
        writer.close()
        # Usar um timeout para wait_closed para evitar bloqueio indefinido 
        # em caso de desconexão súbita (Connection reset by peer)
        await asyncio.wait_for(writer.wait_closed(), timeout=2.0)
    except asyncio.TimeoutError:
        logger.info(f"[{call_id}] Timeout ao aguardar fechamento do socket do morador - provavelmente já foi fechado pelo cliente")
    except ConnectionResetError:
        # Isso é esperado se o cliente desconectar abruptamente após receber KIND_HANGUP
        logger.info(f"[{call_id}] Conexão do morador resetada pelo cliente após KIND_HANGUP - comportamento normal")
    except Exception as e:
        # Capturar qualquer outro erro durante o fechamento da conexão
        logger.warning(f"[{call_id}] Erro ao fechar conexão do morador: {str(e)}")
    
    logger.info(f"[{call_id}] Conexão do morador encerrada.")
    call_logger.log_call_ended("resident_connection_closed", call_duration)
    
    # Remover logger para liberar recursos
    CallLoggerManager.remove_logger(call_id)
    
    logger.info(f"[{call_id}] Socket do morador encerrado e liberado para novas conexões")


.DS_Store:
Erro ao ler arquivo: 'utf-8' codec can't decode byte 0x86 in position 23: invalid start byte

azure_speech_callbacks.py:
"""
Módulo de callbacks do Azure Speech SDK - Versão otimizada para diagnóstico.

Este módulo contém callbacks básicos para interação com o Azure Speech SDK,
focando na detecção e diagnóstico dos eventos de reconhecimento.
"""

import asyncio
import logging
import os
import time
from typing import List, Optional, Callable, Any

import azure.cognitiveservices.speech as speechsdk

logger = logging.getLogger(__name__)

class SpeechCallbacks:
    """
    Classe simplificada para gerenciar callbacks do Azure Speech SDK.
    
    Esta versão mantém apenas o essencial para detecção de voz e 
    processamento de texto reconhecido, com melhor diagnóstico.
    """
    
    def __init__(self, call_id: str, 
                 is_visitor: bool = True,
                 call_logger=None):
        """
        Inicializa os callbacks do Azure Speech.
        
        Args:
            call_id: ID da chamada
            is_visitor: True se for visitante, False se for morador
            call_logger: Logger específico da chamada
        """
        self.call_id = call_id
        self.is_visitor = is_visitor
        self.call_logger = call_logger
        
        # Estado interno básico
        self.collecting_audio = False
        self.audio_buffer: List[bytes] = []
        self.speech_detected = False
        
        # Timestamps para cálculos de duração
        self.speech_start_time = None
        
        # Função de processamento a ser definida pelo chamador
        self.process_callback: Optional[Callable] = None
        
        logger.info(f"[{self.call_id}] Inicializando callbacks para {'visitante' if is_visitor else 'morador'}")
    
    def set_process_callback(self, callback: Callable[[str, bytes], Any]):
        """Define a função de callback para processar texto reconhecido."""
        self.process_callback = callback
    
    def on_recognized(self, evt):
        """Callback quando a fala é reconhecida completamente."""
        role = "visitante" if self.is_visitor else "morador"
        
        # Log detalhado do evento - IMPORTANTE PARA DIAGNÓSTICO
        logger.info(f"[{self.call_id}] *** EVENTO ON_RECOGNIZED DISPARADO! ***")
        
        # Logar informações detalhadas do evento
        try:
            # Logar tipo do evento
            logger.info(f"[{self.call_id}] Tipo do evento: {type(evt).__name__}")
            
            # Logar tipo do resultado
            if hasattr(evt, 'result'):
                logger.info(f"[{self.call_id}] Tipo do resultado: {type(evt.result).__name__}")
                
                # Logar reason do resultado
                if hasattr(evt.result, 'reason'):
                    reason_str = f"{evt.result.reason}"
                    reason_int = int(evt.result.reason) if hasattr(evt.result.reason, '__int__') else -1
                    logger.info(f"[{self.call_id}] Reason: {reason_str} (valor: {reason_int})")
                
                # Logar texto do resultado
                if hasattr(evt.result, 'text'):
                    logger.info(f"[{self.call_id}] Texto reconhecido: '{evt.result.text}'")
        except Exception as e:
            logger.error(f"[{self.call_id}] Erro ao logar detalhes do evento recognized: {e}")
        
        logger.info(f"[{self.call_id}] Estado do buffer: {len(self.audio_buffer)} frames, collecting={self.collecting_audio}")
        
        # Log detalhado de propriedades do resultado para diagnóstico
        if hasattr(evt.result, 'properties'):
            try:
                properties = evt.result.properties
                if properties:
                    logger.info(f"[{self.call_id}] Propriedades do resultado: {properties}")
                
                # Capturar resposta JSON completa do serviço, se disponível
                # Verificar se properties é um objeto que tem o método get_property ou se é um dicionário
                if properties:
                    if hasattr(properties, 'get_property'):
                        # É um objeto com método get_property
                        if properties.get_property(speechsdk.PropertyId.SpeechServiceResponse_JsonResult):
                            json_result = properties.get_property(speechsdk.PropertyId.SpeechServiceResponse_JsonResult)
                            logger.info(f"[{self.call_id}] JSON do resultado: {json_result}")
                    elif isinstance(properties, dict):
                        # É um dicionário
                        logger.info(f"[{self.call_id}] Propriedades como dicionário: {properties}")
            except Exception as e:
                logger.error(f"[{self.call_id}] Erro ao acessar propriedades: {e}")
        
        # Tratamento baseado na razão do evento
        if evt.result.reason == speechsdk.ResultReason.RecognizedSpeech:
            # Fala reconhecida com sucesso
            logger.info(f"[{self.call_id}] RECONHECIMENTO BEM-SUCEDIDO: '{evt.result.text}'")
            
            # Processar o texto reconhecido
            if len(self.audio_buffer) > 0 and self.process_callback:
                audio_data = b"".join(self.audio_buffer) 
                
                # Executar callback de processamento
                loop = asyncio.get_event_loop()
                asyncio.run_coroutine_threadsafe(
                    self.process_callback(evt.result.text, audio_data), 
                    loop
                )
            else:
                logger.warning(f"[{self.call_id}] Texto reconhecido, mas sem dados de áudio para processar")
        
        elif evt.result.reason == speechsdk.ResultReason.NoMatch:
            # Tratamento para quando o Azure detecta fala mas não consegue transcrever
            no_match_details = "N/A"
            if hasattr(evt.result, 'no_match_details'):
                no_match_details = evt.result.no_match_details
            
            logger.info(f"[{self.call_id}] NoMatch detectado. Detalhes: {no_match_details}")
            
            # Processar o áudio mesmo sem reconhecimento se tiver buffer
            if len(self.audio_buffer) > 0 and self.process_callback:
                audio_data = b"".join(self.audio_buffer)
                
                # Executar callback com texto vazio
                loop = asyncio.get_event_loop()
                asyncio.run_coroutine_threadsafe(
                    self.process_callback("", audio_data), 
                    loop
                )
        
        else:
            # Outros casos (cancelamento, erro, etc.)
            logger.warning(f"[{self.call_id}] on_recognized com reason desconhecido: {evt.result.reason}")
        
        # Limpar buffer e resetar estado
        self.audio_buffer = []
        self.collecting_audio = False
        self.speech_detected = False
    
    def on_speech_start_detected(self, evt):
        """Callback quando o início de fala é detectado."""
        role = "visitante" if self.is_visitor else "morador"
        logger.info(f"[{self.call_id}] INÍCIO DE FALA DETECTADO ({role})")
        
        # Registrar no logger específico da chamada se disponível
        if self.call_logger:
            self.call_logger.log_speech_detected(is_visitor=self.is_visitor)
        
        # Resetar buffer e iniciar coleta
        self.audio_buffer = []
        self.collecting_audio = True
        self.speech_detected = True
        self.speech_start_time = time.time()
    
    def on_speech_end_detected(self, evt):
        """Callback quando o fim de fala é detectado."""
        role = "visitante" if self.is_visitor else "morador"
        logger.info(f"[{self.call_id}] FIM DE FALA DETECTADO ({role})")
        
        # Calcular duração da fala se possível
        if self.speech_start_time:
            duration_ms = (time.time() - self.speech_start_time) * 1000
            logger.info(f"[{self.call_id}] Duração da fala: {duration_ms:.1f}ms")
            
            if self.call_logger:
                self.call_logger.log_speech_ended(duration_ms, is_visitor=self.is_visitor)
        
        # Verificar se temos dados para processar
        if not self.collecting_audio or len(self.audio_buffer) == 0:
            logger.warning(f"[{self.call_id}] Fim de fala detectado, mas sem dados para processar")
            return
        
        # Marcar que a coleta terminou - agora vamos aguardar pelo evento on_recognized
        # sem intervenção manual
        logger.info(f"[{self.call_id}] Aguardando evento on_recognized para processar o áudio (buffer: {len(self.audio_buffer)} frames)")
        self.collecting_audio = False
    
    def on_recognizing(self, evt):
        """Callback para resultados parciais de reconhecimento."""
        # Importante para diagnóstico - mostrar resultados parciais
        if evt.result and evt.result.text:
            logger.info(f"[{self.call_id}] Reconhecimento parcial: {evt.result.text}")
    
    def on_session_started(self, evt):
        """Callback quando a sessão de reconhecimento é iniciada."""
        logger.info(f"[{self.call_id}] Sessão de reconhecimento iniciada: {evt.session_id}")
    
    def on_session_stopped(self, evt):
        """Callback quando a sessão de reconhecimento é encerrada."""
        logger.info(f"[{self.call_id}] Sessão de reconhecimento encerrada: {evt.session_id}")
    
    def on_canceled(self, evt):
        """Callback quando o reconhecimento é cancelado."""
        logger.error(f"[{self.call_id}] Reconhecimento cancelado: {evt.reason}")
        if evt.reason == speechsdk.CancellationReason.Error:
            logger.error(f"[{self.call_id}] Erro: {evt.error_details}")
            
            # Processar o áudio mesmo com erro se tiver dados
            if len(self.audio_buffer) > 0 and self.process_callback:
                audio_data = b"".join(self.audio_buffer)
                
                # Executar callback
                loop = asyncio.get_event_loop()
                asyncio.run_coroutine_threadsafe(
                    self.process_callback("", audio_data),
                    loop
                )
        
        # Limpar buffer e resetar estado
        self.audio_buffer = []
        self.collecting_audio = False
        self.speech_detected = False
    
    def add_audio_chunk(self, chunk: bytes):
        """
        Adiciona um chunk de áudio ao buffer quando estiver coletando.
        
        Returns:
            True se o áudio estiver sendo coletado, False caso contrário
        """
        # Verificação básica do chunk
        if not chunk or len(chunk) == 0:
            return False
        
        # Se fala foi detectada, garantir que estamos coletando
        if self.speech_detected and not self.collecting_audio:
            self.collecting_audio = True
        
        # Se estamos coletando, adicionar ao buffer
        if self.collecting_audio:
            self.audio_buffer.append(chunk)
            
            # Log periódico para informar status do buffer
            if len(self.audio_buffer) % 20 == 0:  # Log a cada 20 chunks
                buffer_duration_ms = len(self.audio_buffer) * 20  # Aproximadamente 20ms por chunk
                logger.info(f"[{self.call_id}] Buffer: {len(self.audio_buffer)} chunks (~{buffer_duration_ms}ms)")
            
            return True
        
        return False
    
    def is_collecting(self) -> bool:
        """Retorna se está coletando áudio."""
        return self.collecting_audio
    
    def register_callbacks(self, recognizer, speech_config=None):
        """Registra todos os callbacks com o recognizer."""
        # ATENÇÃO: Corrigido o erro de registro do callback
        logger.info(f"[{self.call_id}] Registrando callbacks para {'visitante' if self.is_visitor else 'morador'}")
        
        # Verificar se o recognizer é válido
        if not recognizer:
            logger.error(f"[{self.call_id}] ERRO: Recognizer é None ou inválido!")
            return
            
        # Registrar callbacks com verificação de erros
        try:
            # Registrar callback para evento recognized (MAIS IMPORTANTE)
            recognizer.recognized.connect(self.on_recognized)
            logger.info(f"[{self.call_id}] Callback 'recognized' registrado com sucesso")
            
            # Registrar outros callbacks
            recognizer.speech_start_detected.connect(self.on_speech_start_detected)
            recognizer.speech_end_detected.connect(self.on_speech_end_detected)
            recognizer.recognizing.connect(self.on_recognizing)
            recognizer.session_started.connect(self.on_session_started)
            recognizer.session_stopped.connect(self.on_session_stopped)
            recognizer.canceled.connect(self.on_canceled)
            
            logger.info(f"[{self.call_id}] Todos os callbacks registrados com sucesso")
        except Exception as e:
            logger.error(f"[{self.call_id}] ERRO AO REGISTRAR CALLBACKS: {e}")
        
        # Configurar arquivo de log do SDK para diagnóstico detalhado
        if speech_config:
            try:
                log_dir = os.path.join("logs", "azure_speech")
                os.makedirs(log_dir, exist_ok=True)
                log_file = os.path.join(log_dir, f"azure_speech_{self.call_id}.txt")
                
                speech_config.set_property(speechsdk.PropertyId.Speech_LogFilename, log_file)
                logger.info(f"[{self.call_id}] Logs internos do Azure Speech SDK ativados: {log_file}")
                
                # Configurar formato de saída detalhado
                speech_config.output_format = speechsdk.OutputFormat.Detailed
                logger.info(f"[{self.call_id}] Formato de saída detalhado ativado")
                
                # Configurar timeout de silêncio e outras propriedades importantes
                speech_config.set_property(speechsdk.PropertyId.SpeechServiceConnection_EndSilenceTimeoutMs, "2000")
                logger.info(f"[{self.call_id}] Timeout de silêncio final configurado para 2000ms")
                
                # Ajustar propriedades adicionais para melhorar o reconhecimento
                if hasattr(speechsdk.PropertyId, "Speech_SegmentationSilenceTimeoutMs"):
                    speech_config.set_property(speechsdk.PropertyId.Speech_SegmentationSilenceTimeoutMs, "1000")
                    logger.info(f"[{self.call_id}] Timeout de segmentação configurado para 1000ms")
                
                # Configurar idioma explicitamente
                speech_config.speech_recognition_language = "pt-BR"
                logger.info(f"[{self.call_id}] Idioma de reconhecimento configurado para pt-BR")
            except Exception as e:
                logger.error(f"[{self.call_id}] Erro ao configurar logs do SDK: {e}")
        
        logger.info(f"[{self.call_id}] Configuração de callbacks concluída para {'visitante' if self.is_visitor else 'morador'}")
    
    def mark_ia_audio_sent(self):
        """
        Limpa o estado após envio de áudio pela IA.
        """
        # Limpar buffer e estado
        self.audio_buffer = []
        self.collecting_audio = False
        self.speech_detected = False

requirements.txt:
aiohappyeyeballs==2.6.1
aiohttp==3.11.16
aiosignal==1.3.2
alembic==1.15.2
annotated-types==0.7.0
anyio==4.9.0
appdirs==1.4.4
arrow==1.3.0
asgiref==3.8.1
asttokens==3.0.0
asyncio==3.4.3
attrs==25.3.0
auth0-python==4.7.2
azure-cognitiveservices-speech==1.41.1
backoff==2.2.1
bcrypt==4.3.0
beautifulsoup4==4.13.4
blinker==1.9.0
build==1.2.2.post1
cachetools==5.5.2
certifi==2025.1.31
cffi==1.17.1
charset-normalizer==3.4.1
chroma-hnswlib==0.7.6
chromadb==0.5.23
click==8.1.8
cohere==5.15.0
colorama==0.4.6
coloredlogs==15.0.1
crewai==0.108.0
crewai-tools==0.38.1
cryptography==43.0.3
dataclasses-json==0.6.7
decorator==5.2.1
Deprecated==1.2.18
deprecation==2.1.0
diff-match-patch==20230430
distro==1.9.0
docker==7.1.0
docstring_parser==0.16
durationpy==0.9
embedchain==0.1.128
et_xmlfile==2.0.0
executing==2.2.0
Faker==25.9.2
fastapi==0.115.9
fastavro==1.10.0
filelock==3.18.0
flatbuffers==25.2.10
fqdn==1.5.1
frozenlist==1.5.0
fsspec==2025.3.2
google-auth==2.39.0
googleapis-common-protos==1.70.0
gptcache==0.1.44
griffe==0.36.9
grpcio==1.71.0
grpcio-tools==1.71.0
guardrails-ai==0.6.5
guardrails-api-client==0.4.0a1
guardrails_hub_types==0.0.4
h11==0.14.0
h2==4.2.0
hpack==4.1.0
httpcore==1.0.8
httptools==0.6.4
httpx==0.27.2
httpx-sse==0.4.0
huggingface-hub==0.30.2
humanfriendly==10.0
hyperframe==6.1.0
idna==3.10
importlib_metadata==8.6.1
importlib_resources==6.5.2
instructor==1.7.9
ipython==9.1.0
ipython_pygments_lexers==1.1.1
isoduration==20.11.0
jedi==0.19.2
Jinja2==3.1.6
jiter==0.8.2
json5==0.12.0
json_repair==0.41.1
jsonpatch==1.33
jsonpickle==4.0.5
jsonpointer==3.0.0
jsonref==1.1.0
jsonschema==4.23.0
jsonschema-specifications==2024.10.1
kubernetes==32.0.1
lancedb==0.21.2
langchain==0.3.23
langchain-cohere==0.3.5
langchain-community==0.3.21
langchain-core==0.3.52
langchain-experimental==0.3.4
langchain-openai==0.2.14
langchain-text-splitters==0.3.8
langsmith==0.3.31
litellm==1.60.2
lxml==4.9.4
Mako==1.3.10
markdown-it-py==3.0.0
MarkupSafe==3.0.2
marshmallow==3.26.1
matplotlib-inline==0.1.7
mdurl==0.1.2
mem0ai==0.1.91
mmh3==5.1.0
monotonic==1.6
mpmath==1.3.0
multidict==6.4.3
mypy-extensions==1.0.0
networkx==3.4.2
nodeenv==1.9.1
numpy==2.2.4
oauthlib==3.2.2
onnxruntime==1.21.0
openai==1.75.0
openpyxl==3.1.5
opentelemetry-api==1.32.1
opentelemetry-exporter-otlp-proto-common==1.32.1
opentelemetry-exporter-otlp-proto-grpc==1.32.1
opentelemetry-exporter-otlp-proto-http==1.32.1
opentelemetry-instrumentation==0.53b1
opentelemetry-instrumentation-asgi==0.53b1
opentelemetry-instrumentation-fastapi==0.53b1
opentelemetry-proto==1.32.1
opentelemetry-sdk==1.32.1
opentelemetry-semantic-conventions==0.53b1
opentelemetry-util-http==0.53b1
orjson==3.10.16
overrides==7.7.0
packaging==24.2
pandas==2.2.3
parso==0.8.4
pdfminer.six==20250327
pdfplumber==0.11.6
pexpect==4.9.0
pika==1.3.2
pillow==11.2.1
portalocker==2.10.1
posthog==3.25.0
prompt_toolkit==3.0.51
propcache==0.3.1
protobuf==5.29.4
psycopg2-binary==2.9.10
ptyprocess==0.7.0
pure_eval==0.2.3
pyarrow==19.0.1
pyasn1==0.6.1
pyasn1_modules==0.4.2
PyAudio==0.2.14
pycparser==2.22
pydantic==2.11.3
pydantic-settings==2.8.1
pydantic_core==2.33.1
pydash==7.0.7
pydub==0.25.1
Pygments==2.19.1
PyJWT==2.10.1
pypdf==5.4.0
pypdfium2==4.30.1
PyPika==0.48.9
pyproject_hooks==1.2.0
pyright==1.1.399
pysbd==0.3.4
python-dateutil==2.9.0.post0
python-dotenv==1.0.1
pytube==15.0.0
pytz==2024.2
pyvis==0.3.2
PyYAML==6.0.2
qdrant-client==1.13.3
RapidFuzz==3.13.0
redis==5.2.1
referencing==0.36.2
regex==2024.11.6
requests==2.32.3
requests-oauthlib==2.0.0
requests-toolbelt==1.0.0
rfc3339-validator==0.1.4
rfc3986-validator==0.1.1
rich==13.9.4
rpds-py==0.24.0
rsa==4.9.1
rstr==3.2.2
schema==0.7.7
semver==3.0.4
shellingham==1.5.4
six==1.17.0
sniffio==1.3.1
soupsieve==2.6
SQLAlchemy==2.0.40
stack-data==0.6.3
starlette==0.45.3
sympy==1.13.3
tabulate==0.9.0
tenacity==9.1.2
tiktoken==0.9.0
tokenizers==0.20.3
tomli==2.2.1
tomli_w==1.2.0
tqdm==4.66.5
traitlets==5.14.3
typer==0.15.2
types-python-dateutil==2.9.0.20241206
types-requests==2.32.0.20250328
typing-inspect==0.9.0
typing-inspection==0.4.0
typing_extensions==4.12.2
tzdata==2025.2
uri-template==1.3.0
urllib3==2.0.7
uuid-v7==1.0.0
uv==0.6.14
uvicorn==0.34.1
uvloop==0.21.0
watchfiles==1.0.5
wcwidth==0.2.13
webcolors==24.11.1
webrtcvad==2.0.10
websocket-client==1.8.0
websockets==12.0
wrapt==1.17.2
wsproto==1.2.0
yarl==1.20.0
zipp==3.21.0
zstandard==0.23.0


config.json:
{
    "greeting": {
        "message": "Condom\u00ednio Apoena, em que posso ajudar?",
        "voice": "pt-BR-AntonioNeural",
        "delay_seconds": 2
    },
    "system": {
        "default_state": "STANDBY",
        "silence_threshold_seconds": 1.5,
        "max_transaction_time_seconds": 60,
        "goodbye_delay_seconds": 3.0,
        "voice_detection_type": "azure_speech",
        "azure_speech_segment_timeout_ms": 1000,
        "azure_speech_visitor_timeout_ms": 1000,
        "azure_speech_resident_timeout_ms": 600,
        "resident_max_silence_seconds": 45.0,
        "azure_speech_sensitivity": 0.7
    },
    "audio": {
        "sample_rate": 8000,
        "channels": 1,
        "format": "SLIN",
        "chunk_size": 320,
        "transmission_delay_ms": 10,
        "post_audio_delay_seconds": 0.3,
        "discard_buffer_frames": 5,
        "anti_echo_delay_ms": 800
    },
    "call_termination": {
        "enabled": true,
        "goodbye_messages": {
            "visitor": {
                "authorized_entrega": "Sua entrega foi autorizada pelo morador. Obrigado por utilizar nossa portaria inteligente.",
                "authorized_visita": "Sua visita foi autorizada pelo morador. Obrigado por utilizar nossa portaria inteligente.",
                "authorized": "Sua entrada foi autorizada pelo morador. Obrigado por utilizar nossa portaria inteligente.",
                "denied": "Sua entrada n\u00e3o foi autorizada pelo morador. Obrigado por utilizar nossa portaria inteligente.",
                "default": "Obrigado por utilizar nossa portaria inteligente. At\u00e9 logo."
            },
            "resident": {
                "default": "Obrigado pela sua resposta. Encerrando a chamada."
            }
        }
    }
}

session_manager.py:
# session_manager.py

import asyncio
import logging
from typing import Dict, Optional, List
from uuid import uuid4  # Usando UUID v4 padrão

# Mantenha seu import do crew se quiser, mas não vamos chamar direto aqui
# from ai.crew import process_user_message_with_coordinator

from conversation_flow import ConversationFlow

logger = logging.getLogger(__name__)


class SessionData:
    def __init__(self, session_id: str, extension_manager=None):
        self.session_id = session_id

        self.visitor_state = "USER_TURN"
        self.resident_state = "STANDBY"

        self.history: List[str] = []

        # Filas de mensagens
        self.visitor_queue: asyncio.Queue = asyncio.Queue()
        self.resident_queue: asyncio.Queue = asyncio.Queue()

        # Flags para controle de terminação
        self.should_terminate_visitor = False
        self.should_terminate_resident = False
        
        # Sinal para encerrar conexões específicas
        self.terminate_visitor_event = asyncio.Event()
        self.terminate_resident_event = asyncio.Event()

        self.intent_data = {}

        # Aqui criamos uma instância do Flow para cada sessão
        # Passando o extension_manager para o flow
        self.flow = ConversationFlow(extension_manager)


class SessionManager:
    def __init__(self, extension_manager=None):
        self.sessions: Dict[str, SessionData] = {}
        self.extension_manager = extension_manager

    def create_session(self, session_id: Optional[str] = None) -> SessionData:
        if not session_id:
            # Gerar UUID para identificação da sessão
            session_id = str(uuid4())
            logger.info(f"[SessionManager] Novo UUID gerado: {session_id}")

        if session_id not in self.sessions:
            # Passamos o extension_manager para cada sessão
            self.sessions[session_id] = SessionData(session_id, self.extension_manager)
            logger.info(f"[SessionManager] Criada nova sessão: {session_id}")
        return self.sessions[session_id]

    def get_session(self, session_id: str) -> Optional[SessionData]:
        return self.sessions.get(session_id)

    # Métodos p/ enfileirar msgs (chamados no flow)
    def enfileirar_visitor(self, session_id: str, mensagem: str):
        session = self.get_session(session_id)
        if session:
            session.visitor_queue.put_nowait(mensagem)

    def enfileirar_resident(self, session_id: str, mensagem: str):
        session = self.get_session(session_id)
        if session:
            session.resident_queue.put_nowait(mensagem)

    def get_message_for_visitor(self, session_id: str) -> Optional[str]:
        session = self.get_session(session_id)
        if not session:
            return None
        if session.visitor_queue.empty():
            return None
        return session.visitor_queue.get_nowait()

    def get_message_for_resident(self, session_id: str) -> Optional[str]:
        session = self.get_session(session_id)
        if not session:
            return None
        if session.resident_queue.empty():
            return None
        return session.resident_queue.get_nowait()

    # -------------------------------------------------------------
    # Novo process_visitor_text + process_resident_text
    # -------------------------------------------------------------
    def process_visitor_text(self, session_id: str, text: str):
        """
        Agora chamamos o Flow para lidar com a msg do visitante.
        """
        session = self.get_session(session_id)
        if not session:
            session = self.create_session(session_id)

        # logs + history
        logger.info(f"[Session {session_id}] Visitor disse: {text}")
        session.history.append(f"[Visitor] {text}")

        # repassar p/ on_visitor_message
        session.flow.on_visitor_message(session_id, text, self)

    def process_resident_text(self, session_id: str, text: str):
        """
        Agora chamamos o Flow para lidar com a msg do morador.
        """
        session = self.get_session(session_id)
        if not session:
            session = self.create_session(session_id)

        logger.info(f"[Session {session_id}] Resident disse: {text}")
        session.history.append(f"[Resident] {text}")

        session.flow.on_resident_message(session_id, text, self)

    def end_session(self, session_id: str):
        """
        Prepara a sessão para encerramento, sinalizando para as tarefas
        de audiosocket que devem terminar graciosamente.
        """
        session = self.get_session(session_id)
        if not session:
            logger.warning(f"[SessionManager] Tentativa de encerrar sessão inexistente: {session_id}")
            return
            
        # Evitar que end_session seja chamado múltiplas vezes
        if not session.terminate_visitor_event.is_set() and not session.terminate_resident_event.is_set():
            logger.info(f"[SessionManager] Sinalizando para encerrar sessão {session_id}")
            session.terminate_visitor_event.set()
            session.terminate_resident_event.set()
        else:
            logger.info(f"[SessionManager] Sinais de terminação já estão ativos para sessão {session_id}, ignorando")
        
        # Não removemos a sessão imediatamente, permitindo que as tarefas
        # de audiosocket terminem graciosamente

    def _complete_session_termination(self, session_id: str):
        """
        Remove efetivamente a sessão após tarefas de audiosocket terem encerrado.
        Este método deve ser chamado apenas após o encerramento completo das conexões.
        """
        if session_id in self.sessions:
            del self.sessions[session_id]
            logger.info(f"[SessionManager] Sessão {session_id} finalizada e completamente removida.")


record_slin_audio.py:
#!/usr/bin/env python3
"""
Script para gravar um áudio no formato SLIN (Signed Linear) para diagnóstico.
Este script grava um áudio do microfone e o salva no formato SLIN para ser usado
como referência em testes de diagnóstico.

Uso:
    python record_slin_audio.py [nome_arquivo] [duracao_segundos]

Argumentos:
    nome_arquivo: Nome do arquivo de saída (padrão: test_audio.slin)
    duracao_segundos: Duração da gravação em segundos (padrão: 10)

Requisitos:
    - PyAudio
    - PySLIN
"""

import os
import sys
import time
import pyaudio
import wave
import hashlib
import argparse
from pathlib import Path

# Configurações de áudio
SAMPLE_RATE = 8000  # SLIN é 8kHz por padrão
CHANNELS = 1        # Mono
FORMAT = pyaudio.paInt16  # 16-bit
CHUNK_SIZE = 320    # 40ms de áudio por chunk (mesmo valor usado no audiosocket_handler)

# Diretório padrão para salvar os arquivos
AUDIO_CACHE_DIR = "audio/cache"


def md5_hash(data):
    """Gera um hash MD5 dos dados binários do áudio."""
    return hashlib.md5(data).hexdigest()


def record_audio(duration_seconds=10, progress_callback=None):
    """
    Grava áudio do microfone por um período de tempo especificado.
    
    Args:
        duration_seconds: Duração da gravação em segundos
        progress_callback: Função para reportar progresso
        
    Returns:
        Dados de áudio brutos (PCM) no formato SLIN
    """
    # Inicializar PyAudio
    p = pyaudio.PyAudio()
    
    # Mostrar informações sobre o dispositivo de entrada padrão
    device_info = p.get_default_input_device_info()
    print(f"Dispositivo de entrada: {device_info['name']}")
    
    # Abrir stream para captura
    stream = p.open(format=FORMAT,
                   channels=CHANNELS,
                   rate=SAMPLE_RATE,
                   input=True,
                   frames_per_buffer=CHUNK_SIZE)
    
    print(f"\nIniciando gravação de {duration_seconds} segundos...")
    print("Fale algo no microfone...")
    
    # Calcular número de iterações baseado no chunk_size e duração
    n_chunks = int(SAMPLE_RATE / CHUNK_SIZE * duration_seconds)
    
    # Lista para armazenar os chunks de áudio
    audio_chunks = []
    
    # Loop de gravação
    for i in range(n_chunks):
        # Atualizar progresso
        if i % 10 == 0:  # A cada 10 chunks (cerca de 400ms)
            progress = (i / n_chunks) * 100
            print(f"\rProgresso: {progress:.1f}%", end="")
            if progress_callback:
                progress_callback(progress)
                
        # Ler um chunk de áudio
        data = stream.read(CHUNK_SIZE, exception_on_overflow=False)
        audio_chunks.append(data)
    
    print("\rProgresso: 100.0%")
    print("\nGravação concluída!")
    
    # Fechar e liberar recursos
    stream.stop_stream()
    stream.close()
    p.terminate()
    
    # Juntar todos os chunks em um único buffer
    return b"".join(audio_chunks)


def save_slin_file(audio_data, output_filename):
    """
    Salva dados de áudio diretamente no formato SLIN.
    
    Args:
        audio_data: Bytes de áudio no formato PCM
        output_filename: Nome do arquivo de saída
    """
    # Garantir que o diretório exista
    os.makedirs(os.path.dirname(output_filename), exist_ok=True)
    
    # Salvar o arquivo SLIN (que é basicamente PCM raw)
    with open(output_filename, 'wb') as f:
        f.write(audio_data)
    
    print(f"Arquivo SLIN salvo: {output_filename}")
    print(f"Tamanho: {len(audio_data)} bytes")
    
    # Gerar um hash do arquivo para referência
    audio_hash = md5_hash(audio_data)
    print(f"Hash MD5: {audio_hash}")
    
    return audio_hash


def save_wav_file(audio_data, output_filename):
    """
    Salva uma cópia do áudio no formato WAV para referência.
    Útil para ouvir o áudio gravado com um player padrão.
    
    Args:
        audio_data: Bytes de áudio no formato PCM
        output_filename: Nome do arquivo de saída
    """
    # Criar arquivo WAV para referência (mais fácil de ouvir)
    wav_filename = output_filename.replace('.slin', '.wav')
    
    with wave.open(wav_filename, 'wb') as wf:
        wf.setnchannels(CHANNELS)
        wf.setsampwidth(2)  # 16-bit = 2 bytes
        wf.setframerate(SAMPLE_RATE)
        wf.writeframes(audio_data)
    
    print(f"Arquivo WAV de referência salvo: {wav_filename}")


def main():
    # Configurar parser de argumentos
    parser = argparse.ArgumentParser(description="Grava áudio no formato SLIN para diagnóstico")
    parser.add_argument('output_file', nargs='?', default="test_audio.slin", 
                        help="Nome do arquivo de saída (padrão: test_audio.slin)")
    parser.add_argument('duration', nargs='?', type=int, default=10,
                        help="Duração da gravação em segundos (padrão: 10)")
    parser.add_argument('--cache', action='store_true',
                        help="Salvar no diretório de cache com nome baseado no hash")
    
    args = parser.parse_args()
    
    # Gravar o áudio
    audio_data = record_audio(args.duration)
    
    # Determinar nome do arquivo de saída
    if args.cache:
        # Gerar hash do áudio
        audio_hash = md5_hash(audio_data)
        # Criar nome de arquivo baseado no hash
        output_filename = os.path.join(AUDIO_CACHE_DIR, f"{audio_hash}.slin")
    else:
        # Usar nome fornecido pelo usuário
        output_filename = args.output_file
        # Se for um caminho relativo, converter para o diretório atual
        if not os.path.isabs(output_filename):
            output_filename = os.path.join(os.getcwd(), output_filename)
    
    # Garantir que o diretório de saída exista
    os.makedirs(os.path.dirname(output_filename), exist_ok=True)
    
    # Salvar o arquivo SLIN
    audio_hash = save_slin_file(audio_data, output_filename)
    
    # Salvar uma cópia em WAV para referência
    save_wav_file(audio_data, output_filename)
    
    # Mostrar instruções de uso
    print("\n" + "="*60)
    print("INSTRUÇÕES DE USO:")
    print("="*60)
    print(f"1. Para usar este áudio nos testes, adicione essa linha ao config.json:")
    print(f'   "test_audio_file": "{os.path.basename(output_filename)}"')
    print("2. Execute a aplicação principal normalmente")
    print("3. Para mudar o arquivo de teste, modifique a configuração ou:")
    print(f"   Grave outro áudio: python {sys.argv[0]} outro_nome.slin")
    print("="*60)
    
    return audio_hash


if __name__ == "__main__":
    main()

state_machine.py:
#!/usr/bin/env python3
# state_machine.py - Máquina de estado simplificada para gerenciar turnos de conversa

import enum
import logging
import uuid
from datetime import datetime
from typing import Callable, List, Dict, Optional
import asyncio  # <-- necessário para as funções assíncronas adicionadas

logger = logging.getLogger(__name__)


class State(enum.Enum):
    """Estados simplificados para a conversa entre usuário e IA."""
    STANDBY = "STANDBY"  # Estado inicial, aguardando nova chamada
    USER_TURN = "USER_TURN"  # Turno do usuário (sistema está ouvindo)
    WAITING = "WAITING"  # Estado intermediário de processamento
    IA_TURN = "IA_TURN"  # Turno da IA (sistema está respondendo)


class StateMachine:
    """
    Máquina de estados simplificada para controlar o fluxo de comunicação
    entre o usuário e a IA em uma chamada.
    """

    def __init__(self):
        self.current_state = State.STANDBY
        self.conversation_id = None
        self.state_change_callbacks: Dict[State, List[Callable]] = {
            state: [] for state in State
        }
        self.transcricoes = []
        self.ultima_resposta = None
        logger.info(f"Máquina de estados inicializada em {self.current_state}")

    def get_state(self) -> State:
        return self.current_state

    def transition_to(self, new_state: State) -> None:
        if new_state == self.current_state:
            logger.debug(f"Ignorando transição redundante para {new_state}")
            return

        old_state = self.current_state
        self.current_state = new_state
        logger.info(f"Transição de estado: {old_state} -> {new_state}")

        if old_state == State.IA_TURN and new_state == State.USER_TURN:
            logger.info("*** IMPORTANTE: Transição de IA_TURN para USER_TURN - ativando escuta ***")
            self.registrar_transcricao_sistema("Sistema ativou escuta - aguardando fala do usuário")
        elif old_state == State.WAITING:
            logger.info(f"Saindo do estado WAITING para {new_state}")
            self.registrar_transcricao_sistema(f"Estado alterado: {old_state.value} -> {new_state.value}")
        if new_state == State.USER_TURN:
            logger.info("*** Sistema pronto para ouvir o usuário ***")

        for callback in self.state_change_callbacks.get(new_state, []):
            try:
                callback()
            except Exception as e:
                logger.error(f"Erro no callback de mudança de estado: {e}")

    def on_state_change(self, state: State, callback: Callable) -> None:
        if state in self.state_change_callbacks:
            self.state_change_callbacks[state].append(callback)

    def is_user_turn(self) -> bool:
        return self.current_state == State.USER_TURN

    def is_ai_turn(self) -> bool:
        return self.current_state == State.IA_TURN

    def is_waiting(self) -> bool:
        return self.current_state == State.WAITING

    def is_standby(self) -> bool:
        return self.current_state == State.STANDBY

    def start_new_conversation(self, standby=False) -> str:
        # Usando UUID v4 padrão como identificador único
        self.conversation_id = str(uuid.uuid4())
        self.transcricoes = []
        self.ultima_resposta = None

        if not standby:
            self.transition_to(State.USER_TURN)
        else:
            self.transition_to(State.STANDBY)

        logger.info(f"Nova conversa iniciada com ID: {self.conversation_id}")
        return self.conversation_id

    def registrar_transcricao_usuario(self, texto: str) -> None:
        self.transcricoes.append({
            "timestamp": datetime.now().isoformat(),
            "origem": "usuario",
            "texto": texto
        })
        logger.info(f"Transcrição do usuário registrada: {texto}")

    def registrar_transcricao_ia(self, texto: str, resposta_completa: Optional[Dict] = None) -> None:
        self.transcricoes.append({
            "timestamp": datetime.now().isoformat(),
            "origem": "ia",
            "texto": texto
        })
        self.ultima_resposta = resposta_completa
        logger.info(f"Resposta da IA registrada: {texto}")

    def registrar_transcricao_sistema(self, texto: str) -> None:
        self.transcricoes.append({
            "timestamp": datetime.now().isoformat(),
            "origem": "sistema",
            "texto": texto
        })
        logger.info(f"Mensagem do sistema registrada: {texto}")

    def obter_historico_transcricoes(self) -> List[Dict]:
        return self.transcricoes

    def get_conversation_id(self) -> Optional[str]:
        return self.conversation_id

    def end_conversation(self) -> None:
        logger.info(f"Conversa {self.conversation_id} finalizada")
        self.conversation_id = None
        self.transition_to(State.STANDBY)

    # Novas funções adicionadas:
    async def wait_for_state(self, state: State):
        """Aguarda até que o estado especificado seja atingido."""
        while self.current_state != state:
            await asyncio.sleep(0.1)

    async def wait_for_state_change(self):
        """Aguarda até que ocorra uma mudança do estado atual."""
        current_state = self.current_state
        while self.current_state == current_state:
            await asyncio.sleep(0.1)


audio_utils.py:
#!/usr/bin/env python3
# audio_utils.py - Utilitários para manipulação de áudio

from pydub import AudioSegment
from io import BytesIO
import logging

logger = logging.getLogger(__name__)

def converter_bytes_para_wav(dados_slin, sample_rate=8000):
    """
    Converte bytes no formato SLIN para WAV.
    
    Args:
        dados_slin: Bytes de áudio no formato SLIN
        sample_rate: Taxa de amostragem (padrão: 8000 Hz)
        
    Returns:
        Bytes WAV do áudio convertido
    """
    try:
        audio_segment = AudioSegment(
            data=bytes(dados_slin),
            sample_width=2,  # 16 bits = 2 bytes
            frame_rate=sample_rate,
            channels=1
        )
        buffer = BytesIO()
        audio_segment.export(buffer, format='wav')
        return buffer.getvalue()
    except Exception as e:
        logger.error(f"Erro ao converter bytes para WAV: {e}")
        return None

def converter_wav_para_slin(dados_wav, sample_rate=8000):
    """
    Converte bytes WAV para o formato SLIN.
    
    Args:
        dados_wav: Bytes de áudio no formato WAV
        sample_rate: Taxa de amostragem desejada (padrão: 8000 Hz)
        
    Returns:
        Bytes SLIN do áudio convertido
    """
    try:
        audio_segment = AudioSegment.from_file(BytesIO(dados_wav), format="wav")
        audio_segment = audio_segment.set_frame_rate(sample_rate).set_channels(1).set_sample_width(2)
        return audio_segment.raw_data
    except Exception as e:
        logger.error(f"Erro ao converter WAV para SLIN: {e}")
        return None

.gitignore:
# Editors
.vscode/
.idea/

# Vagrant
.vagrant/

# Mac/OSX
.DS_Store

# Windows
Thumbs.db

# Source for the following rules: https://raw.githubusercontent.com/github/gitignore/master/Python.gitignore
# Byte-compiled / optimized / DLL files
__pycache__/
*.py[cod]
*$py.class

# C extensions
*.so

# Distribution / packaging
.Python
build/
develop-eggs/
dist/
downloads/
eggs/
.eggs/
lib/
lib64/
parts/
sdist/
var/
wheels/
*.egg-info/
.installed.cfg
*.egg
MANIFEST

# PyInstaller
#  Usually these files are written by a python script from a template
#  before PyInstaller builds the exe, so as to inject date/other infos into it.
*.manifest
*.spec

# Installer logs
pip-log.txt
pip-delete-this-directory.txt

# Unit test / coverage reports
htmlcov/
.tox/
.nox/
.coverage
.coverage.*
.cache
nosetests.xml
coverage.xml
*.cover
.hypothesis/
.pytest_cache/

# Translations
*.mo
*.pot

# Django stuff:
*.log
local_settings.py
db.sqlite3

# Flask stuff:
instance/
.webassets-cache

# Scrapy stuff:
.scrapy

# Sphinx documentation
docs/_build/

# PyBuilder
target/

# Jupyter Notebook
.ipynb_checkpoints

# IPython
profile_default/
ipython_config.py

# pyenv
.python-version

# celery beat schedule file
celerybeat-schedule

# SageMath parsed files
*.sage.py

# Environments
.env
.venv
env/
venv/
ENV/
env.bak/
venv.bak/

# Spyder project settings
.spyderproject
.spyproject

# Rope project settings
.ropeproject

# mkdocs documentation
/site

# mypy
.mypy_cache/
.dmypy.json
dmypy.json
__pycache__/*
__pycache__/session_manager.cpython-311.pyc
.env


microfone_client.py:
#!/usr/bin/env python3
import socket, struct, threading, pyaudio, logging, time
import uuid

logging.basicConfig(level=logging.INFO, format='%(asctime)s %(levelname)s %(message)s')

KIND_ID, KIND_SLIN, KIND_HANGUP = 0x01, 0x10, 0x00

class AudioSocketClient:
    def __init__(self, host='127.0.0.1', port=8080):
        self.host, self.port = host, port
        # Gerando UUID para identificação da chamada
        self.call_id = uuid.uuid4().bytes
        self.sample_rate, self.channels, self.chunk_size = 8000, 1, 320
        self.format = pyaudio.paInt16
        self.running = False
        
        # Socket reconfigurado para melhor performance
        self.socket_buffer_size = 1024 * 16  # 16KB de buffer

    def connect(self):
        try:
            # Criar socket com tratamento de erros
            try:
                self.socket = socket.socket(socket.AF_INET, socket.SOCK_STREAM)
                logging.info("Socket criado com sucesso")
            except socket.error as e:
                logging.error(f"Erro ao criar socket: {e}")
                raise
            
            # Configurar buffer de socket
            try:
                self.socket.setsockopt(socket.SOL_SOCKET, socket.SO_RCVBUF, self.socket_buffer_size)
                self.socket.setsockopt(socket.SOL_SOCKET, socket.SO_SNDBUF, self.socket_buffer_size)
                
                # Configurar timeout para operações de socket
                self.socket.settimeout(5.0)  # 5 segundos de timeout para conexão inicial
                
                # Opções avançadas para TCP
                # Desabilitar algoritmo de Nagle para reduzir latência
                self.socket.setsockopt(socket.IPPROTO_TCP, socket.TCP_NODELAY, 1)
                
                # Permitir reutilização de endereço local
                self.socket.setsockopt(socket.SOL_SOCKET, socket.SO_REUSEADDR, 1)
                
                # Em sistemas que suportam, configurar TCP keepalive
                if hasattr(socket, 'SO_KEEPALIVE'):
                    self.socket.setsockopt(socket.SOL_SOCKET, socket.SO_KEEPALIVE, 1)
                
                # Em macOS/Linux, configurar tempo de TCP keepalive se disponível
                if hasattr(socket, 'TCP_KEEPIDLE'):
                    self.socket.setsockopt(socket.IPPROTO_TCP, socket.TCP_KEEPIDLE, 60)  # 60 segundos
                
                logging.info("Socket configurado com opções avançadas")
            except Exception as e:
                logging.warning(f"Erro ao configurar opções de socket (não fatal): {e}")
            
            # Tentar estabelecer conexão com retry
            max_retries = 3
            retry_count = 0
            retry_delay = 1.0  # segundos
            
            while retry_count < max_retries:
                try:
                    logging.info(f"Tentando conectar a {self.host}:{self.port} (tentativa {retry_count+1}/{max_retries})...")
                    self.socket.connect((self.host, self.port))
                    logging.info(f"Conexão estabelecida com {self.host}:{self.port}")
                    break
                except socket.timeout:
                    retry_count += 1
                    if retry_count >= max_retries:
                        logging.error(f"Timeout ao conectar após {max_retries} tentativas")
                        raise ConnectionError(f"Não foi possível conectar a {self.host}:{self.port} após {max_retries} tentativas")
                    logging.warning(f"Timeout ao conectar. Tentando novamente em {retry_delay}s...")
                    time.sleep(retry_delay)
                    retry_delay *= 1.5  # Aumentar o delay exponencialmente
                except ConnectionRefusedError:
                    logging.error(f"Conexão recusada por {self.host}:{self.port}. O servidor está em execução?")
                    raise
                except Exception as e:
                    logging.error(f"Erro ao conectar: {e}")
                    raise
            
            # Ajustar timeout para operações após conexão estabelecida
            self.socket.settimeout(2.0)  # 2 segundos para operações normais
            
            # Enviar ID da chamada com retry em caso de falha
            try:
                logging.info(f"Enviando ID da chamada: {self.call_id.hex()}")
                packet = struct.pack('>B H', KIND_ID, len(self.call_id)) + self.call_id
                bytes_sent = self.socket.send(packet)
                
                if bytes_sent != len(packet):
                    logging.warning(f"Pacote ID enviado parcialmente: {bytes_sent}/{len(packet)} bytes")
                    # Tentar enviar o restante
                    remaining = packet[bytes_sent:]
                    self.socket.sendall(remaining)
                
                logging.info("ID da chamada enviado com sucesso")
            except Exception as e:
                logging.error(f"Erro ao enviar ID da chamada: {e}")
                self.socket.close()
                raise
            
            # Configurar estado e iniciar threads
            self.running = True
            logging.info("Conexão estabelecida! Iniciando transmissão de áudio...")
            
            # Iniciar threads para envio e recebimento de áudio
            try:
                self.send_thread = threading.Thread(target=self.send_audio, name="SendAudio")
                self.send_thread.daemon = True
                self.send_thread.start()
                
                self.receive_thread = threading.Thread(target=self.receive_audio, name="ReceiveAudio")
                self.receive_thread.daemon = True
                self.receive_thread.start()
                
                logging.info("Threads de áudio iniciadas com sucesso")
            except Exception as e:
                logging.error(f"Erro ao iniciar threads de áudio: {e}")
                self.running = False
                self.socket.close()
                raise
                
        except Exception as e:
            logging.error(f"Erro na conexão: {e}")
            self.running = False
            if hasattr(self, 'socket') and self.socket:
                try:
                    self.socket.close()
                except:
                    pass
            raise

    def send_audio(self):
        try:
            # Envolva a inicialização do PyAudio em um bloco try/except
            p = pyaudio.PyAudio()
            
            # Verificar dispositivos de entrada disponíveis
            info = p.get_host_api_info_by_index(0)
            input_devices = []
            
            for i in range(info.get('deviceCount')):
                device_info = p.get_device_info_by_host_api_device_index(0, i)
                if device_info.get('maxInputChannels') > 0:
                    input_devices.append((i, device_info.get('name')))
            
            if input_devices:
                logging.info(f"Dispositivos de entrada disponíveis:")
                for idx, name in input_devices:
                    logging.info(f"  [{idx}] {name}")
                
                # Usar o primeiro dispositivo de entrada disponível
                input_device_idx = input_devices[0][0]
                logging.info(f"Usando dispositivo de entrada: [{input_device_idx}] {input_devices[0][1]}")
            else:
                input_device_idx = None
                logging.warning("Nenhum dispositivo de entrada encontrado! Verifique seu microfone.")
            
            # Configurar buffer de áudio maior com parâmetros mais seguros
            stream = p.open(format=self.format, 
                          channels=self.channels, 
                          rate=self.sample_rate, 
                          input=True, 
                          input_device_index=input_device_idx,
                          frames_per_buffer=self.chunk_size,
                          start=True)
            
            logging.info("Stream de áudio iniciado com sucesso")
            
            try:
                while self.running:
                    try:
                        # Usar exception_on_overflow=False para evitar erros em caso de sobrecarga
                        data = stream.read(self.chunk_size, exception_on_overflow=False)
                        if data and len(data) == self.chunk_size * 2:  # 2 bytes por amostra (16 bits)
                            # Garantir que o tamanho do chunk esteja correto (320 bytes)
                            # O audiosocket_handler.py espera exatamente 320 bytes para processar corretamente
                            if len(data) != 640:
                                logging.warning(f"Tamanho de chunk inesperado: {len(data)} bytes, esperado 640 bytes")
                                # Padding ou truncamento para garantir 640 bytes exatos
                                if len(data) < 640:
                                    # Adicionar padding se menor
                                    data = data + b'\x00' * (640 - len(data))
                                else:
                                    # Truncar se maior
                                    data = data[:640]
                                
                            self.socket.sendall(struct.pack('>B H', KIND_SLIN, 640) + data)
                        # Adicionar pequeno delay para evitar sobrecarga de pacotes
                        time.sleep(0.02)  # 20ms de delay entre pacotes
                    except OSError as e:
                        # Captura especificamente erros de E/S que podem ocorrer durante a leitura
                        logging.error(f"Erro de E/S durante a leitura do áudio: {e}")
                        time.sleep(0.1)  # Pequena pausa para evitar loop rápido em caso de erro
            except Exception as e:
                logging.error(f"Erro no loop de envio de áudio: {e}")
            finally:
                logging.info("Fechando stream de entrada")
                try:
                    stream.stop_stream()
                    stream.close()
                except Exception as e:
                    logging.error(f"Erro ao fechar stream de entrada: {e}")
        except Exception as e:
            logging.error(f"Erro ao inicializar PyAudio para captura: {e}")
        finally:
            self.running = False
            try:
                p.terminate()
            except:
                pass
            logging.info("Thread de envio de áudio encerrada")

    def receive_audio(self):
        try:
            # Inicializar PyAudio de forma segura
            p = pyaudio.PyAudio()
            
            # Verificar dispositivos de saída disponíveis
            info = p.get_host_api_info_by_index(0)
            output_devices = []
            
            for i in range(info.get('deviceCount')):
                device_info = p.get_device_info_by_host_api_device_index(0, i)
                if device_info.get('maxOutputChannels') > 0:
                    output_devices.append((i, device_info.get('name')))
            
            if output_devices:
                logging.info(f"Dispositivos de saída disponíveis:")
                for idx, name in output_devices:
                    logging.info(f"  [{idx}] {name}")
                
                # Usar o primeiro dispositivo de saída disponível
                output_device_idx = output_devices[0][0]
                logging.info(f"Usando dispositivo de saída: [{output_device_idx}] {output_devices[0][1]}")
            else:
                output_device_idx = None
                logging.warning("Nenhum dispositivo de saída encontrado! Verifique sua configuração de áudio.")
            
            # Configurar buffer de áudio maior para melhor qualidade de reprodução
            # Usando try/except para capturar erros específicos de inicialização de stream
            try:
                stream = p.open(format=self.format, 
                              channels=self.channels, 
                              rate=self.sample_rate, 
                              output=True, 
                              output_device_index=output_device_idx,
                              frames_per_buffer=1024,
                              start=True)  # Começar imediatamente
                logging.info("Stream de saída de áudio iniciado com sucesso")
            except Exception as e:
                logging.error(f"Erro ao inicializar o stream de saída: {e}")
                raise  # Re-lançar para ser capturado pelo bloco try/except externo
            
            last_audio_time = 0
            audio_count = 0
            audio_buffer = []  # Buffer para acumular pacotes antes de reproduzir
            buffer_size = 3  # Número de pacotes a acumular antes de reproduzir (ajustável)
            max_buffer_size = 10  # Limite máximo para evitar atrasos muito grandes
            
            # Configurar timeout para recepção de socket para evitar bloqueio indefinido
            self.socket.settimeout(0.5)  # 500ms timeout
            
            empty_packet_count = 0
            max_empty_packets = 10  # Número máximo de pacotes vazios antes de considerar desconexão
            
            try:
                while self.running:
                    try:
                        header = self.socket.recv(3)
                        if not header or len(header) < 3: 
                            empty_packet_count += 1
                            logging.warning(f"Recebido header incompleto ({empty_packet_count}/{max_empty_packets})")
                            
                            if empty_packet_count >= max_empty_packets:
                                logging.warning("Muitos pacotes vazios recebidos, encerrando conexão...")
                                break
                                
                            # Pequena pausa para evitar loop rápido
                            time.sleep(0.1)
                            continue
                        
                        # Resetar contador de pacotes vazios
                        empty_packet_count = 0
                            
                        kind, length = header[0], struct.unpack('>H', header[1:3])[0]
                        
                        # Verificação de segurança para tamanho de pacote
                        if length > 16384:  # Limitar a 16KB por pacote
                            logging.warning(f"Tamanho de pacote suspeito: {length} bytes, ignorando")
                            continue
                        
                        # Usar timeout para evitar bloqueio na recepção de payload
                        self.socket.settimeout(0.2)  # 200ms timeout para receber payload
                        payload = self.socket.recv(length)
                        self.socket.settimeout(0.5)  # Restaurar timeout normal
                        
                        if kind == KIND_SLIN:
                            # Verificar se o payload tem o tamanho esperado
                            if len(payload) != length:
                                logging.warning(f"Payload incompleto: esperado {length}, recebido {len(payload)}")
                                continue
                                
                            # Acumular pacotes no buffer para reprodução mais suave
                            audio_buffer.append(payload)
                            
                            # Ajuste dinâmico do tamanho do buffer baseado em condições
                            # Se estivermos recebendo pacotes muito rapidamente, aumentar o buffer
                            if last_audio_time > 0:
                                time_diff = time.time() - last_audio_time
                                if time_diff < 0.01 and buffer_size < max_buffer_size:  # Pacotes chegando muito rápido
                                    buffer_size += 1
                                elif time_diff > 0.05 and buffer_size > 2:  # Pacotes chegando com atraso
                                    buffer_size -= 1
                            
                            # Quando tivermos pacotes suficientes acumulados ou buffer muito grande, reproduzir
                            if len(audio_buffer) >= buffer_size or len(audio_buffer) >= max_buffer_size:
                                try:
                                    combined_payload = b''.join(audio_buffer)
                                    stream.write(combined_payload, exception_on_underflow=False)
                                    audio_buffer = []  # Limpar buffer após reprodução
                                except Exception as e:
                                    logging.error(f"Erro ao reproduzir áudio: {e}")
                                    # Limpar buffer em caso de erro para evitar acúmulo
                                    audio_buffer = []
                            
                            audio_count += 1
                            
                            # A cada 50 pacotes de áudio, mostramos um indicador
                            if audio_count % 50 == 0:
                                current_time = time.time()
                                if last_audio_time > 0:
                                    rate = 50 / (current_time - last_audio_time)
                                    latency = len(audio_buffer) * (self.chunk_size / self.sample_rate)
                                    logging.info(f"Recebendo áudio: {rate:.1f} pacotes/s, buffer={buffer_size}, latência={latency*1000:.1f}ms")
                                last_audio_time = current_time
                        elif kind == KIND_HANGUP:
                            logging.info("Recebido sinal de encerramento (HANGUP)")
                            break
                        else:
                            logging.debug(f"Recebido pacote não-SLIN: kind={kind}, length={length}")
                    
                    except socket.timeout:
                        # Timeout na recepção - normal durante períodos sem áudio
                        # Reproduzir qualquer áudio pendente no buffer para evitar atraso
                        if audio_buffer:
                            try:
                                combined_payload = b''.join(audio_buffer)
                                stream.write(combined_payload, exception_on_underflow=False)
                                audio_buffer = []
                            except Exception as e:
                                logging.error(f"Erro ao reproduzir áudio após timeout: {e}")
                                audio_buffer = []
                    
                    except ConnectionResetError:
                        logging.error("Conexão fechada pelo servidor")
                        break
                        
                    except Exception as e:
                        logging.error(f"Erro durante processamento de pacote: {e}")
                        # Pequena pausa para evitar loop rápido em caso de erro persistente
                        time.sleep(0.1)
                
            except Exception as e:
                logging.error(f"Erro no loop principal de recebimento: {e}")
            
            finally:
                # Reproduzir qualquer áudio restante no buffer
                if audio_buffer:
                    try:
                        combined_payload = b''.join(audio_buffer)
                        stream.write(combined_payload, exception_on_underflow=False)
                    except:
                        pass
                
                # Limpar recursos de áudio
                logging.info("Fechando stream de saída")
                try:
                    stream.stop_stream()
                    stream.close()
                except Exception as e:
                    logging.error(f"Erro ao fechar stream de saída: {e}")
        
        except Exception as e:
            logging.error(f"Erro fatal na inicialização de áudio: {e}")
        
        finally:
            # Sinalizar que a thread está encerrando
            self.running = False
            
            # Limpar recursos de PyAudio
            try:
                p.terminate()
            except Exception as e:
                logging.error(f"Erro ao terminar PyAudio: {e}")
                
            logging.info("Thread de recebimento de áudio encerrada")

    def disconnect(self):
        # Primeiro, marcar como não rodando para as threads pararem
        self.running = False
        logging.info("Iniciando processo de desconexão...")
        
        # Esperar threads encerrarem com timeout
        threads_to_wait = []
        
        if hasattr(self, 'send_thread') and self.send_thread and self.send_thread.is_alive():
            threads_to_wait.append(('send_thread', self.send_thread))
            
        if hasattr(self, 'receive_thread') and self.receive_thread and self.receive_thread.is_alive():
            threads_to_wait.append(('receive_thread', self.receive_thread))
        
        # Aguardar threads encerrarem com timeout
        if threads_to_wait:
            logging.info(f"Aguardando {len(threads_to_wait)} threads encerrarem...")
            for thread_name, thread in threads_to_wait:
                # Aguardar no máximo 3 segundos por thread
                thread.join(timeout=3.0)
                if thread.is_alive():
                    logging.warning(f"Thread {thread_name} não encerrou no tempo esperado")
                else:
                    logging.info(f"Thread {thread_name} encerrada com sucesso")
        
        # Enviar sinal de HANGUP e fechar socket
        try:
            # Verificar se o socket existe e está conectado
            if hasattr(self, 'socket') and self.socket:
                # Configurar timeout curto para operações finais
                try:
                    self.socket.settimeout(1.0)
                except:
                    pass
                
                # Enviar comando de HANGUP
                try:
                    logging.info("Enviando sinal de HANGUP...")
                    self.socket.sendall(struct.pack('>B H', KIND_HANGUP, 0))
                    logging.info("Sinal de HANGUP enviado com sucesso")
                except (OSError, BrokenPipeError, socket.timeout) as e:
                    logging.warning(f"Não foi possível enviar comando de hangup: {e}")
                except Exception as e:
                    logging.warning(f"Erro inesperado ao enviar HANGUP: {e}")
                
                # Encerrar socket
                try:
                    logging.info("Fechando socket...")
                    # Primeiro shutdown para indicar que não haverá mais dados
                    try:
                        self.socket.shutdown(socket.SHUT_RDWR)
                    except:
                        pass
                    
                    # Depois fechar o socket
                    self.socket.close()
                    logging.info("Socket fechado com sucesso")
                except Exception as e:
                    logging.warning(f"Erro ao fechar socket: {e}")
                
                # Limpar referência ao socket
                self.socket = None
                    
            logging.info("Desconexão concluída.")
        except Exception as e:
            logging.error(f"Erro durante a desconexão: {e}")
        
        # Garantir que o estado final é consistente
        self.running = False

if __name__ == "__main__":
    import argparse
    import signal
    
    # Adicionar opções de linha de comando
    parser = argparse.ArgumentParser(description='Cliente de microfone para AudioSocket')
    parser.add_argument('--host', default='127.0.0.1', help='Endereço do servidor (padrão: 127.0.0.1)')
    parser.add_argument('--port', type=int, default=8080, help='Porta do servidor (padrão: 8080)')
    parser.add_argument('--verbose', '-v', action='store_true', help='Habilitar logs detalhados')
    parser.add_argument('--quiet', '-q', action='store_true', help='Mostrar apenas logs de erro')
    args = parser.parse_args()
    
    # Configurar nível de log com base nos argumentos
    if args.verbose:
        logging.getLogger().setLevel(logging.DEBUG)
        logging.info("Modo verbose ativado: logs detalhados habilitados")
    elif args.quiet:
        logging.getLogger().setLevel(logging.ERROR)
    
    # Preparar cliente
    client = None
    
    # Handler para sinal de interrupção (Ctrl+C)
    def signal_handler(sig, frame):
        logging.info("Sinal de interrupção recebido. Encerrando...")
        if client and client.running:
            client.disconnect()
        
    # Registrar handler de sinal
    signal.signal(signal.SIGINT, signal_handler)
    
    try:
        # Usar os valores fornecidos pelo usuário ou os padrões
        logging.info(f"Iniciando cliente para servidor {args.host}:{args.port}")
        
        # Inicializar o cliente
        client = AudioSocketClient(host=args.host, port=args.port)
        
        # Tentar conectar ao servidor
        try:
            client.connect()
            logging.info("Conexão estabelecida. Pressione Ctrl+C para encerrar")
        except ConnectionRefusedError:
            logging.error(f"Não foi possível conectar ao servidor {args.host}:{args.port}")
            logging.error("Verifique se o servidor está em execução e a porta está correta")
            exit(1)
        except Exception as e:
            logging.error(f"Erro durante a conexão: {e}")
            exit(1)
        
        # Verificar periodicamente se o cliente ainda está rodando
        # e monitorar saúde da conexão
        monitor_interval = 5.0  # verificar a cada 5 segundos
        last_check_time = time.time()
        
        # Loop principal
        while client.running:
            time.sleep(0.1)  # Pequeno delay para não consumir CPU
            
            # Verificações periódicas
            current_time = time.time()
            if current_time - last_check_time >= monitor_interval:
                if hasattr(client, 'send_thread') and not client.send_thread.is_alive():
                    logging.error("Thread de envio de áudio encerrada inesperadamente!")
                    break
                    
                if hasattr(client, 'receive_thread') and not client.receive_thread.is_alive():
                    logging.error("Thread de recebimento de áudio encerrada inesperadamente!")
                    break
                
                last_check_time = current_time
        
    except KeyboardInterrupt:
        # Este bloco é um fallback - normalmente o signal_handler lidará com Ctrl+C
        logging.info("Interrupção detectada. Encerrando conexão...")
    except Exception as e:
        logging.error(f"Erro inesperado: {e}")
        import traceback
        logging.error(traceback.format_exc())
    finally:
        # Garantir que o cliente se desconecte adequadamente
        if client and client.running:
            try:
                client.disconnect()
            except Exception as e:
                logging.error(f"Erro ao desconectar: {e}")
        
        logging.info("Cliente encerrado. Até mais!")
        
    # Forçar a saída para garantir que não há threads bloqueando
    import os, sys
    try:
        sys.exit(0)
    except SystemExit:
        os._exit(0)


code_gpt.py:
import os

IGNORE = {'__pycache__', '.env', '.git', '.idea', 'audio', "*.md", "logs"}

def list_structure(base_path, prefix=""):
    tree = ""
    items = sorted(os.listdir(base_path))
    for item in items:
        if item in IGNORE:
            continue
        path = os.path.join(base_path, item)
        if os.path.isdir(path):
            tree += f"{prefix}{item}/\n"
            tree += list_structure(path, prefix + "    ")
        else:
            tree += f"{prefix}{item}\n"
    return tree

def list_code_files(base_path):
    code = ""
    for root, dirs, files in os.walk(base_path):
        dirs[:] = [d for d in dirs if d not in IGNORE]
        for file in files:
            if file in IGNORE:
                continue
            file_path = os.path.join(root, file)
            rel_path = os.path.relpath(file_path, base_path)
            code += f"\n{rel_path}:\n"
            try:
                with open(file_path, "r", encoding="utf-8") as f:
                    code += f.read() + "\n"
            except Exception as e:
                code += f"Erro ao ler arquivo: {e}\n"
    return code

def main():
    base_path = "."
    structure = list_structure(base_path)
    code_files = list_code_files(base_path)

    with open("code.txt", "w", encoding="utf-8") as f:
        f.write("Estrutura do projeto:\n")
        f.write(structure)
        f.write("\nCódigo fonte:\n")
        f.write(code_files)

    print("Arquivo 'code.txt' criado com sucesso.")

if __name__ == "__main__":
    main()


conversation_flow.py:
# conversation_flow.py

import logging
import time
from enum import Enum, auto
from typing import Optional

from ai.crew import process_user_message_with_coordinator
from ai.tools import validar_intent_com_fuzzy

import pika
import json
import asyncio

logger = logging.getLogger(__name__)

class FlowState(Enum):
    COLETANDO_DADOS = auto()
    VALIDADO = auto()
    CHAMANDO_MORADOR = auto()
    CALLING_IN_PROGRESS = auto()  # Estado para processos de chamada em andamento (sem notificar o visitante)
    ESPERANDO_MORADOR = auto()
    FINALIZADO = auto()

class ConversationFlow:
    """
    Define o fluxo de interação entre visitante e morador, passo a passo.
    """

    def __init__(self, extension_manager=None):
        self.state = FlowState.COLETANDO_DADOS
        self.intent_data = {}
        self.is_fuzzy_valid = False
        self.voip_number_morador: Optional[str] = None
        self.extension_manager = extension_manager

        # Para controlar tentativas de chamada
        self.tentativas_chamada = 0
        self.max_tentativas = 2
        self.call_timeout_seconds = 10  # Tempo para aguardar antes de tentar novamente
        self.calling_task = None  # Referência para a tarefa assíncrona de chamada

    # ---------------
    # VISITOR
    # ---------------
    def on_visitor_message(self, session_id: str, text: str, session_manager):
        logger.debug(f"[Flow] Visitor message in state={self.state}, text='{text}'")

        if self.state == FlowState.COLETANDO_DADOS:
            try:
                # Adicionar timeout para prevenção de bloqueio
                result = process_user_message_with_coordinator(session_id, text)
                logger.debug(f"[Flow] result IA: {result}")
                
                # Verificar se o resultado é None ou está vazio
                if result is None:
                    logger.error(f"[Flow] IA retornou resultado None para '{text}'")
                    session_manager.enfileirar_visitor(
                        session_id,
                        "Desculpe, tive um problema ao processar sua mensagem. Por favor, repita ou informe novamente seus dados."
                    )
                    return
                
                # Se a chamada ao morador está em progresso, não processamos novas entradas do visitante
                if self.state in [FlowState.CALLING_IN_PROGRESS, FlowState.ESPERANDO_MORADOR]:
                    logger.info(f"[Flow] Ignorando entrada do visitante durante estado {self.state}")
                    return
                
                # Atualiza self.intent_data com quaisquer dados retornados
                if "dados" in result:
                    for k, v in result["dados"].items():
                        self.intent_data[k] = v
                else:
                    logger.warning(f"[Flow] Resultado sem campo 'dados': {result}")
                    
                # Log de segurança para entender o estado atual
                logger.info(f"[Flow] Dados acumulados: {self.intent_data}")

                # Se veio alguma mensagem para o visitante, enfileira
                if "mensagem" in result:
                    session_manager.enfileirar_visitor(session_id, result["mensagem"])
                else:
                    # Mensagem de fallback caso não tenha mensagem no resultado
                    session_manager.enfileirar_visitor(
                        session_id,
                        "Por favor, continue informando os dados necessários."
                    )

                # Se valid_for_action, tentamos fuzzy
                if result.get("valid_for_action"):
                    # Verificação de segurança nos dados antes da validação fuzzy
                    apt = self.intent_data.get("apartment_number", "").strip()
                    resident = self.intent_data.get("resident_name", "").strip()
                    
                    # Log detalhado para cada etapa
                    logger.info(f"[Flow] Preparando para validação fuzzy com: apt={apt}, resident={resident}, data={self.intent_data}")
                    
                    if not apt or not resident:
                        logger.warning(f"[Flow] Dados incompletos antes do fuzzy: apt={apt}, resident={resident}")
                        session_manager.enfileirar_visitor(
                            session_id,
                            "Preciso do número do apartamento e nome do morador para continuar."
                        )
                        return
                    
                    # Verificação extra para depuração
                    logger.info(f"[Flow] Iniciando validação fuzzy com intent_data: {self.intent_data}")
                    
                    # Executa a validação fuzzy
                    fuzzy_res = validar_intent_com_fuzzy(self.intent_data)
                    logger.info(f"[Flow] Resultado do fuzzy: {fuzzy_res}")

                    if fuzzy_res["status"] == "válido":
                        self.is_fuzzy_valid = True
                        self.voip_number_morador = fuzzy_res.get("voip_number")
                        
                        # Atualizar o intent_data com o nome correto do apartamento/morador
                        if "apartment_number" in fuzzy_res:
                            self.intent_data["apartment_number"] = fuzzy_res["apartment_number"]
                        if "match_name" in fuzzy_res:
                            self.intent_data["resident_name"] = fuzzy_res["match_name"]
                            
                        self.state = FlowState.VALIDADO

                        # Mensagem única ao visitante (sem informar detalhes das tentativas)
                        session_manager.enfileirar_visitor(
                            session_id,
                            "Aguarde enquanto entramos em contato com o morador..."
                        )

                        # Avança para CHAMANDO_MORADOR e inicia o processo de chamada
                        self.state = FlowState.CHAMANDO_MORADOR
                        # Iniciar o processo de chamada como uma task assíncrona
                        loop = asyncio.get_event_loop()
                        self.calling_task = loop.create_task(self.iniciar_processo_chamada(session_id, session_manager))
                    else:
                        # Mensagem com mais detalhes sobre o motivo da falha
                        if "best_match" in fuzzy_res and fuzzy_res.get("best_score", 0) > 50:
                            session_manager.enfileirar_visitor(
                                session_id,
                                f"Encontrei um morador similar ({fuzzy_res['best_match']}), mas preciso que confirme o apartamento e nome corretos."
                            )
                        else:
                            session_manager.enfileirar_visitor(
                                session_id,
                                f"Desculpe, dados inválidos: {fuzzy_res.get('reason','motivo')}. Vamos tentar novamente."
                            )
            except Exception as e:
                # Tratamento de erro global para evitar travar o fluxo
                logger.error(f"[Flow] Erro no processamento: {str(e)}")
                session_manager.enfileirar_visitor(
                    session_id,
                    "Desculpe, ocorreu um erro ao processar sua solicitação. Por favor, tente novamente."
                )

        elif self.state == FlowState.CHAMANDO_MORADOR or self.state == FlowState.CALLING_IN_PROGRESS:
            # Não atualizamos o visitante durante o processo de chamada
            # apenas log para debug
            logger.debug(f"[Flow] Visitante tentou interagir durante processo de chamada em state={self.state}")

        elif self.state == FlowState.ESPERANDO_MORADOR:
            session_manager.enfileirar_visitor(
                session_id,
                "O morador está na linha. Aguarde a resposta."
            )

        elif self.state == FlowState.FINALIZADO:
            session_manager.enfileirar_visitor(session_id, "A chamada já foi encerrada. Obrigado.")
        else:
            session_manager.enfileirar_visitor(session_id, "Aguarde, por favor.")

    # ---------------
    # RESIDENT
    # ---------------
    def on_resident_message(self, session_id: str, text: str, session_manager):
        logger.debug(f"[Flow] Resident message in state={self.state}, text='{text}'")

        # Detectar conexão de áudio do morador (trigger especial do socket)
        is_connection_trigger = text == "AUDIO_CONNECTION_ESTABLISHED"
        
        if (self.state == FlowState.CHAMANDO_MORADOR or self.state == FlowState.CALLING_IN_PROGRESS) and (is_connection_trigger or text):
            # Mensagem especial para log quando é o gatilho de conexão
            if is_connection_trigger:
                logger.info(f"[Flow] Detectada conexão de áudio do morador para session_id={session_id}")
            else:
                logger.info(f"[Flow] Morador atendeu e começou a falar: '{text}'")
                
            # Em qualquer caso, mudar para o estado de espera de resposta
            self.state = FlowState.ESPERANDO_MORADOR
            logger.info(f"[Flow] Morador atendeu chamada para sessão {session_id}. Mudando para estado ESPERANDO_MORADOR")
            
            # Verificar se temos os dados necessários para continuar
            visitor_name = self.intent_data.get("interlocutor_name", "")
            intent_type = self.intent_data.get("intent_type", "")
            apt = self.intent_data.get("apartment_number", "")
            
            if not visitor_name or not intent_type or not apt:
                logger.warning(f"[Flow] Dados incompletos ao atender morador: visitor={visitor_name}, intent={intent_type}, apt={apt}")
                visitor_name = visitor_name or "Um visitante"
                intent_type = intent_type or "acesso"
                apt = apt or "[não identificado]"
            
            # Mensagem detalhada para o morador com o contexto da visita
            intent_desc = {
                "entrega": "uma entrega",
                "visita": "uma visita",
                "servico": "um serviço",
            }.get(intent_type, "um acesso")
            
            # Mensagem de saudação com pausa para evitar que a chamada caia imediatamente
            initial_greeting = f"Olá, morador do apartamento {apt}. Um momento por favor..."
            session_manager.enfileirar_resident(session_id, initial_greeting)
            
            # Aguardar 1 segundo antes de enviar a próxima mensagem
            # Isso será processado assincronamente por enviar_mensagens_morador
            
            # Mensagem principal com os detalhes da visita
            resident_msg = (f"{visitor_name} está na portaria solicitando {intent_desc}. "
                           f"Você autoriza a entrada? Responda SIM ou NÃO.")
            session_manager.enfileirar_resident(session_id, resident_msg)
            
            # Notificar o visitante que o morador atendeu
            session_manager.enfileirar_visitor(session_id, "O morador atendeu. Aguarde enquanto verificamos sua autorização...")

        elif self.state == FlowState.ESPERANDO_MORADOR:
            # Processamento da resposta do morador
            lower_text = text.lower()
            visitor_name = self.intent_data.get("interlocutor_name", "Visitante")
            
            # Verificar se contém pergunta antes de checar sim/não
            if "quem" in lower_text or "?" in lower_text:
                # Morador está pedindo mais informações
                intent_type = self.intent_data.get("intent_type", "")
                apt = self.intent_data.get("apartment_number", "")
                
                # Mensagem detalhada sobre o visitante
                additional_info = f"{visitor_name} está na portaria para {intent_type}. "
                if intent_type == "entrega":
                    additional_info += "É uma entrega para seu apartamento."
                elif intent_type == "visita":
                    additional_info += "É uma visita pessoal."
                
                # Aguarda decisão após fornecer mais informações
                session_manager.enfileirar_resident(
                    session_id,
                    f"{additional_info} Por favor, responda SIM para autorizar ou NÃO para negar."
                )
                
            # Lista mais precisa e controlada de termos de aprovação - removida a opção de string vazia
            elif any(word in lower_text for word in ["sim", "autorizo", "pode entrar", "autorizado", "deixa entrar", "libera", "ok", "claro", "positivo"]) or text.strip().lower() == "sim" or text.strip().lower() == "s":
                # Morador autorizou
                logger.info(f"[Flow] Morador AUTORIZOU a entrada com resposta: '{text}'")
                
                # Intent type para mensagem personalizada
                intent_type = self.intent_data.get("intent_type", "")
                intent_msg = ""
                if intent_type == "entrega":
                    intent_msg = "entrega"
                elif intent_type == "visita":
                    intent_msg = "visita"
                else:
                    intent_msg = "entrada"
                
                # Mensagens personalizadas para o tipo de intent
                session_manager.enfileirar_resident(
                    session_id, 
                    f"Obrigado! {visitor_name} será informado que a {intent_msg} foi autorizada."
                )
                session_manager.enfileirar_visitor(
                    session_id, 
                    f"Ótima notícia! O morador autorizou sua {intent_msg}."
                )
                
                # Salvar resultado da autorização na sessão
                self.intent_data["authorization_result"] = "authorized"
                
                # Registrar log especial para sinalizar finalização
                logger.info(f"[Flow] Autorização CONCLUÍDA - alterando estado para FINALIZADO")
                
                # Atualizar o state e iniciar encerramento de forma controlada
                self.state = FlowState.FINALIZADO
                
                # Código para enviar mensagem AMQP para o sistema físico de autorização
                # Desabilitado temporariamente para uso futuro
                """
                from services.amqp_service import enviar_msg_autorizacao_morador
                
                # Criação do payload adequado
                payload = {
                    "call_id": session_id,
                    "action": "authorize",
                    "apartment": self.intent_data.get("apartment_number", ""),
                    "resident": self.intent_data.get("resident_name", ""),
                    "visitor": self.intent_data.get("interlocutor_name", ""),
                    "intent_type": self.intent_data.get("intent_type", "entrada"),
                    "authorization_result": "authorized"
                }
                
                # Envia assíncronamente para não bloquear o fluxo
                logger.info(f"[Flow] Enviando notificação de AUTORIZAÇÃO para sistema físico: {payload}")
                try:
                    success = enviar_msg_autorizacao_morador(payload)
                    if success:
                        logger.info(f"[Flow] Notificação enviada com sucesso para sistema físico")
                    else:
                        logger.error(f"[Flow] Falha ao enviar notificação para sistema físico")
                except Exception as e:
                    logger.error(f"[Flow] Erro ao notificar sistema físico: {str(e)}")
                """
                
                # Log para desenvolvimento
                logger.info(f"[Flow] Módulo AMQP para notificação de portaria desabilitado - uso futuro")
                
                # Finalmente, iniciar processo de finalização controlada
                self._finalizar(session_id, session_manager)
                
            # Lista expandida de termos de negação    
            elif any(word in lower_text for word in ["não", "nao", "nego", "negativa", "negado", "bloqueado", "barrado", "recusado", "nunca"]):
                # Morador negou
                logger.info(f"[Flow] Morador NEGOU a entrada com resposta: '{text}'")
                
                # Intent type para mensagem personalizada
                intent_type = self.intent_data.get("intent_type", "")
                intent_msg = ""
                if intent_type == "entrega":
                    intent_msg = "entrega"
                elif intent_type == "visita":
                    intent_msg = "visita"
                else:
                    intent_msg = "entrada"
                
                session_manager.enfileirar_resident(
                    session_id, 
                    f"Entendido. {visitor_name} será informado que a {intent_msg} não foi autorizada."
                )
                session_manager.enfileirar_visitor(
                    session_id, 
                    f"Infelizmente o morador não autorizou sua {intent_msg} neste momento."
                )
                
                # Salvar resultado da autorização na sessão
                self.intent_data["authorization_result"] = "denied"
                
                # Registrar log especial para sinalizar finalização
                logger.info(f"[Flow] Negação CONCLUÍDA - alterando estado para FINALIZADO")
                
                # Atualizar o state e iniciar encerramento de forma controlada
                self.state = FlowState.FINALIZADO
                
                # Código para enviar mensagem AMQP para o sistema físico de negação
                # Desabilitado temporariamente para uso futuro
                """
                from services.amqp_service import enviar_msg_autorizacao_morador
                
                # Criação do payload adequado
                payload = {
                    "call_id": session_id,
                    "action": "deny",
                    "apartment": self.intent_data.get("apartment_number", ""),
                    "resident": self.intent_data.get("resident_name", ""),
                    "visitor": self.intent_data.get("interlocutor_name", ""),
                    "intent_type": self.intent_data.get("intent_type", "entrada"),
                    "authorization_result": "denied"
                }
                
                # Envia assíncronamente para não bloquear o fluxo
                logger.info(f"[Flow] Enviando notificação de NEGAÇÃO para sistema físico: {payload}")
                try:
                    success = enviar_msg_autorizacao_morador(payload)
                    if success:
                        logger.info(f"[Flow] Notificação enviada com sucesso para sistema físico")
                    else:
                        logger.error(f"[Flow] Falha ao enviar notificação para sistema físico")
                except Exception as e:
                    logger.error(f"[Flow] Erro ao notificar sistema físico: {str(e)}")
                """
                
                # Log para desenvolvimento
                logger.info(f"[Flow] Módulo AMQP para notificação de portaria desabilitado - uso futuro")
                
                # Finalmente, iniciar processo de finalização controlada
                self._finalizar(session_id, session_manager)
                
            else:
                # Resposta não reconhecida
                session_manager.enfileirar_resident(
                    session_id, 
                    "Desculpe, não consegui entender sua resposta. Por favor, responda SIM para autorizar a entrada ou NÃO para negar."
                )

        elif self.state == FlowState.FINALIZADO:
            session_manager.enfileirar_resident(session_id, "O fluxo já foi finalizado. Obrigado.")

        elif self.state == FlowState.COLETANDO_DADOS:
            session_manager.enfileirar_resident(
                session_id,
                "Ainda estamos coletando dados do visitante. Aguarde um instante..."
            )

        else:
            # Estado VALIDADO ou outro
            session_manager.enfileirar_resident(session_id, "Ainda estou preparando a chamada, aguarde.")

    # ----------------------------------------------------
    #  PROCESSO DE CHAMADA AO MORADOR (ASSÍNCRONO)
    # ----------------------------------------------------
    async def iniciar_processo_chamada(self, session_id: str, session_manager):
        """
        Gerencia o processo completo de chamada ao morador de forma assíncrona,
        sem notificar o visitante sobre cada etapa.
        """
        # Log detalhado para diagnóstico
        logger.info(f"[Flow] Iniciando processo de chamada para morador: voip={self.voip_number_morador}, session_id={session_id}")
        logger.info(f"[Flow] Dados do intent: {self.intent_data}")
        
        if not self.voip_number_morador:
            logger.warning("[Flow] voip_number_morador está vazio, não posso discar.")
            session_manager.enfileirar_visitor(
                session_id,
                "Não foi possível entrar em contato com o morador. Tente novamente mais tarde."
            )
            self.state = FlowState.FINALIZADO
            self._finalizar(session_id, session_manager)
            return

        # Mudamos para o estado de processamento em andamento
        self.state = FlowState.CALLING_IN_PROGRESS
        
        # Realizar tentativas de chamada sem notificar o visitante
        while self.tentativas_chamada < self.max_tentativas:
            self.tentativas_chamada += 1
            logger.info(f"[Flow] Tentativa {self.tentativas_chamada} de chamar o morador {self.voip_number_morador}")
            
            try:
                # Enviar comando para fazer a ligação
                logger.info(f"[Flow] Enviando clicktocall para {self.voip_number_morador} na tentativa {self.tentativas_chamada}")
                
                success = self.enviar_clicktocall(self.voip_number_morador, session_id)
                
                if not success:
                    logger.error(f"[Flow] Falha ao enviar clicktocall na tentativa {self.tentativas_chamada}")
                    # Se falhou no envio e é a última tentativa, sair do loop
                    if self.tentativas_chamada >= self.max_tentativas:
                        break
                    
                    # Extra logging para diagnóstico
                    logger.error(f"[Flow] Dados para clicktocall que falharam: voip={self.voip_number_morador}, intent={self.intent_data}")
                    continue  # Tenta novamente na próxima iteração
                
                logger.info(f"[Flow] AMQP enviado com sucesso para origin={self.voip_number_morador}, tentativa={self.tentativas_chamada}")

                # Aguarda o timeout para ver se o morador atende
                for _ in range(self.call_timeout_seconds):
                    await asyncio.sleep(1)  # Verifica a cada 1 segundo
                    # Se o morador atendeu neste meio tempo, o estado terá mudado
                    if self.state == FlowState.ESPERANDO_MORADOR:
                        logger.info(f"[Flow] Morador atendeu na tentativa {self.tentativas_chamada}")
                        return  # Processo concluído com sucesso
                
                # Se chegou aqui, o timeout foi atingido e o morador não atendeu
                logger.info(f"[Flow] Timeout de {self.call_timeout_seconds}s atingido na tentativa {self.tentativas_chamada}")
                
            except Exception as e:
                logger.error(f"[Flow] Erro inesperado ao processar chamada: {e}")
                if self.tentativas_chamada >= self.max_tentativas:
                    break  # Sai do loop após a última tentativa falhar
            
            # Aguarda um breve período entre tentativas
            if self.tentativas_chamada < self.max_tentativas:
                await asyncio.sleep(1)  # Pequeno intervalo entre tentativas
        
        # Se todas as tentativas falharam, notifica o visitante
        logger.info(f"[Flow] Todas as {self.max_tentativas} tentativas de contato com o morador falharam")
        session_manager.enfileirar_visitor(
            session_id,
            "Não foi possível contatar o morador no momento. Por favor, tente novamente mais tarde."
        )
        
        # Finaliza o processo
        self.state = FlowState.FINALIZADO
        self._finalizar(session_id, session_manager)


    def enviar_clicktocall(self, morador_voip_number: str, guid: str):
        """
        Envia solicitação de chamada para o morador via AMQP, garantindo
        que o mesmo GUID da sessão original seja utilizado como identificador.
        """
        rabbit_host = 'mqdev.tecnofy.com.br'
        rabbit_user = 'fonia'
        rabbit_password = 'fonia123'
        rabbit_vhost = 'voip'
        queue_name = 'api-to-voip1'

        # Verificação de segurança - GUID não pode estar vazio
        if not guid or len(guid) < 8:
            logger.error(f"[Flow] GUID inválido para clicktocall: '{guid}'")
            return False

        # Verificação de segurança - número do morador não pode estar vazio
        if not morador_voip_number:
            logger.error(f"[Flow] Número do morador inválido: '{morador_voip_number}'")
            return False

        try:
            # Se temos um extension_manager, tentamos obter o ramal de retorno correto
            ramal_retorno = morador_voip_number
            if self.extension_manager:
                ext_info = self.extension_manager.get_extension_info(call_id=guid)
                if ext_info:
                    ramal_retorno = ext_info.get('ramal_retorno', morador_voip_number)
                    logger.info(f"[Flow] Usando ramal de retorno dinâmico: {ramal_retorno} para sessão {guid}")
                else:
                    logger.warning(f"[Flow] Usando ramal de retorno padrão: {morador_voip_number}, pois não encontrei configuração dinâmica")
            else:
                logger.warning(f"[Flow] Extension manager não disponível, usando ramal padrão: {morador_voip_number}")

            credentials = pika.PlainCredentials(rabbit_user, rabbit_password)
            parameters = pika.ConnectionParameters(
                host=rabbit_host,
                virtual_host=rabbit_vhost,
                credentials=credentials
            )

            connection = pika.BlockingConnection(parameters)
            channel = connection.channel()
            channel.queue_declare(queue=queue_name, durable=True)

            # Timestamp atual para o evento
            current_timestamp = int(time.time())

            # IMPORTANTE: Garantir que o mesmo GUID da sessão seja usado
            # na chamada para o morador, para que os contextos se conectem
            payload = {
                "data": {
                    "destiny": "IA",
                    "guid": guid,  # GUID da sessão original
                    "license": "123456789012",
                    "origin": ramal_retorno  # Usando o ramal dinâmico ou o padrão
                },
                "operation": {
                    "eventcode": "8001",
                    "guid": "cmd-" + guid,
                    "msg": "",
                    "timestamp": current_timestamp,
                    "type": "clicktocall"
                }
            }

            channel.basic_publish(
                exchange='',
                routing_key=queue_name,
                body=json.dumps(payload)
            )
            
            logger.info(f"[Flow] Mensagem AMQP enviada: origin={ramal_retorno}, guid={guid}, timestamp={current_timestamp}")
            connection.close()
            return True
            
        except Exception as e:
            logger.error(f"[Flow] Erro ao enviar AMQP clicktocall: {e}")
            return False

    # ----------------------------------------------------
    # FINALIZAR (chamar end_session, etc.)
    # ----------------------------------------------------
    def _finalizar(self, session_id: str, session_manager):
        """
        Prepara o encerramento controlado da conversa e das conexões.
        
        1. Envia mensagens finais a ambos os participantes
        2. Aciona o mecanismo de encerramento controlado
        3. O session_manager sinaliza que as conexões devem ser encerradas
        4. As tasks assíncronas de audiosocket detectam o sinal e encerram graciosamente
        """
        logger.info(f"[Flow] Iniciando encerramento controlado da sessão {session_id}")
        
        # Carregar intenções
        authorization_result = self.intent_data.get("authorization_result", "")
        intent_type = self.intent_data.get("intent_type", "entrada")
        logger.info(f"[Flow] Finalizando com authorization_result={authorization_result}, intent_type={intent_type}")
            
        # Mensagens para os participantes
        if self.state in [FlowState.CHAMANDO_MORADOR, FlowState.CALLING_IN_PROGRESS, FlowState.ESPERANDO_MORADOR, FlowState.FINALIZADO]:
            # Se o morador estava envolvido, avisar ambos
            session_manager.enfileirar_resident(
                session_id, 
                "A conversa foi finalizada. Obrigado pela sua resposta."
            )
            
            # O texto para o visitante depende do resultado da autorização
            authorization_result = self.intent_data.get("authorization_result", "")
            intent_type = self.intent_data.get("intent_type", "entrada")
            
            # Verificar se é o caso de teste para o KIND_HANGUP
            is_test_hangup = self.intent_data.get("test_hangup", False)
            
            if is_test_hangup:
                # Definir flag específica para teste de hangup
                session = session_manager.get_session(session_id)
                if session:
                    session.intent_data["test_hangup"] = True
                    logger.info(f"[Flow] Flag de teste KIND_HANGUP ativada para sessão {session_id}")
                
                # Usar mensagem de finalização específica para teste
                session_manager.enfileirar_visitor(
                    session_id,
                    "A chamada com o morador foi finalizada. Obrigado por utilizar nosso sistema."
                )
            elif authorization_result == "authorized":
                if intent_type == "entrega":
                    session_manager.enfileirar_visitor(
                        session_id,
                        "Sua entrega foi autorizada pelo morador. Finalizando a chamada."
                    )
                elif intent_type == "visita":
                    session_manager.enfileirar_visitor(
                        session_id, 
                        "Sua visita foi autorizada pelo morador. Finalizando a chamada."
                    )
                else:
                    session_manager.enfileirar_visitor(
                        session_id,
                        "Sua entrada foi autorizada pelo morador. Finalizando a chamada."
                    )
            elif authorization_result == "denied":
                session_manager.enfileirar_visitor(
                    session_id,
                    "Sua entrada não foi autorizada pelo morador. Finalizando a chamada."
                )
            else:
                session_manager.enfileirar_visitor(
                    session_id,
                    "A chamada com o morador foi finalizada. Obrigado por utilizar nosso sistema."
                )
        else:
            # Caso padrão (apenas visitante)
            session_manager.enfileirar_visitor(
                session_id,
                "Conversa finalizada. Obrigado por utilizar nosso sistema."
            )
        
        # Utilizar encerramento ativo KIND_HANGUP após um delay para permitir que todas as mensagens
        # sejam enviadas e ouvidas
        self._schedule_active_hangup(session_id, session_manager)
        logger.info(f"[Flow] Finalização programada com encerramento ativo KIND_HANGUP para sessão {session_id}")
    
    def _schedule_active_hangup(self, session_id: str, session_manager, delay=5.0):
        """
        Agenda o envio de KIND_HANGUP ativo após um delay para encerrar a chamada.
        O delay permite que todas as mensagens de áudio sejam reproduzidas primeiro.
        
        Args:
            session_id: ID da sessão
            session_manager: Gerenciador de sessões
            delay: Tempo em segundos para aguardar antes de enviar KIND_HANGUP (padrão: 5s)
        """
        import asyncio
        
        async def send_hangup_after_delay():
            # Aguardar o delay para permitir que as mensagens sejam enviadas
            await asyncio.sleep(delay)
            
            # Verificar se a sessão ainda existe
            session = session_manager.get_session(session_id)
            if not session:
                logger.info(f"[Flow] Sessão {session_id} já foi encerrada antes do KIND_HANGUP")
                return
                
            try:
                # Importar ResourceManager para acessar conexões ativas
                from extensions.resource_manager import resource_manager
                import struct
                
                # Enviar KIND_HANGUP para o visitante
                visitor_conn = resource_manager.get_active_connection(session_id, "visitor")
                if visitor_conn and 'writer' in visitor_conn:
                    try:
                        logger.info(f"[Flow] Enviando KIND_HANGUP ativo para visitante na sessão {session_id}")
                        visitor_conn['writer'].write(struct.pack('>B H', 0x00, 0))
                        await visitor_conn['writer'].drain()
                    except ConnectionResetError:
                        logger.info(f"[Flow] Conexão do visitante já foi resetada durante envio de KIND_HANGUP - comportamento normal")
                    except Exception as e:
                        logger.warning(f"[Flow] Erro ao enviar KIND_HANGUP para visitante: {e}")
                else:
                    logger.warning(f"[Flow] Conexão do visitante não encontrada para enviar KIND_HANGUP na sessão {session_id}")
                
                # Enviar KIND_HANGUP para o morador (se existir conexão)
                resident_conn = resource_manager.get_active_connection(session_id, "resident")
                if resident_conn and 'writer' in resident_conn:
                    try:
                        logger.info(f"[Flow] Enviando KIND_HANGUP ativo para morador na sessão {session_id}")
                        resident_conn['writer'].write(struct.pack('>B H', 0x00, 0))
                        await resident_conn['writer'].drain()
                    except ConnectionResetError:
                        logger.info(f"[Flow] Conexão do morador já foi resetada durante envio de KIND_HANGUP - comportamento normal")
                    except Exception as e:
                        logger.warning(f"[Flow] Erro ao enviar KIND_HANGUP para morador: {e}")
                
                # Após enviar os KIND_HANGUP, aguardar um pouco e finalizar a sessão completamente
                await asyncio.sleep(1.0)
                session_manager.end_session(session_id)
                
                # Uma limpeza final após mais um pequeno delay
                await asyncio.sleep(1.0)
                if session_manager.get_session(session_id):
                    logger.info(f"[Flow] Forçando limpeza final da sessão {session_id}")
                    session_manager._complete_session_termination(session_id)
                    
            except Exception as e:
                logger.error(f"[Flow] Erro ao enviar KIND_HANGUP ativo: {e}", exc_info=True)
                
                # Em caso de erro, tentar finalizar a sessão do modo tradicional
                session_manager.end_session(session_id)
        
        # Criar e iniciar a tarefa assíncrona
        loop = asyncio.get_event_loop()
        task = loop.create_task(send_hangup_after_delay())
        
        # Não aguardamos a conclusão da tarefa para não bloquear o fluxo


.env.example:
# Configurações do banco de dados
DB_NAME=postgres
DB_USER=admincd
DB_PASSWORD=Isabela@2022!!
DB_HOST=dev-postgres-cd.postgres.database.azure.com
DB_PORT=5432

# Configurações do servidor
API_PORT=8082

# Configurações do Azure Speech
AZURE_SPEECH_KEY=sua_chave_aqui
AZURE_SPEECH_REGION=sua_regiao

# Outras configurações
SILENCE_THRESHOLD_SECONDS=1.5
AI_API_URL=http://localhost:8000/messages

main.py:
import asyncio
import logging
import os
from dotenv import load_dotenv
from audiosocket_handler import iniciar_servidor_audiosocket_visitante, iniciar_servidor_audiosocket_morador, set_extension_manager
from speech_service import pre_sintetizar_frases_comuns
from extensions.api_server import APIServer
from extensions.server_manager import ServerManager
from extensions.config_persistence import ConfigPersistence
from extensions.db_connector import DBConnector

logging.basicConfig(level=logging.INFO)

load_dotenv()

# Inicializar diretório de logs se não existir
if not os.path.exists('logs'):
    os.makedirs('logs', exist_ok=True)

async def main():
    # Pré-sintetizar frases comuns para reduzir latência
    logging.info("Pré-sintetizando frases comuns...")
    pre_sintetizar_frases_comuns()
    
    # Configurar componentes para o servidor web
    server_manager = ServerManager()
    config_persistence = ConfigPersistence()
    
    # Configurar com modo de compatibilidade se não conseguir conectar ao DB
    try:
        db_connector = DBConnector()
        # Testar a conexão para verificar se está funcionando
        if not db_connector.test_connection():
            raise Exception("Teste de conexão falhou")
        logging.info("Conexão com banco de dados estabelecida com sucesso")
    except Exception as e:
        logging.warning(f"Não foi possível conectar ao banco de dados: {e}")
        logging.warning("Utilizando modo de compatibilidade com MockDBConnector")
        from extensions.mock_db_connector import MockDBConnector
        db_connector = MockDBConnector()
    
    # Passar o extension_manager para o audiosocket_handler
    set_extension_manager(server_manager)
    
    # Iniciar servidor para visitantes
    server_visitante = await asyncio.start_server(iniciar_servidor_audiosocket_visitante, '0.0.0.0', 8080)
    logging.info("Servidor AudioSocket para VISITANTES iniciado na porta 8080")
    
    # Iniciar servidor para moradores 
    # IMPORTANTE: Este servidor deve receber conexões com o mesmo GUID
    # da sessão original do visitante para manter o contexto da conversa
    server_morador = await asyncio.start_server(iniciar_servidor_audiosocket_morador, '0.0.0.0', 8081)
    logging.info("Servidor AudioSocket para MORADORES iniciado na porta 8081")
    
    # Iniciar servidor web na porta 8082 para API
    api_server = APIServer(server_manager, config_persistence, db_connector)
    api_port = int(os.getenv('API_PORT', '8082'))
    api_runner, api_site = await api_server.start(host='0.0.0.0', port=api_port)
    logging.info(f"Servidor API HTTP iniciado na porta {api_port}")
    
    logging.info("Sistema pronto para processar chamadas de visitantes e moradores")

    # Iniciar todos os serviços simultaneamente
    async with server_visitante, server_morador:
        await asyncio.gather(
            server_visitante.serve_forever(),
            server_morador.serve_forever()
        )

if __name__ == "__main__":
    asyncio.run(main())


setup_system.py:
#!/usr/bin/env python
# setup_system.py

import asyncio
import logging
import sys
import os
from dotenv import load_dotenv
from speech_service import pre_sintetizar_frases_comuns
from extensions.extension_manager import ExtensionManager
from extensions.server_manager import ServerManager
from extensions.db_connector import DBConnector
from extensions.config_persistence import ConfigPersistence
from extensions.api_server import APIServer
from audiosocket_handler import set_extension_manager
from session_manager import SessionManager

# Configurar logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('logs/audiosocket.log'),
        logging.StreamHandler(sys.stdout)
    ]
)

logger = logging.getLogger(__name__)

# Carregar variáveis de ambiente
load_dotenv()

# Inicializar diretório de logs se não existir
if not os.path.exists('logs'):
    os.makedirs('logs', exist_ok=True)

async def setup_extensions():
    """
    Configura todo o sistema de ramais dinâmicos sem iniciar os servidores.
    Retorna os objetos necessários para inicialização em outro módulo.
    """
    # Inicializar todos os componentes
    db_connector = DBConnector()
    config_persistence = ConfigPersistence()
    server_manager = ServerManager()
    extension_manager = ExtensionManager()
    
    # Teste de conexão com banco de dados
    if db_connector.connect():
        logger.info("Conexão com banco de dados estabelecida")
    else:
        logger.warning("Falha ao conectar com banco de dados, usando configurações locais")
    
    # Inicializar extension_manager
    await extension_manager.initialize(api_port=int(os.getenv('API_PORT', '8082')))
    
    # Configurar o extension_manager no audiosocket_handler global
    set_extension_manager(extension_manager)
    
    # Criar uma nova instância de SessionManager que use o extension_manager
    session_manager = SessionManager(extension_manager)
    
    # Configurar audiosocket_handler para usar o novo session_manager
    import audiosocket_handler
    audiosocket_handler.session_manager = session_manager
    
    # Pré-sintetizar frases comuns para cache
    logger.info("Pré-sintetizando frases comuns...")
    pre_sintetizar_frases_comuns()
    
    return {
        'extension_manager': extension_manager,
        'session_manager': session_manager,
        'db_connector': db_connector,
        'server_manager': server_manager,
        'config_persistence': config_persistence
    }

if __name__ == "__main__":
    try:
        print("Este script não deve ser executado diretamente.")
        print("Ele é importado pelo main_dynamic.py para configurar o sistema.")
        print("Execute 'python main_dynamic.py' para iniciar o sistema.")
    except Exception as e:
        logger.critical(f"Erro fatal: {e}", exc_info=True)
        sys.exit(1)

speech_service.py:
import asyncio
import os
import hashlib
from io import BytesIO

from audio_utils import converter_bytes_para_wav, converter_wav_para_slin
import azure.cognitiveservices.speech as speechsdk
from pydub import AudioSegment

# Diretório de cache para síntese de voz
CACHE_DIR = 'audio/cache'
os.makedirs(CACHE_DIR, exist_ok=True)

async def transcrever_audio_async(dados_audio_slin, call_id=None):
    """
    Versão assíncrona da transcrição de áudio que aceita parâmetro de call_id
    para recursos de monitoramento e gerenciamento.
    """
    try:
        # Antes de transcrever, verificar disponibilidade no ResourceManager
        if 'resource_manager' in globals() and call_id:
            from extensions.resource_manager import resource_manager
            # Adquirir semáforo para limitar número de transcrições simultâneas
            await resource_manager.acquire_transcription_lock(call_id)
            
        # Usar executor para não bloquear a thread principal
        loop = asyncio.get_event_loop()
        start_time = asyncio.get_event_loop().time()
        result = await loop.run_in_executor(None, transcrever_audio, dados_audio_slin)
        
        # Registrar métricas se temos gerenciamento de recursos
        if 'resource_manager' in globals() and call_id:
            duration_ms = (asyncio.get_event_loop().time() - start_time) * 1000
            resource_manager.record_transcription(call_id, duration_ms)
            
        return result
    finally:
        # Sempre liberar o lock quando terminar
        if 'resource_manager' in globals() and call_id:
            resource_manager.release_transcription_lock(call_id)

async def sintetizar_fala_async(texto, call_id=None):
    """
    Versão assíncrona da síntese de fala que aceita parâmetro de call_id
    para recursos de monitoramento e gerenciamento.
    """
    # Verificar cache antes de sintetizar
    hash_texto = hashlib.md5(texto.encode('utf-8')).hexdigest()
    cache_path = os.path.join(CACHE_DIR, f"{hash_texto}.slin")
    
    # Se já existe no cache, retornar o arquivo de áudio imediatamente
    if os.path.exists(cache_path):
        with open(cache_path, 'rb') as f:
            return f.read()
    
    try:
        # Antes de sintetizar, verificar disponibilidade no ResourceManager
        if 'resource_manager' in globals() and call_id:
            from extensions.resource_manager import resource_manager
            # Adquirir semáforo para limitar número de sínteses simultâneas
            await resource_manager.acquire_synthesis_lock(call_id)
            
        # Se não está no cache, sintetizar e salvar
        start_time = asyncio.get_event_loop().time()
        loop = asyncio.get_event_loop()
        audio_data = await loop.run_in_executor(None, sintetizar_fala, texto)
        
        # Registrar métricas se temos gerenciamento de recursos
        if 'resource_manager' in globals() and call_id:
            duration_ms = (asyncio.get_event_loop().time() - start_time) * 1000
            resource_manager.record_synthesis(call_id, duration_ms)
        
        # Salvar no cache para uso futuro (apenas se a síntese foi bem-sucedida)
        if audio_data:
            with open(cache_path, 'wb') as f:
                f.write(audio_data)
        
        return audio_data
    finally:
        # Sempre liberar o lock quando terminar
        if 'resource_manager' in globals() and call_id:
            resource_manager.release_synthesis_lock(call_id)

def transcrever_audio(dados_audio_slin):
    """
    Transcrever áudio usando Azure Speech com abordagem simplificada e robusta.
    Comportamento alinhado com o método VAD para garantir consistência na aplicação.
    """
    # Log detalhado para diagnóstico
    audio_size = len(dados_audio_slin)
    frames_estimate = audio_size // 320  # Estimativa de frames (20ms cada)
    duracao_estimada = frames_estimate * 0.02  # Duração em segundos
    
    print(f"[TRANSCRIÇÃO] Iniciando transcrição de {audio_size} bytes de áudio SLIN (~{frames_estimate} frames, ~{duracao_estimada:.2f}s)")
    
    # Verificações de segurança para garantir que temos dados válidos
    if not dados_audio_slin:
        print("[TRANSCRIÇÃO] Dados de áudio vazios")
        return None
    
    if audio_size < 320:  # Menos de um frame de áudio
        print(f"[TRANSCRIÇÃO] Áudio muito pequeno para transcrição: {audio_size} bytes")
        return None
    
    # Verificação para áudio muito curto - provável ruído
    if audio_size < 4800:  # Menos de 300ms de áudio (~15 frames)
        print(f"[TRANSCRIÇÃO] Áudio muito curto detectado ({audio_size} bytes, ~{duracao_estimada:.2f}s) - considerando ruído ou resposta curta")
        return "ok"
    
    # Verificar energia do áudio para descartar ruído
    try:
        import struct
        # Converter bytes para valores PCM 16-bit
        samples = struct.unpack('<' + 'h' * (len(dados_audio_slin) // 2), dados_audio_slin)
        # Calcular energia média
        energy = sum(sample ** 2 for sample in samples) / len(samples)
        ENERGY_THRESHOLD = 600  # Threshold ajustável para considerar áudio válido
        
        if energy < ENERGY_THRESHOLD:
            print(f"[TRANSCRIÇÃO] Áudio com energia muito baixa ({energy:.2f} < {ENERGY_THRESHOLD}) - considerando ruído")
            # Para áudios com pouca energia, tratamos como confirmação
            return "ok"
        else:
            print(f"[TRANSCRIÇÃO] Energia do áudio: {energy:.2f} (acima do threshold {ENERGY_THRESHOLD})")
    except Exception as e:
        print(f"[TRANSCRIÇÃO] Erro ao analisar energia do áudio: {e}")
        # Em caso de erro, continuamos com a transcrição normal
        
    # Verificação para áudio curto - possível "sim"
    is_short_audio = audio_size < 8000  # ~0.5 segundo de áudio
    if is_short_audio:
        print(f"[TRANSCRIÇÃO] Áudio curto detectado ({audio_size} bytes, ~{duracao_estimada:.2f}s) - possível 'sim'")
    
    # Método direto mais simples, usando exatamente a mesma abordagem do VAD
    try:
        print(f"[TRANSCRIÇÃO] Usando método de transcrição direto (estilo VAD) - {audio_size} bytes")
        
        # 1. Converter dados PCM para WAV
        print("[TRANSCRIÇÃO] Convertendo SLIN para WAV")
        audio_wav = converter_bytes_para_wav(dados_audio_slin, 8000)
        if not audio_wav:
            print("[TRANSCRIÇÃO] Falha na conversão para WAV - formato de áudio não suportado")
            # Para áudio curto, tentamos retornar "sim" mesmo com falha de conversão
            if is_short_audio:
                print(f"[TRANSCRIÇÃO] Áudio curto com falha de conversão - retornando 'sim' como fallback")
                return "sim"
            return None
            
        # 2. Converter para WAV de 16k (requisito da Azure)
        print("[TRANSCRIÇÃO] Convertendo para WAV 16kHz")
        audio_segment = AudioSegment.from_file(BytesIO(audio_wav), format="wav")
        audio_segment = audio_segment.set_frame_rate(16000)
        
        # Aplicar normalização para melhorar reconhecimento
        print("[TRANSCRIÇÃO] Normalizando áudio")
        audio_segment = audio_segment.normalize()
        
        # 3. Exportar para WAV
        buffer = BytesIO()
        audio_segment.export(buffer, format='wav')
        audio_wav_16k = buffer.getvalue()
        
        if not audio_wav_16k:
            print("[TRANSCRIÇÃO] Falha ao exportar para WAV 16kHz")
            if is_short_audio:
                print(f"[TRANSCRIÇÃO] Áudio curto com falha de exportação - retornando 'sim' como fallback")
                return "sim"
            return None
            
        print(f"[TRANSCRIÇÃO] Áudio WAV 16kHz gerado com sucesso: {len(audio_wav_16k)} bytes")
        
        # 4. Configurações otimizadas do Azure Speech
        print("[TRANSCRIÇÃO] Configurando Azure Speech SDK")
        speech_config = speechsdk.SpeechConfig(subscription=os.getenv('AZURE_SPEECH_KEY'), region=os.getenv('AZURE_SPEECH_REGION'))
        speech_config.speech_recognition_language = 'pt-BR'
        
        # Melhorias para reconhecimento mais preciso
        speech_config.set_property(speechsdk.PropertyId.SpeechServiceResponse_PostProcessingOption, "TrueText")
        speech_config.enable_dictation()  # Melhor para frases curtas
        
        # 5. Configurar streaming de áudio 
        print("[TRANSCRIÇÃO] Configurando stream de áudio")
        audio_stream = speechsdk.audio.PushAudioInputStream()
        audio_stream.write(audio_wav_16k)
        audio_stream.close()
        audio_config = speechsdk.audio.AudioConfig(stream=audio_stream)
        
        # 6. Criar reconhecedor e executar reconhecimento
        recognizer = speechsdk.SpeechRecognizer(speech_config=speech_config, audio_config=audio_config)
        print("[TRANSCRIÇÃO] Executando recognize_once()...")
        result = recognizer.recognize_once()
        
        # 7. Processar resultado
        if result.reason == speechsdk.ResultReason.RecognizedSpeech:
            # Verificar se o texto está vazio - pode acontecer quando Azure reconhece com sucesso mas sem conteúdo
            if result.text and result.text.strip():
                # Texto não vazio - retornar normalmente
                print(f"[TRANSCRIÇÃO] Bem-sucedida: '{result.text}'")
                return result.text
            else:
                # Texto vazio - tratar de acordo com o tamanho do áudio
                print("[TRANSCRIÇÃO] Texto reconhecido está vazio")
                
                # Para áudios curtos, tratamos como resposta curta "sim"
                if is_short_audio:
                    print(f"[TRANSCRIÇÃO] Texto vazio em áudio curto ({audio_size} bytes) - considerando 'sim'")
                    return "sim"
                else:
                    # Para áudios mais longos, consideramos como um "ok"
                    print(f"[TRANSCRIÇÃO] Texto vazio em áudio normal ({audio_size} bytes) - considerando 'ok'")
                    return "ok"
        elif result.reason == speechsdk.ResultReason.NoMatch:
            no_match_info = "Sem detalhes disponíveis"
            try:
                no_match_info = result.no_match_details.reason
            except:
                pass
            print(f"[TRANSCRIÇÃO] Nenhuma fala reconhecida (NoMatch): {no_match_info}")
            
            # Para áudios curtos, consideramos "sim"
            if is_short_audio:
                print(f"[TRANSCRIÇÃO] Áudio curto não reconhecido ({audio_size} bytes) - considerando 'sim'")
                return "sim"
            return None
        else:
            print(f"[TRANSCRIÇÃO] Falha na transcrição. Reason: {result.reason}")
            # Para áudios curtos, ainda consideramos "sim" mesmo em caso de falha
            if is_short_audio:
                print(f"[TRANSCRIÇÃO] Áudio curto com falha de reconhecimento - retornando 'sim' como fallback")
                return "sim"
            return None
            
    except Exception as e:
        print(f"[TRANSCRIÇÃO] Erro durante a transcrição: {e}")
        import traceback
        print(traceback.format_exc())
        
        # Para áudios curtos, consideramos "sim" mesmo em caso de exceção
        if is_short_audio:
            print(f"[TRANSCRIÇÃO] Áudio curto com exceção - retornando 'sim' como fallback de emergência")
            return "sim"
        return None

def sintetizar_fala(texto):
    speech_config = speechsdk.SpeechConfig(subscription=os.getenv('AZURE_SPEECH_KEY'), region=os.getenv('AZURE_SPEECH_REGION'))
    speech_config.speech_synthesis_language = 'pt-BR'
    synthesizer = speechsdk.SpeechSynthesizer(speech_config=speech_config, audio_config=None)
    result = synthesizer.speak_text_async(texto).get()
    if result.reason == speechsdk.ResultReason.SynthesizingAudioCompleted:
        return converter_wav_para_slin(result.audio_data, 8000)
    return None

# Pré-carregar frases comuns
def pre_sintetizar_frases_comuns():
    """Pré-sintetiza frases comuns para o cache."""
    frases_comuns = [
        "Olá, seja bem-vindo! Em que posso ajudar?",
        "Por favor, me informe o seu nome",
        "Por favor, me informe para qual apartamento e o nome do morador",
        "Obrigado, aguarde um instante",
        "Ok, vamos entrar em contato com o morador. Aguarde, por favor.",
        "Desculpe, não consegui entender. Pode repetir por favor?",
        "Olá, morador! Você está em ligação com a portaria inteligente."
    ]
    
    for frase in frases_comuns:
        hash_texto = hashlib.md5(frase.encode('utf-8')).hexdigest()
        cache_path = os.path.join(CACHE_DIR, f"{hash_texto}.slin")
        
        # Só sintetiza se não existir no cache
        if not os.path.exists(cache_path):
            audio_data = sintetizar_fala(frase)
            if audio_data:
                with open(cache_path, 'wb') as f:
                    f.write(audio_data)
                print(f"Sintetizado e cacheado: '{frase}'")

# Pré-sintetizar frases na inicialização (descomente para habilitar)
# pre_sintetizar_frases_comuns()

tools/play_slin.py:
#!/usr/bin/env python3
"""
Ferramenta para reproduzir arquivos .slin salvos para debug.
Converte de SLIN (PCM 16-bit 8kHz mono) para WAV para verificação.

Uso:
    python tools/play_slin.py audio/debug/arquivo.slin

Ou converte todos os arquivos de um diretório:
    python tools/play_slin.py audio/debug/
"""

import os
import sys
import wave
import struct
import subprocess
from pathlib import Path

def convert_slin_to_wav(slin_file, wav_file=None):
    """Converte um arquivo SLIN para WAV para poder ser reproduzido."""
    if wav_file is None:
        wav_file = os.path.splitext(slin_file)[0] + '.wav'
    
    with open(slin_file, 'rb') as f:
        slin_data = f.read()
    
    # Criar arquivo WAV
    with wave.open(wav_file, 'wb') as wav:
        wav.setnchannels(1)  # Mono
        wav.setsampwidth(2)  # 16-bit
        wav.setframerate(8000)  # 8kHz
        wav.writeframes(slin_data)
    
    print(f"Convertido: {slin_file} -> {wav_file}")
    return wav_file

def play_wav(wav_file):
    """Tenta reproduzir o arquivo WAV usando o player disponível no sistema."""
    try:
        if sys.platform == 'darwin':  # macOS
            subprocess.run(['afplay', wav_file], check=True)
        elif sys.platform == 'linux':
            subprocess.run(['aplay', wav_file], check=True)
        elif sys.platform == 'win32':  # Windows
            subprocess.run(['start', wav_file], check=True, shell=True)
        else:
            print(f"Não foi possível reproduzir automaticamente. Abra o arquivo: {wav_file}")
    except Exception as e:
        print(f"Erro ao reproduzir: {e}")
        print(f"Você pode abrir o arquivo manualmente: {wav_file}")

def analyze_slin(slin_file):
    """Analisa um arquivo SLIN e mostra informações sobre ele."""
    with open(slin_file, 'rb') as f:
        slin_data = f.read()
    
    num_samples = len(slin_data) // 2  # 2 bytes por amostra (16-bit)
    duration_ms = (num_samples / 8000) * 1000  # 8000 Hz
    
    # Calcular valor RMS do áudio (indica volume/energia)
    if num_samples > 0:
        samples = struct.unpack(f"<{num_samples}h", slin_data)
        rms = (sum(s*s for s in samples) / num_samples) ** 0.5
    else:
        rms = 0
    
    # Verificar se o áudio contém silêncio ou ruído
    silent_threshold = 100  # Valor arbitrário para considerar silêncio
    if rms < silent_threshold:
        status = "SILÊNCIO"
    else:
        status = "CONTÉM ÁUDIO"
    
    print(f"Análise: {slin_file}")
    print(f"  - Tamanho: {len(slin_data):,} bytes")
    print(f"  - Duração: {duration_ms:.1f}ms (~{duration_ms/1000:.1f}s)")
    print(f"  - Amostras: {num_samples:,}")
    print(f"  - RMS: {rms:.1f} ({status})")
    
    return {
        'file': slin_file,
        'size_bytes': len(slin_data),
        'duration_ms': duration_ms,
        'samples': num_samples,
        'rms': rms,
        'status': status
    }

def process_file(slin_file):
    """Processa um único arquivo SLIN."""
    if not os.path.exists(slin_file):
        print(f"Arquivo não encontrado: {slin_file}")
        return
    
    analysis = analyze_slin(slin_file)
    
    # Converter para WAV se contém áudio
    if analysis['status'] == "CONTÉM ÁUDIO":
        wav_file = convert_slin_to_wav(slin_file)
        
        # Perguntar se quer reproduzir
        choice = input("Reproduzir o áudio? (s/n): ").lower()
        if choice == 's':
            play_wav(wav_file)
    else:
        print("Arquivo contém apenas silêncio, não vale a pena reproduzir.")

def process_directory(directory):
    """Processa todos os arquivos SLIN em um diretório."""
    directory = Path(directory)
    slin_files = list(directory.glob('*.slin'))
    
    if not slin_files:
        print(f"Nenhum arquivo .slin encontrado em: {directory}")
        return
    
    print(f"Encontrados {len(slin_files)} arquivos .slin em: {directory}\n")
    
    # Analisar todos os arquivos
    analyses = []
    for file in slin_files:
        analyses.append(analyze_slin(str(file)))
        print("")  # Linha em branco para separar
    
    # Mostrar informações ordenadas por tamanho
    print("\nArquivos ordenados por tamanho:")
    for i, analysis in enumerate(sorted(analyses, key=lambda x: x['size_bytes'], reverse=True), 1):
        print(f"{i}. {os.path.basename(analysis['file'])} - {analysis['duration_ms']:.1f}ms - {analysis['status']}")
    
    # Perguntar qual arquivo converter
    choice = input("\nDigite o número do arquivo para converter e reproduzir (0 para sair): ")
    try:
        choice = int(choice)
        if choice > 0 and choice <= len(analyses):
            selected = sorted(analyses, key=lambda x: x['size_bytes'], reverse=True)[choice-1]
            wav_file = convert_slin_to_wav(selected['file'])
            play_wav(wav_file)
    except ValueError:
        print("Escolha inválida.")

def main():
    if len(sys.argv) < 2:
        print(f"Uso: {sys.argv[0]} <arquivo.slin ou diretório>")
        return
    
    path = sys.argv[1]
    
    if os.path.isfile(path) and path.endswith('.slin'):
        process_file(path)
    elif os.path.isdir(path):
        process_directory(path)
    else:
        print(f"Caminho inválido ou não é um arquivo .slin: {path}")

if __name__ == "__main__":
    main()

utils/log_analyzer.py:
#!/usr/bin/env python3
"""
Analisador de logs das chamadas.
Este script processa os arquivos de log das chamadas para gerar relatórios
de desempenho e identificar possíveis gargalos.
"""

import json
import os
import sys
import glob
import re
from datetime import datetime
from typing import Dict, List, Tuple, Any
import statistics
import argparse

def parse_log_line(line: str) -> Dict[str, Any]:
    """
    Parse uma linha de log no formato:
    2023-04-23 15:30:45.123 | INFO | EVENT_TYPE | {"key": "value", ...}
    
    Retorna um dicionário com timestamp, level, event_type e data.
    """
    pattern = r'(\d{4}-\d{2}-\d{2} \d{2}:\d{2}:\d{2}\.\d{3}) \| (\w+) \| (\w+) \| (.+)'
    match = re.match(pattern, line)
    
    if not match:
        return None
    
    timestamp_str, level, event_type, data_str = match.groups()
    timestamp = datetime.strptime(timestamp_str, '%Y-%m-%d %H:%M:%S.%f')
    
    try:
        data = json.loads(data_str)
    except json.JSONDecodeError:
        data = {"raw_message": data_str}
    
    return {
        "timestamp": timestamp,
        "level": level,
        "event_type": event_type,
        "data": data
    }

def load_log_file(filepath: str) -> List[Dict[str, Any]]:
    """
    Carrega um arquivo de log e retorna uma lista de eventos parseados.
    """
    events = []
    with open(filepath, 'r', encoding='utf-8') as f:
        for line in f:
            parsed = parse_log_line(line.strip())
            if parsed:
                events.append(parsed)
    
    return events

def calculate_statistics(values: List[float]) -> Dict[str, float]:
    """
    Calcula estatísticas básicas para uma lista de valores.
    """
    if not values:
        return {
            "min": 0,
            "max": 0,
            "avg": 0,
            "median": 0,
            "p90": 0,
            "p95": 0,
            "p99": 0
        }
    
    values = sorted(values)
    n = len(values)
    
    return {
        "min": min(values),
        "max": max(values),
        "avg": sum(values) / n,
        "median": values[n // 2] if n % 2 else (values[n // 2 - 1] + values[n // 2]) / 2,
        "p90": values[int(n * 0.9)],
        "p95": values[int(n * 0.95)],
        "p99": values[int(n * 0.99)] if n >= 100 else values[-1]
    }

def analyze_transcription_times(events: List[Dict[str, Any]]) -> Dict[str, Any]:
    """
    Analisa os tempos de transcrição de áudio.
    """
    times = []
    visitor_times = []
    resident_times = []
    
    for i, event in enumerate(events):
        if event["event_type"] == "TRANSCRIPTION_COMPLETE":
            duration = event["data"].get("duration_ms", 0)
            times.append(duration)
            
            if event["data"].get("source") == "visitor":
                visitor_times.append(duration)
            else:
                resident_times.append(duration)
    
    return {
        "all": calculate_statistics(times),
        "visitor": calculate_statistics(visitor_times),
        "resident": calculate_statistics(resident_times)
    }

def analyze_synthesis_times(events: List[Dict[str, Any]]) -> Dict[str, Any]:
    """
    Analisa os tempos de síntese de áudio.
    """
    times = []
    visitor_times = []
    resident_times = []
    
    for i, event in enumerate(events):
        if event["event_type"] == "SYNTHESIS_COMPLETE":
            duration = event["data"].get("duration_ms", 0)
            times.append(duration)
            
            if event["data"].get("target") == "visitor":
                visitor_times.append(duration)
            else:
                resident_times.append(duration)
    
    return {
        "all": calculate_statistics(times),
        "visitor": calculate_statistics(visitor_times),
        "resident": calculate_statistics(resident_times)
    }

def analyze_ai_processing_times(events: List[Dict[str, Any]]) -> Dict[str, Any]:
    """
    Analisa os tempos de processamento da IA.
    """
    total_times = []
    intent_extraction_times = {
        "intent_type": [],
        "interlocutor_name": [],
        "apartment_and_resident": []
    }
    fuzzy_validation_times = []
    
    for i, event in enumerate(events):
        if event["event_type"] == "AI_PROCESSING_COMPLETE":
            duration = event["data"].get("duration_ms", 0)
            total_times.append(duration)
        
        elif event["event_type"] == "INTENT_EXTRACTION_COMPLETE":
            duration = event["data"].get("duration_ms", 0)
            stage = event["data"].get("stage")
            if stage in intent_extraction_times:
                intent_extraction_times[stage].append(duration)
        
        elif event["event_type"] == "FUZZY_VALIDATION_COMPLETE":
            duration = event["data"].get("duration_ms", 0)
            fuzzy_validation_times.append(duration)
    
    return {
        "total": calculate_statistics(total_times),
        "intent_extraction": {
            "intent_type": calculate_statistics(intent_extraction_times["intent_type"]),
            "interlocutor_name": calculate_statistics(intent_extraction_times["interlocutor_name"]),
            "apartment_and_resident": calculate_statistics(intent_extraction_times["apartment_and_resident"])
        },
        "fuzzy_validation": calculate_statistics(fuzzy_validation_times)
    }

def analyze_vad_performance(events: List[Dict[str, Any]]) -> Dict[str, Any]:
    """
    Analisa o desempenho da detecção de voz (VAD).
    """
    speech_durations = []
    silence_durations = []
    
    for i, event in enumerate(events):
        if event["event_type"] == "SPEECH_ENDED":
            duration = event["data"].get("duration_ms", 0)
            speech_durations.append(duration)
        
        elif event["event_type"] == "SILENCE_DETECTED":
            duration = event["data"].get("duration_ms", 0)
            silence_durations.append(duration)
    
    return {
        "speech_durations": calculate_statistics(speech_durations),
        "silence_durations": calculate_statistics(silence_durations)
    }

def analyze_call_durations(events: List[Dict[str, Any]]) -> Dict[str, float]:
    """
    Analisa a duração total das chamadas.
    """
    call_start = None
    call_end = None
    
    for event in events:
        if event["event_type"] == "CALL_STARTED":
            call_start = event["timestamp"]
        elif event["event_type"] == "CALL_ENDED":
            call_end = event["timestamp"]
    
    if call_start and call_end:
        return (call_end - call_start).total_seconds() * 1000
    
    return 0

def analyze_errors(events: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
    """
    Analisa os erros registrados durante a chamada.
    """
    errors = []
    
    for event in events:
        if event["event_type"] == "ERROR":
            errors.append({
                "timestamp": event["timestamp"],
                "error_type": event["data"].get("error_type", "unknown"),
                "message": event["data"].get("message", ""),
                "details": event["data"].get("details", {})
            })
    
    return errors

def analyze_log_file(filepath: str) -> Dict[str, Any]:
    """
    Analisa um arquivo de log de chamada e gera um relatório.
    """
    call_id = os.path.basename(filepath).replace('.log', '')
    events = load_log_file(filepath)
    
    if not events:
        return {
            "call_id": call_id,
            "error": "Arquivo de log vazio ou formato inválido"
        }
    
    transcription_stats = analyze_transcription_times(events)
    synthesis_stats = analyze_synthesis_times(events)
    ai_processing_stats = analyze_ai_processing_times(events)
    vad_stats = analyze_vad_performance(events)
    call_duration = analyze_call_durations(events)
    errors = analyze_errors(events)
    
    return {
        "call_id": call_id,
        "call_duration_ms": call_duration,
        "transcription_stats": transcription_stats,
        "synthesis_stats": synthesis_stats,
        "ai_processing_stats": ai_processing_stats,
        "vad_stats": vad_stats,
        "errors": errors,
        "event_count": len(events)
    }

def print_stats(title: str, stats: Dict[str, float], indent=0):
    """
    Imprime estatísticas formatadas.
    """
    indentation = " " * indent
    print(f"{indentation}{title}:")
    for key, value in stats.items():
        print(f"{indentation}  {key}: {value:.2f}ms")

def print_report(report: Dict[str, Any]):
    """
    Imprime um relatório formatado.
    """
    print(f"\n==== RELATÓRIO DE CHAMADA: {report['call_id']} ====")
    print(f"Duração total: {report['call_duration_ms']:.2f}ms ({report['call_duration_ms']/1000:.2f}s)")
    print(f"Total de eventos: {report['event_count']}")
    
    print("\n--- TEMPOS DE TRANSCRIÇÃO ---")
    print_stats("Todos", report['transcription_stats']['all'], 2)
    print_stats("Visitante", report['transcription_stats']['visitor'], 2)
    print_stats("Morador", report['transcription_stats']['resident'], 2)
    
    print("\n--- TEMPOS DE SÍNTESE ---")
    print_stats("Todos", report['synthesis_stats']['all'], 2)
    print_stats("Visitante", report['synthesis_stats']['visitor'], 2)
    print_stats("Morador", report['synthesis_stats']['resident'], 2)
    
    print("\n--- TEMPOS DE PROCESSAMENTO IA ---")
    print_stats("Total", report['ai_processing_stats']['total'], 2)
    
    print("\n  Extração de intenção:")
    print_stats("Tipo de intenção", report['ai_processing_stats']['intent_extraction']['intent_type'], 4)
    print_stats("Nome do interlocutor", report['ai_processing_stats']['intent_extraction']['interlocutor_name'], 4)
    print_stats("Apartamento e morador", report['ai_processing_stats']['intent_extraction']['apartment_and_resident'], 4)
    
    print_stats("Validação fuzzy", report['ai_processing_stats']['fuzzy_validation'], 2)
    
    print("\n--- DETECÇÃO DE VOZ ---")
    print_stats("Duração da fala", report['vad_stats']['speech_durations'], 2)
    print_stats("Duração do silêncio", report['vad_stats']['silence_durations'], 2)
    
    if report['errors']:
        print("\n--- ERROS DETECTADOS ---")
        for i, error in enumerate(report['errors']):
            print(f"  {i+1}. {error['error_type']}: {error['message']}")
            if error['details']:
                for k, v in error['details'].items():
                    print(f"     {k}: {v}")

def main():
    parser = argparse.ArgumentParser(description='Analisador de logs de chamadas')
    parser.add_argument('--call_id', help='ID específico da chamada para analisar')
    parser.add_argument('--all', action='store_true', help='Analisar todos os logs')
    parser.add_argument('--summary', action='store_true', help='Mostrar apenas resumo agregado')
    parser.add_argument('--output', help='Arquivo para salvar o relatório em JSON')
    args = parser.parse_args()
    
    logs_dir = os.path.join(os.path.dirname(os.path.dirname(os.path.abspath(__file__))), 'logs')
    
    if args.call_id:
        log_file = os.path.join(logs_dir, f"{args.call_id}.log")
        if not os.path.exists(log_file):
            print(f"Arquivo de log não encontrado: {log_file}")
            return
        
        report = analyze_log_file(log_file)
        print_report(report)
        
        if args.output:
            with open(args.output, 'w', encoding='utf-8') as f:
                json.dump(report, f, indent=2, default=str)
    
    elif args.all or args.summary:
        log_files = glob.glob(os.path.join(logs_dir, "*.log"))
        
        if not log_files:
            print("Nenhum arquivo de log encontrado.")
            return
        
        all_reports = []
        for log_file in log_files:
            report = analyze_log_file(log_file)
            all_reports.append(report)
            
            if args.all and not args.summary:
                print_report(report)
        
        if args.summary:
            # Agregar estatísticas de todos os relatórios
            transcription_times = []
            synthesis_times = []
            ai_processing_times = []
            call_durations = []
            error_count = 0
            
            for report in all_reports:
                # Coletar tempos de transcrição
                for time_list in [report['transcription_stats']['visitor'], report['transcription_stats']['resident']]:
                    for key in ['avg', 'max']:
                        if time_list.get(key):
                            transcription_times.append(time_list[key])
                
                # Coletar tempos de síntese
                for time_list in [report['synthesis_stats']['visitor'], report['synthesis_stats']['resident']]:
                    for key in ['avg', 'max']:
                        if time_list.get(key):
                            synthesis_times.append(time_list[key])
                
                # Coletar tempos de processamento de IA
                if report['ai_processing_stats']['total'].get('avg'):
                    ai_processing_times.append(report['ai_processing_stats']['total']['avg'])
                
                # Coletar duração da chamada
                if report['call_duration_ms']:
                    call_durations.append(report['call_duration_ms'])
                
                # Contar erros
                error_count += len(report['errors'])
            
            print("\n==== RESUMO AGREGADO DE TODAS AS CHAMADAS ====")
            print(f"Total de chamadas analisadas: {len(all_reports)}")
            print(f"Total de erros encontrados: {error_count}")
            
            print("\n--- MÉDIAS GERAIS ---")
            print(f"Duração média das chamadas: {statistics.mean(call_durations)/1000:.2f}s")
            print(f"Tempo médio de transcrição: {statistics.mean(transcription_times):.2f}ms")
            print(f"Tempo médio de síntese: {statistics.mean(synthesis_times):.2f}ms")
            print(f"Tempo médio de processamento de IA: {statistics.mean(ai_processing_times):.2f}ms")
            
            print("\n--- MÁXIMOS GERAIS ---")
            print(f"Duração máxima de chamada: {max(call_durations)/1000:.2f}s")
            print(f"Tempo máximo de transcrição: {max(transcription_times):.2f}ms")
            print(f"Tempo máximo de síntese: {max(synthesis_times):.2f}ms")
            print(f"Tempo máximo de processamento de IA: {max(ai_processing_times):.2f}ms")
        
        if args.output:
            with open(args.output, 'w', encoding='utf-8') as f:
                json.dump(all_reports, f, indent=2, default=str)
    
    else:
        parser.print_help()

if __name__ == "__main__":
    main()

utils/call_logger.py:
import logging
import os
import time
from datetime import datetime
from typing import Dict, Optional, Any, Union
import json

class CallLogger:
    """
    Logger especializado para registrar detalhes de uma chamada específica.
    Cria um arquivo de log único para cada UUID de chamada com timestamps precisos
    para cada etapa do processo.
    """
    
    def __init__(self, call_id: str):
        self.call_id = call_id
        self.start_time = time.time()
        self.log_file = os.path.join('logs', f"{call_id}.log")
        
        # Configurar logger específico para esta chamada
        self.logger = logging.getLogger(f"call.{call_id}")
        
        # Remove handlers existentes para evitar duplicação se o logger já existir
        if self.logger.handlers:
            for handler in self.logger.handlers:
                self.logger.removeHandler(handler)
        
        # Definir nível de logging
        self.logger.setLevel(logging.DEBUG)
        
        # Criar file handler
        os.makedirs(os.path.dirname(self.log_file), exist_ok=True)
        file_handler = logging.FileHandler(self.log_file)
        
        # Definir formato
        formatter = logging.Formatter(
            '%(asctime)s.%(msecs)03d | %(levelname)s | %(message)s',
            datefmt='%Y-%m-%d %H:%M:%S'
        )
        file_handler.setFormatter(formatter)
        
        # Adicionar handler ao logger
        self.logger.addHandler(file_handler)
        
        # Registrar início da chamada
        self.log_event("CALL_STARTED", {
            "timestamp": datetime.now().isoformat()
        })
    
    def log_event(self, event_type: str, data: Dict[str, Any]) -> None:
        """
        Registra um evento com seu timestamp e dados adicionais.
        
        Args:
            event_type: Tipo do evento (ex: SPEECH_DETECTED, TRANSCRIPTION_COMPLETE)
            data: Dicionário com informações adicionais do evento
        """
        # Adicionar timestamp se não fornecido
        if "timestamp" not in data:
            data["timestamp"] = datetime.now().isoformat()
        
        # Adicionar tempo decorrido desde o início da chamada
        elapsed = time.time() - self.start_time
        data["elapsed_seconds"] = round(elapsed, 3)
        
        # Formatar mensagem para o log
        message = f"{event_type} | {json.dumps(data)}"
        self.logger.info(message)
    
    def log_speech_detected(self, is_visitor: bool = True) -> None:
        """Registra quando voz é detectada."""
        self.log_event("SPEECH_DETECTED", {
            "source": "visitor" if is_visitor else "resident"
        })
    
    def log_speech_ended(self, duration_ms: float, is_visitor: bool = True) -> None:
        """Registra quando a fala termina."""
        self.log_event("SPEECH_ENDED", {
            "source": "visitor" if is_visitor else "resident",
            "duration_ms": duration_ms
        })
    
    def log_transcription_start(self, audio_size: int, is_visitor: bool = True) -> None:
        """Registra início da transcrição."""
        self.log_event("TRANSCRIPTION_START", {
            "source": "visitor" if is_visitor else "resident",
            "audio_size_bytes": audio_size
        })
    
    def log_transcription_complete(self, text: str, duration_ms: float, is_visitor: bool = True) -> None:
        """Registra conclusão da transcrição."""
        self.log_event("TRANSCRIPTION_COMPLETE", {
            "source": "visitor" if is_visitor else "resident",
            "text": text,
            "duration_ms": duration_ms
        })
    
    def log_ai_processing_start(self, text: str) -> None:
        """Registra início do processamento pela IA."""
        self.log_event("AI_PROCESSING_START", {
            "input_text": text
        })
    
    def log_ai_processing_complete(self, response: Dict[str, Any], duration_ms: float) -> None:
        """Registra conclusão do processamento pela IA."""
        self.log_event("AI_PROCESSING_COMPLETE", {
            "response": response,
            "duration_ms": duration_ms
        })
    
    def log_synthesis_start(self, text: str, is_visitor: bool = True) -> None:
        """Registra início da síntese de voz."""
        self.log_event("SYNTHESIS_START", {
            "target": "visitor" if is_visitor else "resident",
            "text": text
        })
    
    def log_synthesis_complete(self, audio_size: int, duration_ms: float, is_visitor: bool = True) -> None:
        """Registra conclusão da síntese de voz."""
        self.log_event("SYNTHESIS_COMPLETE", {
            "target": "visitor" if is_visitor else "resident",
            "audio_size_bytes": audio_size,
            "duration_ms": duration_ms
        })
    
    def log_state_change(self, old_state: str, new_state: str) -> None:
        """Registra mudança de estado no fluxo de conversa."""
        self.log_event("STATE_CHANGE", {
            "from": old_state,
            "to": new_state
        })
    
    def log_silence_detected(self, duration_ms: float, is_visitor: bool = True) -> None:
        """Registra detecção de silêncio."""
        self.log_event("SILENCE_DETECTED", {
            "source": "visitor" if is_visitor else "resident",
            "duration_ms": duration_ms
        })
    
    def log_error(self, error_type: str, message: str, details: Optional[Dict[str, Any]] = None) -> None:
        """Registra ocorrência de erro."""
        data = {
            "error_type": error_type,
            "message": message
        }
        if details:
            data["details"] = details
        
        self.log_event("ERROR", data)
        self.logger.error(f"{error_type}: {message}")
    
    def log_call_ended(self, reason: str, duration_ms: Optional[float] = None) -> None:
        """Registra término da chamada."""
        if duration_ms is None:
            duration_ms = (time.time() - self.start_time) * 1000
            
        self.log_event("CALL_ENDED", {
            "reason": reason,
            "total_duration_ms": duration_ms
        })
        
        # Fechar todos os handlers
        for handler in self.logger.handlers:
            handler.close()
            self.logger.removeHandler(handler)


# Singleton para gerenciar os loggers de chamadas
class CallLoggerManager:
    _loggers: Dict[str, CallLogger] = {}
    
    @classmethod
    def get_logger(cls, call_id: str) -> CallLogger:
        """Obtém ou cria um logger para o ID de chamada especificado."""
        if call_id not in cls._loggers:
            cls._loggers[call_id] = CallLogger(call_id)
        return cls._loggers[call_id]
    
    @classmethod
    def remove_logger(cls, call_id: str) -> None:
        """Remove um logger após o término da chamada."""
        if call_id in cls._loggers:
            del cls._loggers[call_id]

utils/__init__.py:


utils/README.md:
# Sistema de Logs para Análise de Desempenho

Este sistema de logs foi projetado para monitorar e analisar o desempenho das chamadas no projeto AudioSocket-Simple. O sistema registra detalhadamente cada etapa do processamento de chamadas, permitindo identificar gargalos e problemas de performance.

## Estrutura

O sistema consiste em:

1. **CallLogger**: Classe que registra eventos específicos de uma chamada em um arquivo log único (um arquivo por chamada).
2. **CallLoggerManager**: Gerenciador singleton que mantém instâncias de loggers para cada chamada ativa.
3. **log_analyzer.py**: Script para analisar os arquivos de log e gerar relatórios.

## Funcionamento

Cada chamada telefônica gera um arquivo de log no formato `{uuid_chamada}.log` contendo entradas JSON estruturadas com:

- Timestamp preciso de cada evento
- Tempo decorrido desde o início da chamada
- Tipo de evento (ex: SPEECH_DETECTED, TRANSCRIPTION_COMPLETE, etc.)
- Dados específicos do evento (duração, texto transcrito, etc.)

## Tipos de Eventos Registrados

O sistema registra eventos em todas as fases de uma chamada:

### Processamento de Áudio
- Detecção de fala (`SPEECH_DETECTED`)
- Término de fala (`SPEECH_ENDED`) 
- Detecção de silêncio (`SILENCE_DETECTED`)

### Transcrição
- Início da transcrição (`TRANSCRIPTION_START`)
- Conclusão da transcrição (`TRANSCRIPTION_COMPLETE`)

### Processamento de IA
- Início do processamento (`AI_PROCESSING_START`)
- Extração de intenções (`INTENT_EXTRACTION_START`, `INTENT_EXTRACTION_COMPLETE`)
- Validação fuzzy (`FUZZY_VALIDATION_START`, `FUZZY_VALIDATION_COMPLETE`)
- Conclusão do processamento (`AI_PROCESSING_COMPLETE`)

### Síntese de Fala
- Início da síntese (`SYNTHESIS_START`)
- Conclusão da síntese (`SYNTHESIS_COMPLETE`)

### Eventos de Chamada
- Configuração da chamada (`CALL_SETUP`)
- Saudação (`GREETING`)
- Mudança de estado (`STATE_CHANGE`)
- Comunicação com morador (`CALL_MORADOR`, `MORADOR_CONNECTED`)
- Término da chamada (`CALL_ENDED`)

### Erros
- Erros ocorridos durante a chamada (`ERROR`)

## Análise de Logs

O script `log_analyzer.py` oferece:

1. **Análise de uma única chamada**: 
   ```
   python utils/log_analyzer.py --call_id UUID_DA_CHAMADA
   ```

2. **Análise de todas as chamadas**: 
   ```
   python utils/log_analyzer.py --all
   ```

3. **Resumo agregado**:
   ```
   python utils/log_analyzer.py --all --summary
   ```

4. **Exportação para JSON**:
   ```
   python utils/log_analyzer.py --all --output relatorio.json
   ```

## Identificando Gargalos

Os relatórios produzidos pelo analisador ajudam a identificar diversos problemas:

1. **Tempos de Transcrição**: Valores altos podem indicar problemas com o serviço de transcrição ou qualidade do áudio.

2. **Tempos de Processamento da IA**: Valores elevados em etapas específicas (ex: extração de intenção) podem apontar necessidade de otimização dos modelos.

3. **Tempos de Síntese**: Atrasos na geração de áudio sintético que podem degradar a experiência.

4. **Detecção de VAD**: Problemas no reconhecimento de início e fim de falas.

5. **Erros**: Registro completo de falhas ocorridas durante as chamadas.

## Exemplo de Uso

Para usar este sistema em seu código, simplesmente obtenha um logger para a chamada atual:

```python
from utils.call_logger import CallLoggerManager

# No início de uma chamada
call_id = "uuid_da_chamada"
call_logger = CallLoggerManager.get_logger(call_id)

# Durante a chamada, registre eventos
call_logger.log_speech_detected()
call_logger.log_transcription_start(audio_data_size)
call_logger.log_transcription_complete(transcribed_text, transcription_time)
call_logger.log_ai_processing_start(transcribed_text)
call_logger.log_ai_processing_complete(response, processing_time)
call_logger.log_synthesis_start(response_text)
call_logger.log_synthesis_complete(audio_size, synthesis_time)

# Registrar eventos personalizados
call_logger.log_event("MY_CUSTOM_EVENT", {
    "some_key": "some_value",
    "another_key": 123
})

# No final da chamada
call_logger.log_call_ended("normal_disconnection")
CallLoggerManager.remove_logger(call_id)  # Liberar recursos
```

utils/unused/uuid_generator.py:
"""
Gerador de UUIDs v7 para o sistema de portaria digital.

Implementação baseada na especificação RFC: https://datatracker.ietf.org/doc/draft-ietf-uuidrev-rfc4122bis/
"""

import time
import uuid
import random

def uuid7():
    """
    Gera um UUID v7 baseado em timestamp.
    
    UUID v7 usa os primeiros 48 bits para um timestamp de milissegundos,
    segmento de 12 bits para sequência e 62 bits aleatórios.
    
    Returns:
        UUID: Um objeto UUID v7
    """
    # Obtém o timestamp em milissegundos (48 bits, mais significativos)
    timestamp_ms = int(time.time() * 1000)
    timestamp_bytes = timestamp_ms.to_bytes(6, byteorder='big')
    
    # Gera 10 bytes aleatórios (80 bits para sequência e aleatoriedade)
    random_bytes = random.randbytes(10)
    
    # Combina os bytes
    uuid_bytes = timestamp_bytes + random_bytes
    
    # Define a versão 7 nos bits apropriados (bits 48-51)
    uuid_bytes = bytearray(uuid_bytes)
    uuid_bytes[6] = (uuid_bytes[6] & 0x0F) | 0x70  # Versão 7
    uuid_bytes[8] = (uuid_bytes[8] & 0x3F) | 0x80  # Variante
    
    # Cria o UUID a partir dos bytes
    return uuid.UUID(bytes=bytes(uuid_bytes))

def uuid7str():
    """
    Retorna o UUID v7 como string.
    """
    return str(uuid7())

docs/03-configuracao.md:
# Configuração e Implantação do AudioSocket-Simple

Este documento descreve como configurar, instalar e executar o sistema AudioSocket-Simple no ambiente de desenvolvimento e produção.

## Requisitos do Sistema

### Software
- Python 3.8 ou superior
- Node.js v20.12.0 (para ferramentas de desenvolvimento)
- Bibliotecas Python listadas em `requirements.txt`
- Acesso a uma API de LLM (OpenAI, Groq, etc.)
- Azure Speech Services (conta e chave de API)

### Hardware Recomendado
- CPU: 2 núcleos ou mais
- RAM: 4GB ou mais
- Armazenamento: 1GB disponível
- Conexão de rede estável

## Instalação e Configuração

### 1. Preparação do Ambiente

```bash
# Clonar o repositório
git clone https://github.com/seu-usuario/audiosocket-simple.git
cd audiosocket-simple

# Ativar o Node.js correto via NVM
nvm use v20.12.0

# Criar e ativar ambiente virtual Python
python -m venv venv
source venv/bin/activate  # No Windows: venv\Scripts\activate

# Instalar dependências
pip install -r requirements.txt
```

### 2. Configuração do Ambiente

Crie um arquivo `.env` na raiz do projeto com as seguintes variáveis:

```
# Configurações do Azure Speech
AZURE_SPEECH_KEY=sua_chave_do_azure
AZURE_SPEECH_REGION=sua_regiao_do_azure

# Configurações da API de IA
AI_API_KEY=sua_chave_da_api_llm
AI_API_URL=https://api.url/v1

# Configurações do sistema
SILENCE_THRESHOLD_SECONDS=1.5
API_PORT=8082
```

### 3. Configuração de Arquivos do Sistema

#### config.json

O arquivo `config.json` na raiz do projeto controla vários aspectos do comportamento do sistema:

```json
{
  "greeting": {
    "message": "Condomínio Apoena, em que posso ajudar?",
    "voice": "pt-BR-AntonioNeural",
    "delay_seconds": 1.5
  },
  "system": {
    "silence_threshold_seconds": 1.5,
    "resident_max_silence_seconds": 45.0,
    "goodbye_delay_seconds": 3.0
  },
  "audio": {
    "transmission_delay_ms": 10,
    "post_audio_delay_seconds": 0.3,
    "discard_buffer_frames": 15
  },
  "call_termination": {
    "enabled": true,
    "goodbye_messages": {
      "visitor": {
        "authorized": "Sua entrada foi autorizada. Obrigado por utilizar nossa portaria inteligente.",
        "denied": "Sua entrada não foi autorizada. Obrigado por utilizar nossa portaria inteligente.",
        "default": "Obrigado por utilizar nossa portaria inteligente. Até a próxima!"
      },
      "resident": {
        "default": "Obrigado pela sua resposta. Encerrando a chamada."
      }
    }
  }
}
```

#### data/apartamentos.json

Este arquivo contém informações sobre apartamentos e moradores para validação:

```json
[
  {
    "apartment_number": "501",
    "residents": ["Daniel dos Reis", "Rafaela Silva"],
    "voip_number": "1003021"
  },
  {
    "apartment_number": "502",
    "residents": ["Maria Oliveira", "João Santos"],
    "voip_number": "1003022"
  }
]
```

## Execução da Aplicação

### Modo de Desenvolvimento

Para iniciar o sistema em modo de desenvolvimento:

```bash
# Ativar ambiente
source /Users/danerdosreis/development/environments/audiosocket-simple/bin/activate
nvm use v20.12.0

# Iniciar a aplicação principal
python main.py
```

Isto iniciará:
- Servidor AudioSocket para visitantes na porta 8080
- Servidor AudioSocket para moradores na porta 8081
- Servidor API HTTP na porta 8082

### Testes Locais sem Asterisk

Para testar o sistema sem um Asterisk real:

```bash
# Terminal 1: Iniciar servidor principal
python main.py

# Terminal 2: Iniciar cliente de microfone simulando um visitante
python microfone_client.py
```

## Configuração do Socket TCP

A otimização dos servidores de socket é importante para garantir a qualidade do áudio. As principais opções de configuração são definidas em `server_manager.py`:

```python
# Servidor para visitante (IA) - com parâmetros para melhor qualidade de áudio
ia_server = await asyncio.start_server(
    iniciar_servidor_audiosocket_visitante,
    binding_ip,  # Use 0.0.0.0 para binding
    porta_ia,
    # Manter apenas parâmetros essenciais e alguns importantes para qualidade
    limit=1024*1024,  # 1MB buffer
    start_serving=True
)
```

## Parâmetros Otimizados para Desempenho

Estes parâmetros foram ajustados para melhorar o desempenho do sistema:

| Parâmetro | Valor | Descrição |
|-----------|-------|-----------|
| `silence_threshold_seconds` | 1.5 | Tempo de silêncio necessário para considerar que o usuário parou de falar |
| `transmission_delay_ms` | 10 | Delay entre transmissões de pacotes de áudio (ms) |
| `post_audio_delay_seconds` | 0.3 | Atraso após envio de áudio antes de retornar ao modo de escuta |
| `discard_buffer_frames` | 15 | Quantidade de frames a descartar após IA falar (evita eco) |

## Integração com o Asterisk

Para integrar o sistema com Asterisk, configure o `extensions.conf`:

```
[from-internal]
exten => 1001,1,Answer()
exten => 1001,n,AudioSocket(127.0.0.1:8080,${CHANNEL(uniqueid)})
exten => 1001,n,Hangup()
```

## Verificação da Instalação

Para verificar se a instalação está funcionando corretamente:

1. Inicie o sistema: `python main.py`
2. Verifique os logs em tempo real: `tail -f logs/audiosocket.log`
3. Teste a API: `curl http://localhost:8082/api/status`
4. Teste o encerramento de chamadas: `curl -X POST -H "Content-Type: application/json" -d '{"call_id":"UUID-DA-CHAMADA", "role":"visitor"}' http://localhost:8082/api/hangup`

## Solução de Problemas

### Portas em Uso
Se encontrar erros sobre portas já em uso:
```bash
sudo lsof -i :8080
sudo lsof -i :8081
sudo kill <PID>
```

### Erros de Transcrição
Verifique suas credenciais do Azure Speech:
```bash
curl -s -X POST "https://<region>.api.cognitive.microsoft.com/sts/v1.0/issuetoken" \
     -H "Ocp-Apim-Subscription-Key: <key>" \
     -H "Content-type: application/x-www-form-urlencoded" \
     -d ""
```

---

*Próximo documento: [04-fluxos-comunicacao.md](04-fluxos-comunicacao.md)*

docs/04-fluxos-comunicacao.md:
# Fluxos de Comunicação

Este documento descreve em detalhes os fluxos de comunicação entre os diversos componentes do sistema AudioSocket-Simple, desde o atendimento do visitante até o encerramento das chamadas.

## Fluxo Completo de Uma Chamada

### 1. Início da Chamada

1. **Inicialização do Sistema**:
   - Sistema inicia dois servidores AudioSocket (visitante e morador)
   - Servidores aguardam conexões nas portas 8080 e 8081

2. **Chegada de Visitante**:
   - Visitante liga para o ramal da portaria
   - Asterisk encaminha chamada para o AudioSocket na porta 8080
   - Sistema recebe conexão e cria uma nova sessão com ID único
   - Mensagem de saudação é enviada ao visitante

### 2. Coleta de Dados do Visitante

1. **Detecção de Fala**:
   - Sistema usa VAD para detectar quando o visitante começa a falar
   - Após silêncio de 1,5 segundos, considera que a fala terminou

2. **Processamento da Fala**:
   - Áudio do visitante é transcrito pelo Azure Speech
   - Texto transcrito é enviado para processamento

3. **Extração de Intenções**:
   - Sistema extrai progressivamente:
     - Tipo de intenção (entrega, visita, serviço)
     - Nome do visitante
     - Número do apartamento e nome do morador

4. **Validação de Dados**:
   - Sistema valida os dados usando fuzzy matching
   - Verifica se apartamento e morador existem no banco de dados

### 3. Contato com o Morador

1. **Estabelecimento de Chamada**:
   - Sistema envia comando clicktocall via AMQP
   - Morador recebe chamada telefônica
   - Sistema aguarda até 2 tentativas se não houver resposta

2. **Quando Morador Atende**:
   - Conexão estabelecida com o servidor na porta 8081 (mesmo ID da sessão)
   - Sistema envia mensagem informando sobre o visitante
   - Estado muda para ESPERANDO_MORADOR

### 4. Interação com o Morador

1. **Apresentação do Contexto**:
   ```
   Olá morador do apartamento 501! Pedro da Silva está na portaria solicitando uma entrega. 
   Você autoriza a entrada? Responda SIM ou NÃO.
   ```

2. **Processamento da Resposta**:
   - Sistema reconhece três tipos de respostas:
     - Pedido de mais informações ("Quem é?" ou "?")
     - Autorização ("sim", "autorizo", "pode entrar")
     - Negação ("não", "nego", "não autorizo")

3. **Tratamento de Respostas Ambíguas**:
   - Sistema pede confirmação quando não entende a resposta
   ```
   Desculpe, não consegui entender sua resposta. Por favor, responda SIM para 
   autorizar a entrada ou NÃO para negar.
   ```

### 5. Comunicação com o Visitante

1. **Informação da Decisão**:
   - Sistema informa o visitante sobre a decisão do morador
   ```
   Boa notícia! O morador autorizou sua entrega.
   ```
   ou
   ```
   Infelizmente o morador não autorizou sua entrada neste momento.
   ```

2. **Mensagem de Despedida**:
   - Sistema envia mensagem de despedida personalizada

### 6. Encerramento das Chamadas

1. **Encerramento Ordenado**:
   - Sistema envia mensagens finais a ambos participantes
   - Após um delay para que as mensagens sejam ouvidas (3-5 segundos)
   - Envia comando KIND_HANGUP (0x00) para encerrar as conexões
   - Libera recursos associados à sessão

## Diagrama de Estados

```
┌─────────────────┐           ┌───────────────┐           ┌────────────────────┐
│  COLETANDO      │           │               │           │                    │
│  DADOS          ├──────────►│   VALIDADO    ├──────────►│  CHAMANDO_MORADOR  │
└─────────────────┘           └───────────────┘           └──────────┬─────────┘
                                                                      │
                                                                      ▼
┌─────────────────┐           ┌───────────────┐           ┌────────────────────┐
│                 │           │               │           │                    │
│   FINALIZADO    │◄──────────┤ ESPERANDO     │◄──────────┤ CALLING_IN_PROGRESS│
│                 │           │ MORADOR       │           │                    │
└─────────────────┘           └───────────────┘           └────────────────────┘
```

## Detalhes por Tipo de Chamada

### Visitante → IA (Porto 8080)

1. **Inicialização**: 
   - `iniciar_servidor_audiosocket_visitante()`
   - Saudação inicial e criação de sessão

2. **Loop Principal**:
   - `receber_audio_visitante()`: Captura áudio, detecta fim da fala, transcreve
   - `enviar_mensagens_visitante()`: Monitora fila de mensagens, sintetiza e envia resposta

3. **Processamento**:
   - `session_manager.process_visitor_text()`: Gerencia o texto do visitante
   - `conversation_flow.on_visitor_message()`: Processa a mensagem com IA

### Morador → IA (Porto 8081)

1. **Inicialização**:
   - `iniciar_servidor_audiosocket_morador()`
   - Recupera contexto da sessão existente

2. **Loop Principal**:
   - `receber_audio_morador()`: Captura áudio, com configuração mais sensível para respostas curtas
   - `enviar_mensagens_morador()`: Monitora fila de mensagens, sintetiza e envia ao morador

3. **Processamento**:
   - `session_manager.process_resident_text()`: Gerencia o texto do morador
   - `conversation_flow.on_resident_message()`: Interpreta decisão do morador

## Tratamento de Encerramento de Chamadas

### Encerramento Programático

O sistema pode encerrar chamadas de forma programática usando o sinal KIND_HANGUP:

1. **Via ConversationFlow**:
   - `_schedule_active_hangup()`: Agenda o encerramento após delay
   - Envia KIND_HANGUP (0x00) para o cliente AudioSocket
   - Trata exceções de ConnectionResetError adequadamente

2. **Via API HTTP**:
   - Endpoint `/api/hangup`
   - Parâmetros: `call_id` e `role` (visitor/resident)
   - Notifica o cliente correto com KIND_HANGUP

### Mecanismo de Encerramento

```python
# 1. Enviar KIND_HANGUP
writer.write(struct.pack('>B H', 0x00, 0))
await writer.drain()

# 2. Sinalizar que a sessão deve terminar
session_manager.end_session(call_id)

# 3. Limpar completamente após delay
session_manager._complete_session_termination(call_id)
```

### Tratamento de Erros

- System trata graciosamente erros de `ConnectionResetError` quando o cliente desconecta
- Implementa timeouts para evitar bloqueios em operações de I/O
- Mantém logs detalhados para diagnóstico e monitoramento

## Comunicação entre Estados

A progressão entre estados é controlada pelo ConversationFlow e comunica-se com o sessão via:

1. `session_manager.enfileirar_visitor()`: Envia mensagem para o visitante
2. `session_manager.enfileirar_resident()`: Envia mensagem para o morador
3. `session_manager.end_session()`: Sinaliza encerramento das conexões

## Diagrama de Sequência Detalhado

```
┌─────────┐   ┌──────────┐   ┌───────────┐   ┌────────────┐   ┌──────────┐   ┌─────────┐
│Visitante│   │AudioSocket│   │SessionMgr │   │  Flow      │   │  IA      │   │ Morador │
└────┬────┘   └─────┬────┘   └─────┬─────┘   └─────┬──────┘   └────┬─────┘   └────┬────┘
     │              │              │               │                │             │
     │──chamada────>│              │               │                │             │
     │              │──cria sessão>│               │                │             │
     │              │<─confirma────│               │                │             │
     │<─saudação────│              │               │                │             │
     │──fala───────>│              │               │                │             │
     │              │──transcreve─>│               │                │             │
     │              │<─texto───────│               │                │             │
     │              │              │──processa────>│                │             │
     │              │              │               │──extrai intent>│             │
     │              │              │               │<─intent────────│             │
     │              │              │<─resposta─────│                │             │
     │<─pergunta────│              │               │                │             │
     │──resposta───>│              │               │                │             │
     │              │              │──processa────>│                │             │
     │              │              │               │──dados validos>│             │
     │              │              │               │<─confirmação───│             │
     │              │              │               │────────────────────clicktocal>│
     │<─aguarde─────│              │               │                │             │
     │              │              │               │                │<─atendimento│
     │              │              │               │<────────────────────────────│
     │              │              │               │────────────────────────────>│
     │              │              │               │<────────────────────decisão│
     │<─autorizado──│              │               │                │             │
     │              │              │<─encerrar─────│                │             │
     │<─kind_hangup─│              │               │                │             │
     │──fim────────>│              │               │                │             │
     │              │<─fim─────────│               │                │<─kind_hangup│
     │              │              │<─limpar───────│                │             │
     
```

---

*Próximo documento: [05-interacao-morador.md](05-interacao-morador.md)*

docs/deteccao-voz-azure.md:
# Detecção de Voz com Azure Speech

Este documento descreve a implementação de detecção de voz e silêncio usando o Azure Speech SDK como alternativa ao webrtcvad no AudioSocket-Simple.

## Visão Geral

O sistema AudioSocket-Simple requer detecção precisa de quando o usuário começa e termina de falar para funcionar corretamente. Originalmente, o sistema utilizava apenas a biblioteca `webrtcvad` para realizar esta detecção. Embora o webrtcvad seja eficaz em ambientes controlados, ele pode ter dificuldades em ambientes ruidosos, onde ruídos de fundo como carros passando, televisores ligados ou conversas ao fundo podem interferir na detecção correta do término da fala.

Para resolver este problema, implementamos um método alternativo de detecção de voz usando o Azure Speech SDK, que oferece algoritmos mais avançados para detecção de atividade de voz em ambientes ruidosos. O sistema agora permite escolher entre os dois métodos de detecção:

1. **WebRTCVAD** (implementação original) - Rápido e leve, ideal para ambientes silenciosos
2. **Azure Speech** (nova implementação) - Mais robusto contra ruídos, ideal para ambientes ruidosos

## Configuração

A escolha do método de detecção de voz é controlada no arquivo `config.json`:

```json
{
  "system": {
    "voice_detection_type": "azure_speech",  // opções: "webrtcvad" ou "azure_speech"
    "azure_speech_segment_timeout_ms": 800,  // timeout de silêncio em ms para segmentação (Azure)
    "silence_threshold_seconds": 1.5,        // timeout de silêncio para webrtcvad (em segundos)
    "resident_max_silence_seconds": 45.0     // timeout máximo para morador
  },
  // ... outras configurações
}
```

### Parâmetros de Configuração

- **voice_detection_type**: Determina qual método de detecção de voz será usado
  - `webrtcvad`: Usa a biblioteca webrtcvad para detecção (padrão)
  - `azure_speech`: Usa o Azure Speech SDK para detecção

- **azure_speech_segment_timeout_ms**: Define o tempo de silêncio (em milissegundos) que o Azure Speech aguarda após detectar silêncio para considerar que a fala terminou.
  - Valores menores (400-600ms) são mais reativos, mas podem cortar frases pausadas
  - Valores maiores (800-1000ms) permitem pausas mais longas na fala
  - Para o morador, o sistema usa um valor ligeiramente menor que este para detectar respostas curtas como "sim" de forma mais rápida

- **silence_threshold_seconds**: Define o tempo de silêncio (em segundos) para o método webrtcvad considerar que a fala terminou.

## Funcionamento

### Azure Speech SDK

Quando o método `azure_speech` está ativo, o sistema:

1. Inicia uma sessão de reconhecimento contínuo com o Azure Speech SDK
2. Monitora eventos do Azure Speech que indicam início e fim da fala
3. Coleta áudio durante períodos detectados como fala ativa
4. Processa o texto reconhecido pelo Azure ou, se necessário, envia o áudio coletado para transcrição

Benefícios específicos:

- **Melhor filtro de ruído**: O Azure Speech SDK usa modelos de IA para distinguir voz humana de ruídos de fundo
- **Detecção mais precisa de fim de fala**: Identifica melhor quando o usuário realmente terminou de falar mesmo em ambientes ruidosos
- **Detecção de fala mais rápida**: O sistema pode detectar o início da fala mais rapidamente

### Diferença entre Visitante e Morador

A implementação tem considerações especiais para cada papel:

- **Visitante**: Configurado para permitir frases mais longas, com maiores limiares de segmentação
- **Morador**: Otimizado para respostas curtas como "sim", "não" ou perguntas rápidas, com limiares de segmentação mais curtos e processamento especial para falas muito breves

## Testes e Verificação

Para testar o sistema com a detecção Azure Speech:

1. Configure `voice_detection_type` como `azure_speech` no `config.json`
2. Verifique se as variáveis de ambiente `AZURE_SPEECH_KEY` e `AZURE_SPEECH_REGION` estão definidas corretamente
3. Inicie o sistema e teste em diferentes ambientes:
   - Ambiente silencioso
   - Ambiente com ruído de fundo (TV, rádio, pessoas conversando)
   - Ambiente com sons intermitentes (portas batendo, telefones tocando)

Os logs da aplicação mostrarão informações detalhadas sobre a detecção de voz:

```
INFO:[call_id] Início de fala detectado pelo Azure Speech
INFO:[call_id] Fim de fala detectado pelo Azure Speech
INFO:[call_id] Azure Speech reconheceu texto: "Olá, gostaria de falar com o morador do 501"
```

## Considerações e Performance

- **Latência**: A detecção com Azure Speech pode adicionar uma pequena latência adicional, mas geralmente é imperceptível para o usuário final
- **Custo**: Esta implementação utiliza o serviço Azure Speech, que gera custos conforme o uso
- **Confiabilidade**: A detecção de voz com Azure Speech é mais confiável em ambientes ruidosos, reduzindo erros de cortes prematuros ou atrasos no processamento

## Recomendações

1. Para ambientes controlados e silenciosos, o método `webrtcvad` pode ser suficiente e mais econômico
2. Para ambientes com ruído de fundo significativo (halls de entrada, portarias com movimento, etc.), recomenda-se o uso do `azure_speech`
3. O parâmetro `azure_speech_segment_timeout_ms` pode ser ajustado conforme necessário:
   - Valores mais baixos (600-700ms) para interações rápidas
   - Valores mais altos (800-1000ms) para conversas mais naturais com pausas

## Solução de Problemas

- **Frases sendo cortadas**: Aumente o valor de `azure_speech_segment_timeout_ms`
- **Demora em processar após parar de falar**: Diminua o valor de `azure_speech_segment_timeout_ms`
- **Não detecta falas curtas do morador**: Verifique se o processamento de falas curtas está funcionando corretamente

## Mecanismo de Detecção de Deadlock

Para evitar situações em que o sistema pode ficar travado ao aguardar eventos do Azure Speech que nunca ocorrem, implementamos um mecanismo de detecção de deadlock com as seguintes características:

1. **Monitoramento periódico**: A cada ~5 segundos (250 frames de áudio), o sistema verifica o estado atual da detecção de voz
2. **Detecção de silêncio prolongado**: Se o sistema estiver coletando áudio por mais de 10 segundos sem que o Azure Speech tenha detectado o fim da fala, é considerado um possível deadlock
3. **Processamento forçado**: Quando um deadlock é detectado, o sistema força o processamento do áudio coletado até aquele momento, como se tivesse detectado o fim da fala naturalmente
4. **Registro de diagnóstico**: O sistema registra um aviso nos logs quando um deadlock é detectado e resolvido

Este mecanismo garante que mesmo se o Azure Speech SDK falhar em detectar corretamente o fim da fala (o que pode ocorrer em condições específicas de áudio), o sistema continuará funcionando e processando o áudio do usuário.

Exemplo de mensagem de log ao detectar um deadlock:
```
WARNING:[call_id] Detectado possível deadlock no Azure Speech! Forçando processamento manual após 10s de silêncio.
```

---

*Ver também: [11-deteccao-voz-avancada.md](11-deteccao-voz-avancada.md)*

docs/10-desenvolvimento-futuro.md:
# Desenvolvimento Futuro e Backlog

Este documento lista melhorias planejadas, backlog de funcionalidades e direções futuras para o projeto AudioSocket-Simple. Ele serve como um registro das próximas etapas e considerações para o desenvolvimento contínuo do sistema.

## Itens de Backlog

### Melhorias de Performance

- [ ] **Paralelização de Processamento de IA**
  - Implementar extração simultânea de intenções usando `asyncio.gather()`
  - Prioridade: Alta
  - Benefício: Redução de ~30-40% no tempo total de processamento de IA

- [ ] **Transcrição em Streaming**
  - Implementar processamento de áudio em streaming em vez de batch
  - Prioridade: Média
  - Benefício: Feedback mais rápido durante fala longa

- [ ] **Otimização de Modelos LLM**
  - Testar modelos LLM menores e mais rápidos para tarefas específicas
  - Prioridade: Média
  - Benefício: Redução de custos e latência

### Novos Recursos

- [ ] **Timeout Automático de Sessões**
  - Implementar encerramento automático após inatividade prolongada
  - Prioridade: Alta
  - Detalhes: Encerrar chamada após 2 minutos sem interação

- [ ] **Síntese de Voz Aprimorada**
  - Explorar vozes mais naturais e expressivas
  - Prioridade: Baixa
  - Detalhes: Testar novos modelos de síntese com prosódia melhorada

### Monitoramento e Observabilidade

- [ ] **Dashboard em Tempo Real**
  - Implementar dashboard para monitoramento de chamadas ativas
  - Prioridade: Média
  - Detalhes: Métricas de tempo real, estado das chamadas, logs

- [ ] **Sistema de Métricas Detalhadas**
  - Coletar métricas detalhadas sobre todos os aspectos do sistema
  - Prioridade: Média
  - Detalhes: Tempo médio de chamada, taxa de sucesso, uso de cache

- [ ] **Alarmes e Notificações**
  - Sistema para alertar sobre problemas ou anomalias
  - Prioridade: Alta
  - Detalhes: Notificações via e-mail ou Slack para erros críticos

### Infraestrutura e Arquitetura

- [ ] **Sistema de Multi-Servidores**
  - Implementar uma aplicação supervisora para gerenciar ramais dinâmicos
  - Prioridade: Alta
  - Detalhes: Aplicação separada que escuta alterações no banco de dados e inicia/para servidores AudioSocket

- [ ] **Containerização**
  - Preparar sistema para execução em contêineres Docker
  - Prioridade: Média
  - Detalhes: Dockerfile, docker-compose.yml, considerações para estado

- [ ] **Configuração Externalizada**
  - Mover todas as configurações para variáveis de ambiente ou arquivo externo
  - Prioridade: Média
  - Detalhes: Separar completamente código e configuração

### Segurança e Resiliência

- [ ] **Autenticação na API**
  - Adicionar autenticação para endpoints da API HTTP
  - Prioridade: Alta
  - Detalhes: Implementar sistema de API keys ou tokens JWT

- [ ] **TLS para AudioSocket**
  - Investigar possibilidade de usar TLS para conexões AudioSocket
  - Prioridade: Baixa
  - Detalhes: Pesquisar suporte no Asterisk, implementar se possível

- [ ] **Persistência de Estado**
  - Salvar estado das sessões para recuperação em caso de reinício
  - Prioridade: Média
  - Detalhes: Serializar estado mínimo para arquivo ou banco de dados

## Ramais Dinâmicos

> **Nota**: A funcionalidade de ramais dinâmicos gerenciados pela própria aplicação foi removida desta implementação devido à complexidade de gerenciar múltiplos sockets em diversas portas. Em vez disso, planejamos criar uma aplicação supervisora separada que gerenciará os servidores AudioSocket.

### Plano de Implementação:

1. **Aplicação Supervisora**
   - Lê configurações do banco de dados PostgreSQL
   - Inicia instâncias separadas do AudioSocket-Simple
   - Monitora saúde e reinicia quando necessário

2. **Arquitetura Proposta**
   ```
   ┌───────────────┐      ┌─────────────────┐
   │ PostgreSQL DB │ ←──→ │ Supervisor App  │
   └───────────────┘      └────────┬────────┘
                                    │
                                    ↓
         ┌───────────────┐  ┌───────────────┐  ┌───────────────┐
         │ AudioSocket   │  │ AudioSocket   │  │ AudioSocket   │
         │ Instance #1   │  │ Instance #2   │  │ Instance #3   │
         └───────────────┘  └───────────────┘  └───────────────┘
   ```

## Integração e APIs

- [ ] **API para Gestão de Chamadas**
  - Endpoints para visualizar, iniciar e finalizar chamadas
  - Prioridade: Média
  - Detalhes: Expandir a API HTTP atual com mais funcionalidades

- [ ] **Webhook para Eventos**
  - Sistema de notificação via webhook para eventos importantes
  - Prioridade: Baixa
  - Detalhes: Notificar sistemas externos sobre início/fim de chamadas

- [ ] **Integração com Sistemas de Condomínio**
  - APIs para interagir com sistemas de gestão de condomínio
  - Prioridade: Média
  - Detalhes: Registro de visitas, consulta de moradores autorizados

## Testes e Qualidade

- [ ] **Testes Unitários**
  - Desenvolver suite completa de testes unitários
  - Prioridade: Alta
  - Detalhes: Cobertura para componentes críticos como SessionManager, ConversationFlow

- [ ] **Testes de Integração**
  - Testes automatizados para verificar integrações
  - Prioridade: Média
  - Detalhes: Verificar comunicação entre componentes

- [ ] **Testes de Carga**
  - Verificar comportamento sob carga pesada
  - Prioridade: Média
  - Detalhes: Simular múltiplas chamadas simultâneas

## Experiência do Usuário

- [ ] **Detecção de Interrupção**
  - Melhorar detecção quando visitante ou morador interrompem a IA
  - Prioridade: Média
  - Detalhes: Detectar fala durante reprodução de áudio

- [ ] **Personalização de Vozes**
  - Permitir escolha de vozes por condomínio
  - Prioridade: Baixa
  - Detalhes: Configuração por condomínio no banco de dados

- [ ] **Confirmações Mais Naturais**
  - Melhorar as respostas de confirmação para soarem mais naturais
  - Prioridade: Média
  - Detalhes: Variar respostas, utilizar contexto melhor

## Como Contribuir

Se você deseja implementar algum destes itens ou sugerir novos:

1. Selecione um item do backlog
2. Crie uma branch feature/[nome-da-funcionalidade]
3. Implementa a funcionalidade
4. Abra um Pull Request
5. Atualize este documento marcando o item como concluído

## Status de Implementação

Para marcar um item como concluído, atualize o documento substituindo `[ ]` por `[x]` e adicione a data de conclusão:

```markdown
- [x] **Nome da Funcionalidade** (Concluído: 15/05/2025)
  - Descrição da funcionalidade
  - Detalhes da implementação
```

## Priorização

A priorização dos itens acima é baseada em:

1. **Valor para o Usuário**: Impacto na experiência do usuário
2. **Viabilidade Técnica**: Facilidade de implementação
3. **Dependências**: Requisitos de outros componentes

A ordem de implementação deve seguir geralmente a prioridade indicada, mas pode ser ajustada conforme necessidades específicas do projeto.

---

*Próximo documento: [11-deteccao-voz-avancada.md](11-deteccao-voz-avancada.md)*


docs/05-interacao-morador.md:
# Interação com o Morador

Este documento detalha o fluxo específico de interação com o morador quando ele atende a chamada originada pelo sistema de portaria inteligente.

## Visão Geral

A interação com o morador é uma etapa crítica no fluxo da portaria inteligente. Após coletar e validar as informações do visitante, o sistema inicia uma chamada para o telefone do morador. Esta seção do fluxo foi cuidadosamente projetada para oferecer uma experiência natural e eficiente para o morador.

## Fluxo Detalhado de Interação

### 1. Estabelecimento da Chamada

1. **Conexão com o Morador**:
   - O sistema envia comando `clicktocall` via AMQP/RabbitMQ
   - O Asterisk inicia uma chamada telefônica para o número do morador
   - Quando atendida, a chamada é conectada ao servidor AudioSocket na porta 8081
   - O mesmo UUID (call_id) da sessão do visitante é utilizado para manter o contexto

2. **Detecção de Atendimento**:
   - O sistema detecta quando o morador atende a chamada
   - Estado da conversa muda para `ESPERANDO_MORADOR`
   - É enviada uma mensagem especial `AUDIO_CONNECTION_ESTABLISHED` internamente

### 2. Comunicação Inicial

1. **Saudação Personalizada**:
   ```
   Olá morador do apartamento {apt}! {visitor_name} está na portaria 
   solicitando {intent_desc}. Você autoriza a entrada? Responda SIM ou NÃO.
   ```

2. **Exemplos de Personalização**:
   - **Entrega**: "Olá morador do apartamento 501! Pedro da Silva está na portaria solicitando uma entrega. Você autoriza a entrada? Responda SIM ou NÃO."
   - **Visita**: "Olá morador do apartamento 501! Pedro da Silva está na portaria solicitando uma visita. Você autoriza a entrada? Responda SIM ou NÃO."
   - **Serviço**: "Olá morador do apartamento 501! Pedro da Silva está na portaria solicitando um serviço. Você autoriza a entrada? Responda SIM ou NÃO."

3. **Notificação ao Visitante**:
   - O sistema informa ao visitante: "O morador atendeu. Aguarde enquanto verificamos sua autorização..."

### 3. Processamento de Respostas do Morador

O sistema está preparado para lidar com diferentes tipos de resposta do morador:

#### a) Pedido de Mais Informações

Se o morador responde com uma pergunta (contendo "quem" ou "?"), o sistema fornece detalhes adicionais:

```python
if "quem" in lower_text or "?" in lower_text:
    additional_info = f"{visitor_name} está na portaria para {intent_type}. "
    if intent_type == "entrega":
        additional_info += "É uma entrega para seu apartamento."
    # ...
```

**Exemplo**:
- **Morador**: "Quem está aí?"
- **Sistema**: "Pedro da Silva está na portaria para entrega. É uma entrega para seu apartamento. Por favor, responda SIM para autorizar ou NÃO para negar."

#### b) Autorização

O sistema reconhece diversas expressões de autorização:

```python
elif "sim" in lower_text or "autorizo" in lower_text or "pode entrar" in lower_text or "autorizado" in lower_text or "deixa entrar" in lower_text or "libera" in lower_text or "ok" in lower_text or "claro" in lower_text or "positivo" in lower_text:
    # ...
```

**Exemplo**:
- **Morador**: "Sim, pode entrar"
- **Sistema**: "Obrigado! Pedro da Silva será informado que a entrega foi autorizada."
- **Para o Visitante**: "Ótima notícia! O morador autorizou sua entrega."

#### c) Negação

O sistema também reconhece expressões de negação:

```python
elif "não" in lower_text or "nao" in lower_text or "nego" in lower_text or "negativa" in lower_text or "negado" in lower_text or "bloqueado" in lower_text or "barrado" in lower_text:
    # ...
```

**Exemplo**:
- **Morador**: "Não, não autorizo"
- **Sistema**: "Entendido. Pedro da Silva será informado que a entrega não foi autorizada."
- **Para o Visitante**: "Infelizmente o morador não autorizou sua entrega neste momento."

#### d) Resposta Ambígua

Quando a resposta não é clara:

```python
else:
    # Resposta não reconhecida
    session_manager.enfileirar_resident(
        session_id, 
        "Desculpe, não consegui entender sua resposta. Por favor, responda SIM para autorizar a entrada ou NÃO para negar."
    )
```

### 4. Finalização da Interação

1. **Após Decisão do Morador**:
   - Sistema registra a decisão `authorization_result` (authorized/denied)
   - Envia mensagem de confirmação para o morador
   - Atualiza o estado para `FINALIZADO`

2. **Processo de Encerramento**:
   - Envia mensagem de despedida para o morador
   - Inicia processo de encerramento com `_finalizar()`
   - Agenda envio de KIND_HANGUP após delay para permitir ouvir a mensagem

## Detecção e Transcrição de Fala do Morador

A detecção de fala do morador usa configurações especiais para captar respostas curtas:

1. **Configuração Mais Sensível**:
   ```python
   vad = webrtcvad.Vad(3)  # Nível de agressividade maior (0-3)
   ```

2. **Processamento de Falas Curtas**:
   ```python
   # Mesmo com fala muito curta, processamos, pois pode ser um "Sim" rápido
   if len(frames) < 20:  # ~0.4 segundo de áudio (20 frames de 20ms)
       logger.info(f"Fala CURTA do morador detectada: {len(frames)} frames (~{len(frames)*20}ms) - Processando mesmo assim")
       # NÃO descartamos frames curtos para capturar "Sim" rápidos
   ```

## Diagrama de Sequência

```
┌─────────┐   ┌───────────┐   ┌───────────┐   ┌────────────────┐
│ Morador │   │AudioSocket│   │SessionMgr │   │ConversationFlow│
└────┬────┘   └─────┬─────┘   └─────┬─────┘   └───────┬────────┘
     │              │               │                 │
     │<─chamada─────│               │                 │
     │──atende─────>│               │                 │
     │              │──notifica────>│                 │
     │              │               │─────────────────>
     │              │               │                 │─┐
     │              │               │                 │ │ Muda para 
     │              │               │                 │<┘ ESPERANDO_MORADOR
     │<─saudação────│<──────────────│<────────────────│
     │──pergunta───>│               │                 │
     │              │──transcreve──>│                 │
     │              │               │───────────────>│
     │<─detalhes────│<──────────────│<────────────────│
     │──decisão────>│               │                 │
     │              │──transcreve──>│                 │
     │              │               │───────────────>│
     │              │               │                 │─┐
     │              │               │                 │ │ Muda para 
     │              │               │                 │<┘ FINALIZADO
     │<─confirmação─│<──────────────│<────────────────│
     │              │               │                 │─┐
     │              │               │                 │ │ Inicia
     │              │               │                 │ │ encerramento
     │              │               │<────────────────│<┘
     │<─despedida───│<──────────────│                 │
     │<─kind_hangup─│               │                 │
     │──fim────────>│               │                 │
```

## Tratamento de Timeout e Não-Atendimento

O sistema gerencia casos em que o morador não atende:

1. **Processo de Tentativas**:
   - Até 2 tentativas (configurável)
   - Timeout de 10 segundos por tentativa

2. **Quando Não Há Resposta**:
   ```python
   # Se todas as tentativas falharam, notifica o visitante
   logger.info(f"[Flow] Todas as {self.max_tentativas} tentativas de contato com o morador falharam")
   session_manager.enfileirar_visitor(
       session_id,
       "Não foi possível contatar o morador no momento. Por favor, tente novamente mais tarde."
   )
   ```

## Considerações Importantes

1. **Tempo de Silêncio**: Configuração especial para respostas típicas do morador (geralmente curtas)

2. **Flexibilidade nas Respostas**: Sistema reconhece múltiplas formas de dizer "sim" e "não"

3. **Contextualização**: Mensagens sempre são contextualizadas para o tipo de visita

4. **Resiliência**: Tratamento adequado de casos onde o morador desliga ou há problemas de conexão

---

*Próximo documento: [06-encerramento-chamadas.md](06-encerramento-chamadas.md)*

docs/11-deteccao-voz-avancada.md:
# 11. Detecção de Voz Avançada com Azure Speech

Este documento descreve as melhorias avançadas implementadas no sistema de detecção de voz usando o Azure Speech SDK, com foco nas otimizações para reduzir falsos positivos e garantir processamento robusto em ambientes desafiadores.

## Visão Geral

O sistema AudioSocket-Simple utiliza o Azure Speech SDK como alternativa ao WebRTCVAD para detecção de voz. Embora a implementação básica do Azure Speech tenha melhorado a precisão da detecção, identificamos alguns desafios específicos:

1. **Falsos positivos** em detecção de fim de fala
2. **Processamento desnecessário** de ruídos e áudios curtos
3. **Loops de feedback** quando o sistema detecta sua própria saída de áudio

Este documento descreve as melhorias implementadas para resolver esses problemas e garantir uma detecção de voz mais robusta e confiável.

## Melhorias Implementadas

### 1. Filtragem Rigorosa de Eventos de Fim de Fala

Uma das principais causas de processamento desnecessário era a detecção de eventos de "fim de fala" sem um correspondente "início de fala". Para resolver isso, implementamos um sistema de filtragem em múltiplas camadas:

```python
# Filtragem rigorosa de eventos de fim de fala
if not self.collecting_audio:
    # Verificar se já tivemos alguma detecção de fala antes
    if not self.speech_detected:
        logger.warning(f"[{self.call_id}] IGNORANDO fim de fala por não haver início de fala detectado anteriormente")
        return
    
    # Verificar se temos algo no pre-buffer e se tem tamanho suficiente
    if not hasattr(self, 'pre_buffer') or not self.pre_buffer or len(self.pre_buffer) < 10:
        logger.warning(f"[{self.call_id}] IGNORANDO fim de fala - pre-buffer muito pequeno ou inexistente")
        return
```

Além disso, implementamos análise de energia do áudio para confirmar que o áudio contém voz real:

```python
# Verificar energia do áudio no pre-buffer para confirmar que é uma fala real
try:
    # Analisar apenas os últimos 10 frames para economia de processamento
    frames_to_analyze = self.pre_buffer[-10:]
    total_energy = 0
    
    for frame in frames_to_analyze:
        samples = struct.unpack('<' + 'h' * (len(frame) // 2), frame)
        frame_energy = sum(sample ** 2 for sample in samples) / len(samples)
        total_energy += frame_energy
    
    avg_energy = total_energy / len(frames_to_analyze)
    ENERGY_THRESHOLD = 800  # Threshold mais alto para confirmar que é fala real
    
    if avg_energy < ENERGY_THRESHOLD:
        logger.warning(f"[{self.call_id}] IGNORANDO fim de fala - energia muito baixa no pre-buffer ({avg_energy:.2f} < {ENERGY_THRESHOLD})")
        return
```

### 2. Verificação de Tamanho Mínimo e Energia do Áudio

Para evitar o processamento de áudio muito curto ou com pouca energia (que pode ser ruído ambiente), implementamos verificações adicionais:

1. **Verificação de Tamanho Mínimo do Buffer**:

```python
# Verificação final de tamanho mínimo do buffer
MINIMUM_VALID_FRAMES = 15  # Aproximadamente 300ms de áudio
if len(self.audio_buffer) < MINIMUM_VALID_FRAMES:
    logger.warning(f"[{self.call_id}] Buffer muito pequeno para processamento ({len(self.audio_buffer)} < {MINIMUM_VALID_FRAMES} frames) - descartando evento")
    # Limpar buffer e cancelar evento
    self.audio_buffer = []
    self.collecting_audio = False
    return
```

2. **Verificação de Energia do Áudio na Transcrição**:

```python
# Verificação para áudio muito curto - provável ruído
if audio_size < 4800:  # Menos de 300ms de áudio (~15 frames)
    print(f"[TRANSCRIÇÃO] Áudio muito curto detectado ({audio_size} bytes, ~{duracao_estimada:.2f}s) - considerando ruído ou resposta curta")
    return "ok"

# Verificar energia do áudio para descartar ruído
try:
    # Calcular energia média
    energy = sum(sample ** 2 for sample in samples) / len(samples)
    ENERGY_THRESHOLD = 600  # Threshold ajustável para considerar áudio válido
    
    if energy < ENERGY_THRESHOLD:
        print(f"[TRANSCRIÇÃO] Áudio com energia muito baixa ({energy:.2f} < {ENERGY_THRESHOLD}) - considerando ruído")
        # Para áudios com pouca energia, tratamos como confirmação
        return "ok"
```

### 3. Pré-Buffer Ampliado

Para garantir que o sistema não perca o início da fala mesmo quando o Azure Speech detecta apenas o final, aumentamos o tamanho do pré-buffer:

```python
# Tamanho do pre-buffer aumentado para 2 segundos de áudio para melhor captura
# Isto é importante para casos onde o sistema detecta o fim da fala sem ter detectado o início
pre_buffer_limit = 100  # 2 segundos (100 frames de 20ms)
```

Isto permite que, mesmo quando o Azure Speech falha em detectar o início da fala mas captura o fim, ainda tenhamos os dados de áudio necessários para processamento.

### 4. Proteção Anti-Loop e Anti-Eco

Para evitar que o sistema entre em loop ao detectar sua própria saída de áudio, implementamos várias camadas de proteção:

1. **Períodos de Guarda Após IA Falar**:

```python
# Se a detecção ocorrer menos de 1.5 segundos após o último reset, ignoramos
ANTI_ECHO_GUARD_PERIOD = 1.5  # segundos
if time_since_last_reset < ANTI_ECHO_GUARD_PERIOD:
    logger.warning(f"[{self.call_id}] IGNORANDO detecção de fala por estar muito próxima ao reset do sistema "
                 f"({time_since_last_reset:.2f}s < {ANTI_ECHO_GUARD_PERIOD}s)")
    return  # Simplesmente ignoramos esta detecção
```

2. **Limpeza de Buffer Após IA Falar**:

```python
# PROTEÇÃO ANTI-ECO ADICIONAL: Limpar quaisquer dados coletados durante o período
# em que a IA estava falando - isso evita processamento de eco
if 'speech_callbacks' in locals() or hasattr(session, 'speech_callbacks'):
    speech_callbacks_obj = speech_callbacks if 'speech_callbacks' in locals() else session.speech_callbacks
    
    if hasattr(speech_callbacks_obj, 'audio_buffer'):
        buffer_size = len(speech_callbacks_obj.audio_buffer)
        if buffer_size > 0:
            logger.info(f"[{call_id}] Limpando buffer de {buffer_size} frames coletados durante fala da IA")
            speech_callbacks_obj.audio_buffer = []
```

## Parâmetros Ajustáveis e Configuração

Vários parâmetros podem ser ajustados para otimizar a detecção de voz para diferentes ambientes:

1. **ANTI_ECHO_GUARD_PERIOD**: Tempo em segundos para ignorar eventos de fala após a IA falar (padrão: 1.5s)
2. **ENERGY_THRESHOLD**: Limite de energia para considerar um áudio como voz real (padrão: 600 para transcrição, 800 para validação do fim de fala)
3. **MINIMUM_VALID_FRAMES**: Número mínimo de frames para considerar uma fala válida (padrão: 15 frames ou ~300ms)
4. **pre_buffer_limit**: Tamanho do buffer de pré-captura para garantir que não perdemos o início da fala (padrão: 100 frames ou 2 segundos)

Estes parâmetros podem ser ajustados com base nas características do ambiente:

- Em ambientes mais silenciosos, os thresholds de energia podem ser reduzidos
- Em ambientes ruidosos, os thresholds de energia podem ser aumentados
- Em ambientes com eco significativo, o ANTI_ECHO_GUARD_PERIOD pode ser aumentado

## Resultados e Benefícios

As melhorias implementadas resultaram em:

1. **Redução drástica de falsos positivos** na detecção de fim de fala
2. **Eliminação de loops de processamento** causados por detecção de eco
3. **Melhor qualidade de transcrição** ao ignorar áudio com pouca energia ou muito curto
4. **Sistema mais robusto** em ambientes desafiadores com ruído ou eco

## Logs e Diagnóstico

O sistema agora gera logs detalhados que facilitam o diagnóstico de problemas:

```
INFO:[call_id] Atualizado timestamp de fim de fala: <timestamp>
WARNING:[call_id] IGNORANDO fim de fala - energia muito baixa no pre-buffer (<energy> < <threshold>)
INFO:[call_id] Fim de fala CONFIRMADO por energia do áudio (<energy> > <threshold>)
WARNING:[call_id] Buffer muito pequeno para processamento (<buffer_size> < <min_frames> frames) - descartando evento
```

Estes logs permitem identificar padrões e ajustar os parâmetros conforme necessário.

## Limitações e Trabalho Futuro

Embora as melhorias implementadas tenham resolvido os principais problemas, algumas limitações permanecem:

1. Os thresholds de energia são fixos e podem não ser ideais para todos os ambientes
2. O sistema ainda depende da precisão do Azure Speech SDK para detecção inicial
3. Áudios muito curtos (como "sim" ou "não") podem ser incorretamente classificados como ruído

Possíveis melhorias futuras incluem:

1. Implementação de aprendizado adaptativo para ajustar thresholds com base no ambiente
2. Combinação de múltiplos métodos de detecção para maior precisão
3. Otimização específica para reconhecimento de comandos curtos

## Conclusão

A implementação avançada de detecção de voz com Azure Speech permite um sistema muito mais robusto e confiável para o AudioSocket-Simple, especialmente em ambientes desafiadores. As múltiplas camadas de proteção contra falsos positivos e detecção de eco garantem uma experiência de conversação mais natural e fluida para os usuários.

---

*Próximo documento: [12-desenvolvimento-futuro.md](12-desenvolvimento-futuro.md)*

docs/09-testes.md:
# Testes e Verificação do Sistema

Este documento descreve as metodologias, ferramentas e abordagens para testar o sistema AudioSocket-Simple em diferentes cenários.

## Ambientes de Teste

### 1. Ambiente de Desenvolvimento Local

Testes locais sem dependência de sistemas externos (Asterisk, RabbitMQ):

```bash
# Terminal 1: Servidor principal
python main.py

# Terminal 2: Cliente de teste simulando visitante
python microfone_client.py

# Terminal 3 (opcional): Cliente simulando morador
python microfone_client.py --port 8081
```

### 2. Ambiente de Integração

Testes com integração parcial:
- AudioSocket-Simple conectado a Asterisk de teste
- Sistema de portaria real ou simulado
- Azure Speech Services real

### 3. Ambiente de Produção

Testes completos em ambiente de produção com monitoramento detalhado.

## Ferramentas de Teste

### 1. Cliente de Microfone

O `microfone_client.py` permite simular chamadas sem necessidade do Asterisk:

```bash
# Simular visitante
python microfone_client.py --host 127.0.0.1 --port 8080

# Simular morador 
python microfone_client.py --host 127.0.0.1 --port 8081
```

### 2. API de Testes

A API HTTP na porta 8082 oferece endpoints para testes e controle:

```bash
# Verificar status do sistema
curl http://localhost:8082/api/status

# Testar encerramento de chamadas
curl -X POST -H "Content-Type: application/json" \
  -d '{"call_id":"UUID-DA-CHAMADA", "role":"visitor"}' \
  http://localhost:8082/api/hangup
```

### 3. Logs Detalhados

O sistema mantém logs detalhados para cada chamada:

```bash
# Logs gerais
tail -f logs/audiosocket.log

# Logs específicos de chamada (quando criados)
tail -f logs/UUID-DA-CHAMADA.log
```

## Cenários de Teste

### 1. Fluxo Básico de Visitante

Teste do fluxo completo onde visitante é autorizado:

1. Visitante liga
2. Informa nome, apartamento e morador
3. Sistema conecta com morador
4. Morador autoriza
5. Visitante recebe autorização
6. Chamada encerrada corretamente

### 2. Fluxo de Negação

Teste onde morador nega entrada:

1. Visitante liga
2. Informa nome, apartamento e morador
3. Sistema conecta com morador
4. Morador nega entrada
5. Visitante recebe negação
6. Chamada encerrada corretamente

### 3. Testes de Validação Fuzzy

Teste de reconhecimento aproximado de nomes:

```
Visitante: "Apartamento 501, para Daniel" 
# Sistema deve reconhecer como "Daniel dos Reis"

Visitante: "Apartamento 501, para o Daner"
# Sistema deve reconhecer como "Daniel dos Reis" via fuzzy
```

### 4. Teste de Encerramento Ativo

Teste do mecanismo KIND_HANGUP:

```
# Via API
curl -X POST -H "Content-Type: application/json" \
  -d '{"call_id":"UUID-DA-CHAMADA", "role":"visitor"}' \
  http://localhost:8082/api/hangup

# Via teste especial em conversa
Visitante: "test hangup"
```

### 5. Teste de Desconexão Abrupta

Para testar a recuperação de desconexões abruptas:

1. Iniciar chamada normal
2. Desconectar cliente abruptamente (Ctrl+C)
3. Verificar logs para confirmar tratamento adequado
4. Confirmar que nova chamada pode ser atendida na mesma porta

### 6. Teste de Carregamento

Para testar comportamento sob carga:

1. Iniciar múltiplas chamadas simultâneas
2. Monitorar uso de CPU e memória
3. Verificar throttling adaptativo
4. Verificar que cada chamada é processada corretamente

## Teste da Síntese e Transcrição

### Teste de Síntese

```bash
# Criar arquivo de texto com frases para teste
echo "Olá, como posso ajudar?" > test_phrases.txt
echo "Entendi, você quer entrar no apartamento 501" >> test_phrases.txt

# Script para testar síntese
python -c "
import asyncio
from speech_service import sintetizar_fala_async

async def test_synthesis():
    with open('test_phrases.txt', 'r') as f:
        for line in f:
            start = time.time()
            audio = await sintetizar_fala_async(line.strip())
            duration = (time.time() - start) * 1000
            print(f'Sintetizado em {duration:.2f}ms: {line.strip()}')

asyncio.run(test_synthesis())
"
```

### Teste de Transcrição

```bash
# Usar o cliente de microfone para gravar e transcrever em tempo real
python microfone_client.py --transcribe-only
```

## Testes de Componentes Específicos

### 1. Teste de ResourceManager

```python
# Teste de throttling
resource_manager.register_session("test1", 8080)
resource_manager.register_session("test2", 8080)
resource_manager.register_session("test3", 8080)
resource_manager.register_session("test4", 8080)

# Deve retornar True quando há muitas sessões e CPU alta
should_throttle = resource_manager.should_throttle_audio()
```

### 2. Teste de SessionManager

```python
# Criar sessão e enfileirar mensagens
session_id = "test-session"
session_manager.create_session(session_id)
session_manager.enfileirar_visitor(session_id, "Mensagem de teste")

# Verificar mensagem enfileirada
message = session_manager.get_message_for_visitor(session_id)
assert message == "Mensagem de teste"
```

### 3. Teste de ConversationFlow

```python
# Simular mensagem de visitante
flow = ConversationFlow()
flow.on_visitor_message(session_id, "Eu gostaria de fazer uma entrega para o apartamento 501", session_manager)

# Verificar extração de intenção
assert flow.intent_data.get("intent_type") == "entrega"
```

## Verificações Automáticas

Durante a execução de testes, observe os seguintes indicadores:

1. **Logs sem Erros**: Não deve haver erros não tratados
2. **Uso de Recursos**: CPU e memória devem permanecer em níveis aceitáveis
3. **Tempos de Resposta**: Respostas dentro dos limites esperados (veja [08-otimizacoes.md](08-otimizacoes.md))
4. **Recuperação de Erros**: Sistema deve se recuperar de falhas sem intervenção

## Documentação de Resultados de Teste

Registre os resultados dos testes:

```
Data: 01/05/2025
Teste: Fluxo completo com autorização
Resultado: SUCESSO
Observações: 
- Tempo total do fluxo: 32 segundos
- Nenhum erro observado
- Mensagens de despedida audíveis antes de KIND_HANGUP
```

## Monitoramento em Produção

Recomendações para monitoramento contínuo:

1. **Logs Centralizados**: Consolidar logs para análise
2. **Métricas de Performance**: Registrar tempos de resposta, taxas de sucesso/falha
3. **Alertas**: Configurar alertas para comportamentos anômalos
4. **Revisão Periódica**: Analisar logs e métricas regularmente para identificar melhorias

---

*Próximo documento: [10-desenvolvimento-futuro.md](10-desenvolvimento-futuro.md)*

docs/08-otimizacoes.md:
# Otimizações e Melhorias de Performance

Este documento detalha as otimizações implementadas no sistema AudioSocket-Simple para melhorar seu desempenho, reduzir latência e proporcionar uma experiência mais fluida aos usuários.

## Visão Geral das Otimizações

O sistema passou por várias otimizações para melhorar a performance em diferentes áreas:

1. **Processamento de Áudio**: Ajustes nos parâmetros de detecção de voz e transmissão
2. **Síntese de Voz**: Implementação de cache para mensagens comuns
3. **Inferência de IA**: Migração para serviços mais rápidos
4. **Gestão de Recursos**: Controle de concorrência e throttling adaptativo
5. **Gerenciamento de Memória**: Otimizações para evitar vazamentos de recursos

## Parâmetros Configuráveis

Os seguintes parâmetros foram ajustados e tornados configuráveis via `config.json`:

```json
{
    "system": {
        "silence_threshold_seconds": 1.5
    },
    "audio": {
        "transmission_delay_ms": 10,
        "post_audio_delay_seconds": 0.3,
        "discard_buffer_frames": 15
    }
}
```

### Valores Otimizados

| Parâmetro | Valor Anterior | Valor Atual | Impacto |
|-----------|----------------|-------------|---------|
| `silence_threshold_seconds` | 2.0s | 1.5s | Detecção mais rápida do fim da fala |
| `transmission_delay_ms` | 20ms | 10ms | Transmissão de áudio mais fluida |
| `post_audio_delay_seconds` | 0.5s | 0.3s | Retorno mais rápido ao modo de escuta |
| `discard_buffer_frames` | 25 | 15 | Menos frames descartados após IA falar |

## Sistema de Cache para Síntese de Voz

Uma das otimizações mais significativas foi a implementação de um sistema de cache para mensagens sintetizadas:

```python
# Verificar cache antes de sintetizar
hash_texto = hashlib.md5(texto.encode('utf-8')).hexdigest()
cache_path = os.path.join(CACHE_DIR, f"{hash_texto}.slin")

# Se já existe no cache, retornar o arquivo de áudio
if os.path.exists(cache_path):
    with open(cache_path, 'rb') as f:
        return f.read()
```

### Benefícios do Cache

- **Redução Drástica de Latência**: De 652-972ms por síntese para <1ms em frases cacheadas
- **Economia de Recursos**: Menor uso de API e CPU
- **Menor Carga em Azure**: Menos chamadas à API externa
- **Pré-carregamento Estratégico**: Frases comuns são pré-sintetizadas ao iniciar o sistema

## Migração para Modelos de IA Mais Rápidos (Groq)

A troca da API da OpenAI para Groq trouxe melhorias significativas:

| Etapa de processamento | Antes (OpenAI) | Depois (Groq) | Melhoria |
|------------------------|----------------|---------------|----------|
| Intent type | 3600ms | 889ms | 75% mais rápido |
| Nome do interlocutor | 4285ms | 773-1307ms | 81% mais rápido |
| Apartment/resident | 1788-2763ms | 1207-1318ms | 52% mais rápido |
| Total AI | 3974-7900ms | 1213-2636ms | 67% mais rápido |

## Gestão de Recursos e Throttling Adaptativo

O sistema implementa um `ResourceManager` que monitora a utilização de recursos e ajusta dinamicamente o comportamento:

```python
def should_throttle_audio(self):
    """
    Determina se a transmissão de áudio deve ser limitada com base na carga do sistema.
    Retorna True se o sistema estiver sobrecarregado.
    """
    system_load = self.get_system_load()
    cpu_percent = system_load.get('cpu_percent', 0)
    active_sessions = system_load.get('active_sessions', 0)
    
    # Se temos muitas sessões ativas E a CPU está alta, ativamos throttling
    return active_sessions > 3 and cpu_percent > 85
```

Quando o sistema está sobrecarregado, ajustes automáticos são aplicados:

```python
# Verificar se precisamos aplicar throttling baseado na carga do sistema
should_throttle = resource_manager.should_throttle_audio()
transmission_delay = TRANSMISSION_DELAY_MS * 1.5 if should_throttle else TRANSMISSION_DELAY_MS
```

## Semáforos para Limitar Processamento Concorrente

Para evitar sobrecarga em operações intensivas, o sistema usa semáforos:

```python
# Limite de simultaneidade
self.max_concurrent_transcriptions = int(os.getenv('MAX_CONCURRENT_TRANSCRIPTIONS', '3'))
self.max_concurrent_synthesis = int(os.getenv('MAX_CONCURRENT_SYNTHESIS', '3'))

# Semáforos para controle de acesso
self.transcription_semaphore = asyncio.Semaphore(self.max_concurrent_transcriptions)
self.synthesis_semaphore = asyncio.Semaphore(self.max_concurrent_synthesis)

async def acquire_transcription_lock(self, session_id: str):
    """Adquire um lock para transcrição"""
    await self.transcription_semaphore.acquire()
    self.set_transcribing(session_id, True)
    return True
```

## Otimizações de Socket

Os servidores de socket foram configurados para garantir melhor desempenho:

```python
# Servidor com parâmetros otimizados
server = await asyncio.start_server(
    handler,
    binding_ip,
    porta,
    limit=1024*1024,  # 1MB buffer
    start_serving=True
)
```

## Timeouts e Tratamento de Erros

Todos os timeouts são configuráveis e têm tratamento de erro adequado:

```python
try:
    # Timeout para evitar bloqueio indefinido
    await asyncio.wait_for(writer.wait_closed(), timeout=2.0)
except asyncio.TimeoutError:
    logger.info(f"[{call_id}] Timeout ao aguardar fechamento do socket")
```

## Resultados de Performance

Comparação detalhada entre a versão anterior e a nova versão otimizada:

### 1. Síntese de Voz (Caching)
- **Antes**: 652-972ms por síntese
- **Agora**: Maioria abaixo de 1ms, exceto por sínteses novas
- **Melhoria**: ~99% para frases comuns cacheadas

### 2. Transmissão de Áudio
- **Antes**: 3765-5144ms (média ~4260ms)
- **Agora**: 1535-2656ms (média ~2198ms)
- **Melhoria**: ~48% mais rápido

### 3. Tempo Total de Interação
- Tempo médio para processar cada entrada do usuário:
  - **Antes**: ~14-23 segundos
  - **Agora**: ~9-16 segundos
  - **Melhoria**: ~40% mais rápido

## Próximos Passos Recomendados

Para melhorias futuras de performance, recomendamos:

1. **Paralelização do processamento AI**:
   - Executar tarefas de extração de intenção simultaneamente usando `asyncio.gather()`
   - Atualmente são executadas sequencialmente (tipo, nome, apartamento)

2. **Otimização da transcrição**:
   - Implementar transcrição em streaming para processamento contínuo
   - Avaliar opções para usar modelo de transcrição mais leve quando possível

3. **Monitoramento contínuo**:
   - Adicionar dashboards para análise de desempenho em tempo real
   - Identificar e otimizar outros gargalos conforme uso aumenta

## Configuração Baseada em Hardware

O sistema pode se adaptar ao hardware disponível:

```python
def _configure_based_on_hardware(self):
    """Configura limites baseados nos recursos do hardware."""
    try:
        cpu_count = psutil.cpu_count(logical=False) or 2
        mem_gb = psutil.virtual_memory().total / (1024**3)
        
        # Ajustar limites com base em CPU e memória
        if cpu_count >= 4 and mem_gb >= 8:
            # Hardware robusto
            self.max_concurrent_transcriptions = max(3, min(cpu_count - 1, 6))
            self.max_concurrent_synthesis = max(3, min(cpu_count - 1, 6))
        elif cpu_count >= 2 and mem_gb >= 4:
            # Hardware médio
            self.max_concurrent_transcriptions = 2
            self.max_concurrent_synthesis = 2
        else:
            # Hardware limitado
            self.max_concurrent_transcriptions = 1
            self.max_concurrent_synthesis = 1
    except Exception as e:
        logger.warning(f"Erro ao configurar baseado no hardware: {e}. Usando valores padrão.")
```

## Conclusão

As otimizações implementadas reduziram significativamente o tempo de resposta do sistema, criando uma experiência conversacional mais natural e fluida. A combinação de cache de síntese, redução de atrasos e a troca para Groq proporcionou uma melhoria geral de aproximadamente 40% no tempo de interação.

---

*Próximo documento: [09-testes.md](09-testes.md)*

docs/06-encerramento-chamadas.md:
# Encerramento de Chamadas

Este documento detalha o mecanismo de encerramento ativo de chamadas implementado no sistema AudioSocket-Simple, incluindo o uso do comando KIND_HANGUP e o tratamento de erros relacionados.

## Visão Geral

O encerramento adequado das chamadas é um aspecto crítico do sistema. Ele envolve:

1. A notificação aos participantes da conversa
2. O envio controlado do comando KIND_HANGUP para encerrar o protocolo AudioSocket
3. A liberação eficiente de recursos do sistema
4. O tratamento adequado de desconexões abruptas

## Protocolo AudioSocket e KIND_HANGUP

O protocolo AudioSocket do Asterisk define diferentes tipos de pacotes, incluindo:

- **KIND_ID (0x01)**: Identificação da chamada
- **KIND_SLIN (0x10)**: Dados de áudio no formato SLIN
- **KIND_HANGUP (0x00)**: Sinalização de encerramento da chamada

O pacote KIND_HANGUP tem esta estrutura:
```
┌────────┬──────────┬────────┐
│ 0x00   │ 00 00    │        │
├────────┼──────────┼────────┤
│ 1 byte │ 2 bytes  │ 0 bytes│
│ Tipo   │ Tamanho  │ Dados  │
└────────┴──────────┴────────┘
```

## Mecanismos de Encerramento Implementados

### 1. Encerramento via ConversationFlow

O método `_schedule_active_hangup()` no `ConversationFlow` implementa o encerramento ativo:

```python
def _schedule_active_hangup(self, session_id: str, session_manager, delay=5.0):
    """
    Agenda o envio de KIND_HANGUP ativo após um delay para encerrar a chamada.
    O delay permite que todas as mensagens de áudio sejam reproduzidas primeiro.
    """
    async def send_hangup_after_delay():
        # Aguardar o delay para permitir que as mensagens sejam enviadas
        await asyncio.sleep(delay)
        
        # Verificar se a sessão ainda existe
        session = session_manager.get_session(session_id)
        if not session:
            return
            
        try:
            # Importar ResourceManager para acessar conexões ativas
            from extensions.resource_manager import resource_manager
            import struct
            
            # Enviar KIND_HANGUP para o visitante e morador
            visitor_conn = resource_manager.get_active_connection(session_id, "visitor")
            if visitor_conn and 'writer' in visitor_conn:
                visitor_conn['writer'].write(struct.pack('>B H', 0x00, 0))
                await visitor_conn['writer'].drain()
            
            # Após enviar os KIND_HANGUP, aguardar e finalizar a sessão
            await asyncio.sleep(1.0)
            session_manager.end_session(session_id)
        except Exception as e:
            logger.error(f"[Flow] Erro ao enviar KIND_HANGUP ativo: {e}")
```

### 2. Encerramento via API HTTP

O endpoint `/api/hangup` permite encerrar chamadas através da API REST:

```python
async def hangup_call(self, request: web.Request) -> web.Response:
    """
    Envia sinal de hangup (KIND_HANGUP, 0x00) para uma chamada ativa.
    
    URL: POST /api/hangup
    Body: {"call_id": "uuid-da-chamada", "role": "visitor|resident"}
    """
    # Obter conexão ativa da sessão
    connection = resource_manager.get_active_connection(call_id, role)
    
    # Enviar KIND_HANGUP (0x00)
    writer.write(struct.pack('>B H', 0x00, 0))
    await writer.drain()
    
    # Agendar limpeza completa da sessão após delay
    asyncio.create_task(self._cleanup_session_after_delay(call_id, session_manager))
```

## Tratamento de Erros de Conexão

Uma característica importante é o tratamento adequado de erros de conexão, especialmente `ConnectionResetError` que ocorre quando o cliente desconecta abruptamente:

```python
# Tratar fechamento do socket com robustez
try:
    writer.close()
    # Usar um timeout para wait_closed
    await asyncio.wait_for(writer.wait_closed(), timeout=2.0)
except asyncio.TimeoutError:
    logger.info(f"[{call_id}] Timeout ao aguardar fechamento do socket")
except ConnectionResetError:
    # Isso é esperado se o cliente desconectar abruptamente
    logger.info(f"[{call_id}] Conexão resetada pelo cliente - comportamento normal")
except Exception as e:
    # Capturar qualquer outro erro
    logger.warning(f"[{call_id}] Erro ao fechar conexão: {str(e)}")
```

## Fluxo de Encerramento Completo

O encerramento completo de uma chamada segue esta sequência:

1. **Preparação para Encerramento**:
   - O sistema decide encerrar a conversa (por autorização, negação ou timeout)
   - O método `_finalizar()` é chamado no `ConversationFlow`

2. **Envio de Mensagens de Despedida**:
   - Mensagens de despedida são enfileiradas para visitante e morador
   - As mensagens são enviadas, sintetizadas e reproduzidas

3. **Agendamento de KIND_HANGUP**:
   - Um delay é aplicado (5s padrão) para permitir que as mensagens sejam ouvidas
   - Após o delay, o sistema envia KIND_HANGUP para as conexões ativas

4. **Tratamento de Desconexão**:
   - O sistema trata adequadamente desconexões abruptas
   - Registra o ocorrido em logs informativos (não como erros)

5. **Liberação de Recursos**:
   - A sessão é removida do `SessionManager`
   - Conexões são removidas do `ResourceManager`
   - Logs são limpos e recursos liberados

## Testes e Verificação

### Via API

O endpoint de API permite testar facilmente o encerramento:

```bash
# Obter a lista de sessões ativas para identificar o call_id
curl http://localhost:8082/api/status

# Encerrar uma chamada específica
curl -X POST -H "Content-Type: application/json" \
  -d '{"call_id":"UUID-DA-CHAMADA", "role":"visitor"}' \
  http://localhost:8082/api/hangup
```

### Via Conversa

Durante o desenvolvimento, você pode testar usando um comando especial:

```
Visitante: test hangup
IA: Teste de KIND_HANGUP ativado. Ao finalizar, o sistema enviará ativamente o comando de desconexão.
```

## Logs e Monitoramento

Os logs de encerramento fornecem informações detalhadas sobre o processo:

```
INFO:[99203132-1abf-4309-a05e-d7c6624c74af] Enviando KIND_HANGUP ativo para visitante na sessão
INFO:[99203132-1abf-4309-a05e-d7c6624c74af] Conexão resetada pelo cliente após KIND_HANGUP - comportamento normal
INFO:[99203132-1abf-4309-a05e-d7c6624c74af] Socket encerrado e liberado para novas conexões
```

## Benefícios da Implementação

1. **Controle do Ciclo de Vida**: O sistema controla o encerramento, não dependendo do cliente
2. **Experiência Melhorada**: As mensagens de despedida são ouvidas antes do encerramento
3. **Robustez**: Tratamento adequado de erros de conexão
4. **Eficiência**: Liberação adequada de recursos
5. **Observabilidade**: Logs informativos para diagnóstico e monitoramento

## Considerações para o Futuro

1. **Timeout Automático**: Implementar encerramento automático após inatividade prolongada
2. **Detecção de Problemas**: Encerrar chamadas automaticamente em caso de problemas persistentes
3. **Métricas de Duração**: Coletar estatísticas sobre a duração média das chamadas

---

*Próximo documento: [07-processamento-ai.md](07-processamento-ai.md)*

docs/azure_speech_optimization.md:
# Otimização do Reconhecimento de Voz Azure Speech

## Contexto e Problema

O sistema AudioSocket-Simple utiliza o Azure Speech SDK para reconhecimento de voz em chamadas VoIP. Embora o sistema detectasse corretamente quando o usuário começava e parava de falar (eventos `speech_start_detected` e `speech_end_detected`), o evento crucial `recognized` não estava sendo disparado, impedindo o processamento do texto transcrito.

## Diagnóstico

O áudio estava sendo capturado corretamente (comprovado pelos arquivos .slin gerados), mas o Azure Speech não estava retornando transcrições. Nossas hipóteses para o problema incluíam:

1. **Formato de áudio incompatível**: O formato SLIN (PCM 16-bit 8kHz) não estava sendo configurado corretamente para o Azure Speech SDK.
2. **Configurações inadequadas de segmentação**: Parâmetros como timeout de silêncio podem afetar quando o SDK considera uma frase completa.
3. **Problemas de conectividade**: Falhas na conexão com a API do Azure ou credenciais incorretas.
4. **Casos de NoMatch**: O Azure detectava fala mas não conseguia transcrevê-la, resultando em um evento `NoMatch` que não estava sendo monitorado.

## Otimizações Implementadas

### 1. Correção do Formato de Áudio

- Configuração explícita do formato como 8kHz 16-bit mono para garantir que o Azure Speech interpretasse corretamente o áudio SLIN:
  ```python
  audio_format = speechsdk.audio.AudioStreamFormat(
      samples_per_second=8000,  # Crucialmente importante: 8kHz para SLIN
      bits_per_sample=16,       # 16-bit PCM
      channels=1                # mono
  )
  ```

### 2. Tratamento Explícito de NoMatch e Cancelamentos

- Adicionado tratamento para o resultado `NoMatch` no callback `on_recognized`
- Implementado log detalhado no evento de cancelamento (`on_canceled`) para identificar erros específicos

### 3. Mecanismo de Timeout para Processamento de Áudio

- Implementado um fallback que processa o áudio mesmo sem transcrição:
  ```python
  async def check_recognition_timeout():
      await asyncio.sleep(3.0)  # Esperar 3 segundos
      
      # Se ainda temos o mesmo áudio no buffer, o evento on_recognized não foi disparado
      if self.collecting_audio == False and len(self.audio_buffer) > 0:
          logger.warning(f"[{self.call_id}] Timeout: on_recognized não foi disparado após 3s. Processando áudio diretamente.")
          # ... processamento do áudio ...
  ```

### 4. Aprimoramento da Coleta de Áudio

- Correção na lógica de `add_audio_chunk` para garantir que o áudio seja adicionado ao buffer sempre que fala for detectada
- Melhorada a limpeza do buffer para evitar mistura de áudios de diferentes eventos

### 5. Configurações Otimizadas de Reconhecimento

- Ajustados parâmetros de segmentação de silêncio para melhor desempenho com áudio VoIP
- Forçado o idioma de reconhecimento como Português Brasileiro (pt-BR)
- Desativado o pós-processamento de texto para resultados mais brutos e rápidos

### 6. Ferramentas de Diagnóstico

- Script `diagnose_azure_speech.py`: Testa a conectividade e funcionamento do SDK
- Script `improve_recognizer.py`: Aplica otimizações sem edição manual do código
- Ferramenta `play_slin.py`: Permite reproduzir e analisar os arquivos de áudio capturados

### 7. Logging Aprimorado

- Implementados logs detalhados para cada estágio do processo
- Salvamento automático de arquivos de áudio para diagnóstico
- Rastreamento da duração dos eventos de fala

## Resultados e Conclusões

O foco principal foi garantir que o sistema pudesse funcionar de forma confiável:

1. **Confiando no Azure quando possível**: Utilizando a detecção nativa de início/fim de fala
2. **Sendo resiliente a falhas**: Processando áudio mesmo quando o reconhecimento falha
3. **Mantendo diagnóstico detalhado**: Gerando logs e arquivos para análise posterior

Apesar de ainda haver desafios com o reconhecimento em ambientes ruidosos, as melhorias implementadas permitem que o sistema funcione de maneira mais robusta, superando falhas no reconhecimento de fala do Azure Speech sem sacrificar a qualidade geral do serviço.

## Próximos Passos

1. Monitorar logs de produção para identificar padrões nas falhas de reconhecimento
2. Considerar ajustes futuros nos parâmetros de timeout e segmentação com base em métricas reais
3. Explorar opções para pré-processamento de áudio (redução de ruído, normalização) antes do envio para o Azure

## Referências Técnicas

- [Documentação Azure Speech SDK](https://learn.microsoft.com/en-us/azure/cognitive-services/speech-service/speech-sdk)
- [Configurações de formato de áudio](https://learn.microsoft.com/en-us/azure/cognitive-services/speech-service/how-to-use-audio-input-streams)
- [Timeouts de silêncio e segmentação](https://learn.microsoft.com/en-us/azure/cognitive-services/speech-service/how-to-recognize-speech?pivots=programming-language-python)

docs/07-processamento-ai.md:
# Processamento de IA e Extração de Intenções

Este documento detalha como o sistema AudioSocket-Simple processa as mensagens do usuário utilizando IA para extrair intenções e informações estruturadas.

## Visão Geral

O sistema utiliza um pipeline de processamento de linguagem natural para extrair progressivamente informações do visitante. Este pipeline é construído usando o framework CrewAI com LLMs (Large Language Models) para processar cada etapa da extração de informações.

## Arquitetura do Pipeline de IA

O sistema divide o processamento de intenções em etapas sequenciais:

1. **Extração do Tipo de Intenção**: Identificar se é uma visita, entrega ou serviço
2. **Extração do Nome do Visitante**: Capturar o nome de quem está na portaria
3. **Extração de Apartamento e Morador**: Identificar número do apartamento e nome do morador
4. **Validação via Fuzzy Matching**: Validar as informações contra um banco de dados

Esta abordagem em etapas permite:
- Maior precisão em cada extração
- Mensagens contextuais específicas para cada informação faltante
- Controle granular da conversa

## Componentes Principais

### 1. Gerenciamento de Estado

O sistema mantém o estado da conversa entre mensagens:

```python
def get_user_state(id: str):
    """Obtém o estado atual da conversa com o usuário."""
    
def update_user_state(id: str, intent=None, message=None):
    """Atualiza o estado da conversa com novos dados de intenção e histórico."""
```

### 2. Extração de Intenções

A função principal que orquestra o processo de extração:

```python
def process_user_message_with_coordinator(id: str, message: str) -> dict:
    """
    Processa a mensagem do usuário para extrair intenções e dados estruturados.
    Retorna um dicionário com a resposta e os dados extraídos.
    """
    # Etapas progressivas de extração...
```

### 3. Tarefas Especializadas

Para cada tipo de informação, uma tarefa especializada é criada:

```python
# Extração de tipo de intenção (entrega, visita, etc.)
task = conversation_extractor_intent_task(
    user_message=message,
    conversation_history=history,
    intent=partial_intent
)

# Extração de nome do visitante
task = conversation_extractor_name_task(
    user_message=message,
    conversation_history=history,
    intent=partial_intent
)

# Extração de apartamento e morador
task = conversation_extractor_resident_apartment_task(
    user_message=message,
    conversation_history=history,
    intent=partial_intent
)
```

### 4. Validação via Fuzzy Matching

Após coletar as informações, o sistema valida-as usando fuzzy matching:

```python
def validar_intent_com_fuzzy(intent: Dict) -> Dict:
    """
    Verifica se a combinação apartment_number e resident_name da intent
    corresponde (mesmo que parcialmente) a um morador real.
    """
    # Processar com fuzzy matching...
```

O sistema calcula a similaridade entre as informações fornecidas e os dados reais, utilizando diferentes algoritmos de comparação fuzzy:

```python
# Pontuações para diferentes algoritmos de match
scores = [
    fuzz.ratio(resident_informado, nome_residente),  # Match completo
    fuzz.partial_ratio(resident_informado, nome_residente),  # Match parcial
    fuzz.token_sort_ratio(resident_informado, nome_residente)  # Ignora ordem das palavras
]
```

## Fluxo de Execução Detalhado

### 1. Processamento Inicial

Quando o visitante envia uma mensagem, o processo começa:

```python
texto = await transcrever_audio_async(audio_data, call_id=call_id)
session_manager.process_visitor_text(call_id, texto)
```

### 2. Encaminhamento para o ConversationFlow

```python
def process_visitor_text(self, session_id: str, text: str):
    """Processa o texto do visitante via ConversationFlow."""
    session = self.get_session(session_id)
    if not session:
        session = self.create_session(session_id)
    
    # Registrar no histórico
    session.history.append(f"[Visitor] {text}")
    
    # Repassar para o ConversationFlow
    session.flow.on_visitor_message(session_id, text, self)
```

### 3. Extração Progressiva de Intenções

```python
def on_visitor_message(self, session_id: str, text: str, session_manager):
    """Processa a mensagem do visitante no ConversationFlow."""
    if self.state == FlowState.COLETANDO_DADOS:
        # Processar com IA para extrair intenções
        result = process_user_message_with_coordinator(session_id, text)
        
        # Analisar resultado e verificar completude
        if result.get("valid_for_action"):
            # Validação fuzzy dos dados
            fuzzy_res = validar_intent_com_fuzzy(self.intent_data)
            
            if fuzzy_res["status"] == "válido":
                self.state = FlowState.VALIDADO
                # Avançar para próxima etapa...
```

### 4. Extração em Etapas

O sistema extrai as informações em etapas sequenciais:

1. **Intent Type**:
   - Se não existe intenção ou intent_type está vazio
   - Executa `conversation_extractor_intent_task`
   - Atualiza o estado com o tipo de intenção (visita, entrega, etc.)

2. **Nome do Visitante**:
   - Se intent_type está preenchido mas interlocutor_name está vazio
   - Executa `conversation_extractor_name_task`
   - Atualiza o estado com o nome do visitante

3. **Apartamento e Morador**:
   - Se interlocutor_name está preenchido mas apartment_number ou resident_name estão vazios
   - Executa `conversation_extractor_resident_apartment_task`
   - Atualiza o estado com o número do apartamento e nome do morador

### 5. Validação Fuzzy

```python
def validar_intent_com_fuzzy(intent: Dict) -> Dict:
    apt = intent.get("apartment_number", "").strip().lower()
    resident_informado = intent.get("resident_name", "").strip().lower()
    
    # Verificar se apartamento existe
    apt_matches = [a for a in apartamentos if a["apartment_number"] == apt]
    
    # Procurar melhor match de residente
    for apartamento in apt_matches:
        for residente in apartamento["residents"]:
            nome_residente = residente.strip().lower()
            
            # Calcular scores com diferentes algoritmos...
            score = max(scores)
            
            if score > best_score:
                best_score = score
                best_match = residente
                best_apt = apartamento
    
    # Se score é suficiente, validar
    if best_score >= 75:
        return {
            "status": "válido",
            "match_name": best_match,
            "voip_number": best_apt["voip_number"],
            # ...
        }
    else:
        return {
            "status": "inválido",
            "reason": "Morador não encontrado neste apartamento",
            # ...
        }
```

## Formato das Respostas

As funções de processamento de IA retornam um formato padronizado:

```json
{
    "mensagem": "Texto para responder ao usuário",
    "dados": {
        "intent_type": "entrega",
        "interlocutor_name": "Pedro da Silva",
        "apartment_number": "501",
        "resident_name": "Daniel dos Reis"
    },
    "valid_for_action": true
}
```

O campo `valid_for_action` indica se todos os dados necessários foram coletados e podem ser considerados completos para a próxima ação (como contatar o morador).

## Medição de Performance

O sistema monitora o tempo de processamento de cada etapa:

```python
start_time = time.time()
texto = await transcrever_audio_async(audio_data, call_id=call_id)
transcription_time = (time.time() - start_time) * 1000

start_time = time.time()
result = process_user_message_with_coordinator(session_id, text)
ai_processing_time = (time.time() - start_time) * 1000
```

Estas métricas ajudam a identificar gargalos e otimizar o desempenho.

## Melhorias Recentes (Groq)

O sistema foi migrado da OpenAI para Groq para melhorar o desempenho:

| Etapa de processamento | Antes (OpenAI) | Depois (Groq) | Melhoria |
|------------------------|----------------|---------------|----------|
| Intent type | 3600ms | 889ms | 75% mais rápido |
| Nome do interlocutor | 4285ms | 773-1307ms | 81% mais rápido |
| Apartment/resident | 1788-2763ms | 1207-1318ms | 52% mais rápido |
| Total AI | 3974-7900ms | 1213-2636ms | 67% mais rápido |

## Considerações para o Futuro

1. **Paralelização**: Em vez de extrair informações sequencialmente, elas poderiam ser extraídas em paralelo com `asyncio.gather()`

2. **Ajuste Fino de LLMs**: Treinamento específico para o domínio da portaria

3. **Cache de Respostas Comuns**: Armazenar respostas para perguntas frequentes

4. **Análise Contínua**: Monitoramento de acurácia e performance para identificação de áreas de melhoria

---

*Próximo documento: [08-otimizacoes.md](08-otimizacoes.md)*

docs/02-arquitetura.md:
# Arquitetura do Projeto AudioSocket-Simple

Este documento descreve em detalhes a arquitetura do sistema AudioSocket-Simple, seus componentes e como eles interagem.

## Visão Geral

O AudioSocket-Simple é um sistema de atendimento por IA para condomínios que utiliza o protocolo AudioSocket do Asterisk para gerenciar chamadas VoIP. O sistema permite automatizar o atendimento a visitantes e entregas, conectando-os com os moradores através de um assistente virtual inteligente.

## Componentes da Arquitetura

### 1. Servidores AudioSocket

O sistema mantém dois servidores AudioSocket paralelos:
- **Servidor Visitante (porta 8080)**: Atende chamadas provenientes de visitantes no portão.
- **Servidor Morador (porta 8081)**: Atende chamadas para moradores autorizados.
- **Servidor API (porta 8082)**: API HTTP para gerenciamento e testes.

### 2. Gerenciamento de Sessões

- **SessionManager**: Armazena e gerencia o estado das conversas entre visitantes e moradores.
- **SessionData**: Mantém filas de mensagens independentes para visitantes e moradores, além do histórico da conversa.
- **FlowState**: Define o estado do fluxo da conversa (COLETANDO_DADOS, VALIDADO, CHAMANDO_MORADOR, etc.).

### 3. Processamento de Áudio

- **VAD (Voice Activity Detection)**: Detecta quando o usuário começa e termina de falar.
- **Azure Speech Services**: Realiza a transcrição de fala para texto e síntese de texto para fala.
- **Sistema de Cache**: Armazena áudios sintetizados comuns para reduzir latência.

### 4. Integração com IA

- **CrewAI**: Framework utilizado para coordenar agentes especializados na extração de intenções e dados.
- **Agents**: Agentes especializados para diferentes tarefas de entendimento contextual.
- **Tasks**: Tarefas específicas para extrair informações como intenção, nome do visitante, apartamento e morador.
- **Fuzzy Matching**: Sistema de validação de dados utilizando comparação fuzzy para tolerância a erros.

### 5. Comunicação AMQP

- **RabbitMQ**: Utilizado para enviar solicitações de clicktocall para conectar com moradores.

### 6. Gestão de Recursos e Conexões

- **ResourceManager**: Gerencia recursos do sistema e conexões ativas.
- **Encerramento Ativo**: Sistema de envio de KIND_HANGUP para encerramento controlado de chamadas.

## Estados do Sistema

### Estados da StateMachine
- **STANDBY**: Estado inicial, aguardando nova chamada
- **USER_TURN**: Turno do usuário (sistema está ouvindo)
- **WAITING**: Estado intermediário de processamento
- **IA_TURN**: Turno da IA (sistema está respondendo)

### Estados do ConversationFlow
- **COLETANDO_DADOS**: Fase de extração de informações do visitante
- **VALIDADO**: Dados foram validados com sucesso
- **CHAMANDO_MORADOR**: Sistema está tentando contactar o morador
- **ESPERANDO_MORADOR**: Morador atendeu, aguardando resposta
- **FINALIZADO**: Fluxo concluído, chamada encerrada

## Fluxo de Dados

O fluxo de dados no sistema segue este caminho geral:

1. **Entrada de Áudio** → **VAD** → **Transcrição** → **Extração de Intenções** → **Validação** → **Contato com Morador** → **Processamento de Resposta** → **Síntese de Voz** → **Saída de Áudio**

### Comunicação Entre Componentes

```
┌────────────────┐       ┌─────────────────┐       ┌─────────────────┐
│                │       │                 │       │                 │
│  AudioSocket   │<─────>│ SessionManager  │<─────>│ ConversationFlow│
│                │       │                 │       │                 │
└────────────────┘       └─────────────────┘       └─────────────────┘
        ▲                        ▲                         ▲
        │                        │                         │
        ▼                        ▼                         ▼
┌────────────────┐       ┌─────────────────┐       ┌─────────────────┐
│                │       │                 │       │                 │
│ Azure Speech   │       │ ResourceManager │       │     CrewAI      │
│                │       │                 │       │                 │
└────────────────┘       └─────────────────┘       └─────────────────┘
```

## Modelo de Intenções

O sistema extrai e processa intenções estruturadas:
- **intent_type**: Tipo de intenção (visita/entrega)
- **interlocutor_name**: Nome do visitante
- **apartment_number**: Número do apartamento
- **resident_name**: Nome do morador

## Tecnologias Principais

- **Python**: Linguagem principal do projeto
- **asyncio**: Para operações assíncronas e concorrência
- **Azure Speech Services**: Para processamento de fala
- **webrtcvad**: Para detecção de atividade de voz
- **CrewAI + LLMs**: Para processamento de linguagem natural e extração de intenções
- **Socket TCP**: Para comunicação via protocolo AudioSocket
- **aiohttp**: Para API HTTP de gerenciamento
- **AMQP/RabbitMQ**: Para comunicação com sistemas externos

## Pontos Fortes da Arquitetura

1. **Design Assíncrono**: Utiliza `asyncio` para operações não bloqueantes e concorrentes.
2. **Separação de Responsabilidades**: Componentes bem definidos com funções específicas.
3. **Máquina de Estados Robusta**: Transições claras entre estados da conversa.
4. **Pipeline de IA Modular**: Extração de informações em etapas segregadas e especializadas.
5. **Filas de Mensagens**: Comunicação eficiente entre componentes através de filas assíncronas.
6. **Tratamento de Erros**: Sistema robusto de tratamento de exceções e encerramento gracioso.
7. **Cache de Performance**: Sistema de cache para melhorar desempenho de síntese de voz.

## Diagrama de Interação Durante uma Chamada

```
Visitante    AudioSocket    SessionManager    ConversationFlow    Azure      Morador
   │              │                │                │               │            │
   │───chamada───>│                │                │               │            │
   │              │───cria sessão─>│                │               │            │
   │              │<──confirma────┤                │               │            │
   │<──saudação───│                │                │               │            │
   │───fala───────>│                │                │               │            │
   │              │───transcreve──>│                │────────────────────>│            │
   │              │<──texto────────│                │<───────────────────┤            │
   │              │                │───processa────>│                │            │
   │              │                │<──resposta─────│                │            │
   │              │                │                │───valida dados>│            │
   │              │                │                │<──confirmação──│            │
   │              │                │                │────────────────────────────>│
   │              │                │                │<───────────────────────────┤
   │<──resultado──│                │                │               │            │
   │              │───encerra─────>│──────────────>│               │            │
   │─────fim─────>│                │                │               │            │
```

## Próximos Passos

Consulte os documentos seguintes para detalhes específicos sobre:
- Configuração e implantação
- Fluxos de comunicação
- Protocolos e formatos de mensagem
- Gerenciamento de chamadas múltiplas
- Testes e verificação do sistema

---

*Próximo documento: [03-configuracao.md](03-configuracao.md)*

docs/01-introducao.md:
# Introdução ao AudioSocket-Simple

Este documento fornece uma visão geral do projeto AudioSocket-Simple, um sistema de atendimento por IA para condomínios que utiliza o protocolo AudioSocket do Asterisk.

## Contexto

O AudioSocket-Simple é uma aplicação desenvolvida para automatizar o atendimento de chamadas em portarias de condomínios, permitindo:

1. Atender visitantes no portão
2. Coletar informações como nome, apartamento e motivo da visita
3. Contatar moradores para autorização
4. Gerenciar todo o fluxo conversacional entre visitante e morador

## Configuração do Ambiente de Desenvolvimento

### Pré-requisitos

- Python 3.8+ 
- Node.js v20.12.0
- Banco de dados PostgreSQL (opcional, para testes locais)
- Bibliotecas Python (webrtcvad, Azure Speech SDK, etc.)

### Preparando o Ambiente

Antes de começar a trabalhar com o código, configure o ambiente de desenvolvimento:

```bash
# Ativar o Node.js correto via NVM
nvm use v20.12.0

# Ativar o ambiente virtual Python
source /Users/danerdosreis/development/environments/audiosocket-simple/bin/activate

# Instalar dependências Python
pip install -r requirements.txt
```

## Arquitetura do Sistema

### Componentes Principais

1. **Servidores AudioSocket**
   - Socket TCP para visitantes (porta 8080)
   - Socket TCP para moradores (porta 8081)
   - API HTTP para gerenciamento e testes (porta 8082)

2. **Gerenciamento de Sessões**
   - SessionManager: armazena e gerencia o estado das conversas
   - ConversationFlow: controla o fluxo de interação e lógica de negócio

3. **Processamento de Áudio**
   - VAD (Voice Activity Detection): detecta quando o usuário fala e para
   - Transcrição de áudio via Azure Speech Services
   - Síntese de voz via Azure Speech Services

4. **Inteligência Artificial**
   - Sistema de extração de intenções usando LLMs
   - Processamento estruturado por etapas (intent, nome, apartamento)
   - Fuzzy matching para validação de dados

## Fluxo Básico de Operação

1. **Inicialização**:
   - Sistema inicia servidores na porta 8080 (visitante) e 8081 (morador)
   - Pré-sintetiza frases comuns para melhor desempenho

2. **Atendimento do Visitante**:
   - Visitante liga para o ramal do condomínio
   - Sistema responde com mensagem de boas-vindas
   - Coleta informações (nome, apartamento, motivo)
   - Valida dados informados

3. **Contato com o Morador**:
   - Sistema inicia chamada para o morador
   - Informa sobre o visitante aguardando
   - Recebe autorização ou negação

4. **Conclusão**:
   - Informa o visitante sobre a decisão do morador
   - Encerra ativamente a chamada (KIND_HANGUP)
   - Libera recursos da sessão

## Próximos Passos

Consulte os outros documentos desta série para informações detalhadas sobre:

- Configuração e implantação
- Fluxos de comunicação detalhados
- Sistema de gerenciamento de chamadas
- Tratamento de erros e encerramento de chamadas
- Otimizações e melhorias de desempenho
- Testes e verificações

---

*Próximo documento: [02-arquitetura.md](02-arquitetura.md)*

extensions/extension_manager.py:
import logging
import os
import asyncio
from typing import Dict, List, Any, Optional, Tuple

from .db_connector import DBConnector
from .config_persistence import ConfigPersistence
from .server_manager import ServerManager
from .api_server import APIServer
from .db_listener import PostgresListener

logger = logging.getLogger(__name__)

class ExtensionManager:
    """
    Classe principal que gerencia todo o sistema de extensões da IA.
    Coordena a conexão com banco de dados, persistência local e gerenciamento de servidores.
    """
    
    def __init__(self):
        self.db_connector = DBConnector()
        self.config_persistence = ConfigPersistence()
        self.server_manager = ServerManager()
        self.api_server = APIServer(
            self.server_manager,
            self.config_persistence,
            self.db_connector
        )
        self.db_listener = PostgresListener(self.handle_db_notification)
        self.is_running = False
        self.api_runner = None
        self.api_site = None
    
    async def handle_db_notification(self, payload: Dict[str, Any]):
        """
        Processa notificações recebidas do banco de dados e realiza as ações necessárias.
        
        Args:
            payload: Dicionário contendo os dados da notificação
        """
        try:
            action = payload.get('action', '').upper()  # Converter para maiúsculo para padronização
            data = payload.get('data', {})
            
            logger.info(f"Processando notificação: {action} para extensão")
            
            if action == 'INSERT':
                # Nova extensão foi adicionada
                logger.info(f"Nova extensão detectada: {data.get('extension_ia_number')}")
                
                # Converter para o formato usado pelo ServerManager
                config = {
                    'id': data.get('extension_ia_id'),
                    'ramal_ia': data.get('extension_ia_number', '').strip(),
                    'ramal_retorno': data.get('extension_ia_return', '').strip(),
                    'ip_servidor': data.get('extension_ia_ip', '').strip(),
                    'porta_ia': int(data.get('extension_ia_number_port', 0)),
                    'porta_retorno': int(data.get('extension_ia_return_port', 0)),
                    'condominio_id': data.get('condominium_id', 0)
                }
                
                # Iniciar novo servidor para esta extensão
                try:
                    await self.server_manager.start_server(config)
                    logger.info(f"Servidor para nova extensão {config['ramal_ia']} iniciado com sucesso")
                    
                    # Atualizar configurações locais
                    configs = self.config_persistence.load_configs()
                    configs.append(config)
                    self.config_persistence.save_configs(configs)
                except Exception as e:
                    logger.error(f"Erro ao iniciar servidor para nova extensão: {e}")
                    # Porta pode estar em uso ou outro erro, não adicionamos o ramal
                
            elif action == 'UPDATE':
                # Extensão foi atualizada
                extension_id = data.get('extension_ia_id')
                logger.info(f"Atualização de extensão detectada: ID {extension_id}")
                
                # Converter para o formato usado pelo ServerManager
                config = {
                    'id': extension_id,
                    'ramal_ia': data.get('extension_ia_number', '').strip(),
                    'ramal_retorno': data.get('extension_ia_return', '').strip(),
                    'ip_servidor': data.get('extension_ia_ip', '').strip(),
                    'porta_ia': int(data.get('extension_ia_number_port', 0)),
                    'porta_retorno': int(data.get('extension_ia_return_port', 0)),
                    'condominio_id': data.get('condominium_id', 0)
                }
                
                # Verificar se já temos esta extensão
                if extension_id in self.server_manager.servers:
                    # Parar o servidor atual
                    await self.server_manager.stop_server(extension_id)
                    
                    try:
                        # Iniciar com a nova configuração
                        await self.server_manager.start_server(config)
                        logger.info(f"Servidor para extensão {config['ramal_ia']} reiniciado com nova configuração")
                        
                        # Atualizar configurações locais
                        configs = self.config_persistence.load_configs()
                        for i, existing_config in enumerate(configs):
                            if existing_config.get('id') == extension_id:
                                configs[i] = config
                                break
                        self.config_persistence.save_configs(configs)
                    except Exception as e:
                        logger.error(f"Erro ao reiniciar servidor para extensão ID {extension_id}: {e}")
                        logger.warning(f"A extensão ID {extension_id} foi removida devido à falha na atualização")
                        
                        # Remover das configurações locais já que não conseguimos subir o socket
                        configs = self.config_persistence.load_configs()
                        configs = [c for c in configs if c.get('id') != extension_id]
                        self.config_persistence.save_configs(configs)
                else:
                    # Extensão não existe, tratar como INSERT
                    try:
                        await self.server_manager.start_server(config)
                        logger.info(f"Servidor para extensão atualizada {config['ramal_ia']} iniciado")
                        
                        # Atualizar configurações locais
                        configs = self.config_persistence.load_configs()
                        configs.append(config)
                        self.config_persistence.save_configs(configs)
                    except Exception as e:
                        logger.error(f"Erro ao iniciar servidor para extensão atualizada ID {extension_id}: {e}")
                        # Não persistimos a configuração se falhar ao iniciar o servidor
                
            elif action == 'DELETE':
                # Extensão foi removida
                extension_id = data.get('extension_ia_id')
                logger.info(f"Remoção de extensão detectada: ID {extension_id}")
                
                # Verificar se temos esta extensão
                if extension_id in self.server_manager.servers:
                    # Parar o servidor
                    await self.server_manager.stop_server(extension_id)
                    logger.info(f"Servidor para extensão ID {extension_id} removido com sucesso")
                    
                    # Atualizar configurações locais
                    configs = self.config_persistence.load_configs()
                    configs = [c for c in configs if c.get('id') != extension_id]
                    self.config_persistence.save_configs(configs)
                else:
                    logger.warning(f"Tentativa de remover extensão ID {extension_id} que não está ativa")
            
            else:
                logger.warning(f"Ação desconhecida recebida: {action}")
        
        except Exception as e:
            logger.error(f"Erro ao processar notificação do banco de dados: {e}")

    async def initialize(self, api_port: int = 8082) -> bool:
        """
        Inicializa o sistema de extensões.
        
        Args:
            api_port: Porta para o servidor API
            
        Returns:
            bool: True se inicializado com sucesso
        """
        try:
            # Carregar configurações (do banco ou local)
            configs = self._load_configurations()
            
            if not configs:
                logger.warning("Nenhuma configuração de extensão encontrada. Usando configuração padrão.")
                # Configuração padrão para compatibilidade
                configs = [{
                    'id': 0,
                    'ramal_ia': '1000',
                    'ramal_retorno': '1001',
                    'ip_servidor': '0.0.0.0',
                    'porta_ia': 8080,
                    'porta_retorno': 8081,
                    'condominio_id': 0
                }]
            
            # Iniciar servidores para cada ramal
            success_count = await self.server_manager.start_all_servers(configs)
            
            logger.info(f"Iniciados {success_count} de {len(configs)} servidores de ramais")
            
            # Iniciar servidor API
            self.api_runner, self.api_site = await self.api_server.start(port=api_port)
            
            # Iniciar o listener de banco de dados
            await self.db_listener.start()
            logger.info("Listener de banco de dados para notificações de ramais iniciado")
            
            self.is_running = True
            return True
        
        except Exception as e:
            logger.error(f"Erro ao inicializar sistema de extensões: {e}")
            return False
    
    def _load_configurations(self) -> List[Dict[str, Any]]:
        """
        Carrega configurações de ramais do banco de dados ou arquivo local.
        
        Returns:
            List[Dict]: Lista de configurações de ramais
        """
        # Tentar do banco de dados primeiro
        if self.db_connector.connect():
            configs = self.db_connector.get_extensions()
            
            # Se obteve configurações do banco, salvar localmente
            if configs:
                self.config_persistence.save_configs(configs)
                return configs
        
        # Se não conseguiu do banco, tentar do arquivo local
        logger.info("Não foi possível obter configurações do banco, tentando arquivo local")
        return self.config_persistence.load_configs()
    
    async def refresh_configurations(self) -> Tuple[int, int, int]:
        """
        Atualiza as configurações de ramais a partir do banco de dados.
        
        Returns:
            Tuple[int, int, int]: Contadores de (removidos, atualizados, adicionados)
        """
        if self.db_connector.connect():
            configs = self.db_connector.get_extensions()
            
            if configs:
                # Persistir configurações localmente
                self.config_persistence.save_configs(configs)
                
                # Atualizar servidores
                return await self.server_manager.restart_servers(configs)
        
        return 0, 0, 0
    
    async def shutdown(self) -> bool:
        """
        Encerra todos os servidores e limpa recursos.
        
        Returns:
            bool: True se encerrado com sucesso
        """
        try:
            # Parar o listener do banco de dados
            await self.db_listener.stop()
            logger.info("Listener de banco de dados encerrado")
            
            # Parar todos os servidores
            for extension_id in list(self.server_manager.servers.keys()):
                await self.server_manager.stop_server(extension_id)
            
            # Encerrar servidor API
            if self.api_runner:
                await self.api_runner.cleanup()
            
            # Encerrar conexão com banco
            self.db_connector.disconnect()
            
            self.is_running = False
            return True
        
        except Exception as e:
            logger.error(f"Erro ao encerrar sistema de extensões: {e}")
            return False
    
    def get_extension_info(self, call_id=None, porta=None, ramal=None) -> Dict[str, Any]:
        """
        Obtém informações de um ramal com base em call_id, porta ou número do ramal.
        
        Args:
            call_id: ID da chamada (UUID)
            porta: Número da porta
            ramal: Número do ramal
            
        Returns:
            Dict: Informações do ramal ou dicionário vazio se não encontrado
        """
        return self.server_manager.get_extension_info(call_id, porta, ramal)
    
    def get_all_extensions(self) -> List[Dict[str, Any]]:
        """
        Retorna todas as configurações de ramais ativos.
        
        Returns:
            List[Dict]: Lista de configurações de ramais
        """
        return self.server_manager.get_all_extensions()

extensions/db_connector.py:
import os
import logging
import psycopg2
from psycopg2.extras import RealDictCursor
from dotenv import load_dotenv

load_dotenv()
logger = logging.getLogger(__name__)

class DBConnector:
    def __init__(self):
        self.conn = None
        self.db_config = {
            'dbname': os.getenv('DB_NAME', 'postgres'),
            'user': os.getenv('DB_USER', 'admincd'),
            'password': os.getenv('DB_PASSWORD', 'Isabela@2022!!'),
            'host': os.getenv('DB_HOST', 'dev-postgres-cd.postgres.database.azure.com'),
            'port': os.getenv('DB_PORT', '5432'),
        }
    
    def connect(self):
        """Estabelece conexão com o banco de dados PostgreSQL."""
        try:
            self.conn = psycopg2.connect(**self.db_config)
            logger.info("Conexão com banco de dados PostgreSQL estabelecida com sucesso.")
            return True
        except Exception as e:
            logger.error(f"Erro ao conectar ao banco de dados: {e}")
            return False
    
    def disconnect(self):
        """Encerra a conexão com o banco de dados."""
        if self.conn:
            self.conn.close()
            self.conn = None
            logger.info("Conexão com banco de dados PostgreSQL encerrada.")
    
    def get_extensions(self):
        """
        Obtém todas as configurações de ramais da IA da tabela extension_ia.
        Retorna uma lista de dicionários com as configurações.
        """
        if not self.conn:
            if not self.connect():
                logger.error("Não foi possível conectar ao banco de dados para obter extensões.")
                return []
        
        try:
            cursor = self.conn.cursor(cursor_factory=RealDictCursor)
            query = """
                SELECT 
                    extension_ia_id,
                    TRIM(extension_ia_number) as extension_ia_number,
                    TRIM(extension_ia_return) as extension_ia_return,
                    TRIM(extension_ia_ip) as extension_ia_ip,
                    TRIM(extension_ia_number_port) as extension_ia_number_port,
                    condominium_id,
                    TRIM(extension_ia_return_port) as extension_ia_return_port
                FROM 
                    public.extension_ia
                ORDER BY 
                    extension_ia_id
            """
            cursor.execute(query)
            extensions = cursor.fetchall()
            
            # Converter para formato mais amigável
            result = []
            for ext in extensions:
                result.append({
                    'id': ext['extension_ia_id'],
                    'ramal_ia': ext['extension_ia_number'],
                    'ramal_retorno': ext['extension_ia_return'],
                    'ip_servidor': ext['extension_ia_ip'],
                    'porta_ia': int(ext['extension_ia_number_port']),
                    'porta_retorno': int(ext['extension_ia_return_port'] or 0),
                    'condominio_id': ext['condominium_id']
                })
            
            logger.info(f"Obtidas {len(result)} configurações de ramais do banco de dados.")
            return result
        except Exception as e:
            logger.error(f"Erro ao obter extensões do banco de dados: {e}")
            return []
        finally:
            if cursor:
                cursor.close()
    
    def test_connection(self):
        """Testa a conexão com o banco de dados."""
        if not self.conn:
            return self.connect()
        
        try:
            cursor = self.conn.cursor()
            cursor.execute("SELECT 1")
            cursor.close()
            return True
        except Exception as e:
            logger.error(f"Erro ao testar conexão com banco de dados: {e}")
            self.conn = None
            return False

extensions/resource_manager.py:
import asyncio
import logging
import os
import psutil
import time
from typing import Dict, Set, Optional

logger = logging.getLogger(__name__)

class ResourceManager:
    """
    Classe responsável por gerenciar recursos do sistema para evitar sobrecarga
    quando múltiplos sockets estão ativos simultaneamente.
    """
    
    def __init__(self):
        # Contadores de recursos em uso
        self.active_sessions: Set[str] = set()
        self.speaking_sessions: Set[str] = set()
        self.transcribing_sessions: Set[str] = set()
        
        # Limites de simultaneidade 
        self.max_concurrent_transcriptions = int(os.getenv('MAX_CONCURRENT_TRANSCRIPTIONS', '3'))
        self.max_concurrent_synthesis = int(os.getenv('MAX_CONCURRENT_SYNTHESIS', '3'))
        
        # Semáforos para controle de acesso
        self.transcription_semaphore = asyncio.Semaphore(self.max_concurrent_transcriptions)
        self.synthesis_semaphore = asyncio.Semaphore(self.max_concurrent_synthesis)
        
        # Métricas de performance
        self.metrics: Dict[str, Dict] = {}
        
        # Conexões ativas para cada sessão (permite enviar KIND_HANGUP)
        self.active_connections: Dict[str, Dict] = {}
        
        # Ajustes dinâmicos baseados no hardware
        self._configure_based_on_hardware()
        
        logger.info(f"ResourceManager inicializado: max_concurrent_transcriptions={self.max_concurrent_transcriptions}, "
                   f"max_concurrent_synthesis={self.max_concurrent_synthesis}")
    
    def _configure_based_on_hardware(self):
        """Configura limites baseados nos recursos do hardware."""
        try:
            cpu_count = psutil.cpu_count(logical=False) or 2
            mem_gb = psutil.virtual_memory().total / (1024**3)
            
            # Ajustar limites com base em CPU e memória
            if cpu_count >= 4 and mem_gb >= 8:
                # Hardware robusto
                self.max_concurrent_transcriptions = max(3, min(cpu_count - 1, 6))
                self.max_concurrent_synthesis = max(3, min(cpu_count - 1, 6))
            elif cpu_count >= 2 and mem_gb >= 4:
                # Hardware médio
                self.max_concurrent_transcriptions = 2
                self.max_concurrent_synthesis = 2
            else:
                # Hardware limitado
                self.max_concurrent_transcriptions = 1
                self.max_concurrent_synthesis = 1
            
            logger.info(f"Configuração baseada em hardware: CPUs={cpu_count}, RAM={mem_gb:.1f}GB, "
                       f"Transcrições={self.max_concurrent_transcriptions}, Sínteses={self.max_concurrent_synthesis}")
        except Exception as e:
            logger.warning(f"Erro ao configurar baseado no hardware: {e}. Usando valores padrão.")
    
    def register_session(self, session_id: str, port: Optional[int] = None):
        """Registra uma nova sessão ativa."""
        self.active_sessions.add(session_id)
        self.metrics[session_id] = {
            'start_time': time.time(),
            'port': port,
            'transcription_count': 0,
            'synthesis_count': 0,
            'transcription_time_ms': 0,
            'synthesis_time_ms': 0
        }
        logger.debug(f"Sessão {session_id} registrada. Total de sessões ativas: {len(self.active_sessions)}")
    
    def unregister_session(self, session_id: str):
        """Remove uma sessão terminada."""
        if session_id in self.active_sessions:
            self.active_sessions.remove(session_id)
        
        if session_id in self.speaking_sessions:
            self.speaking_sessions.remove(session_id)
            
        if session_id in self.transcribing_sessions:
            self.transcribing_sessions.remove(session_id)
            
        # Registrar métricas finais
        if session_id in self.metrics:
            duration = time.time() - self.metrics[session_id]['start_time']
            logger.info(f"Sessão {session_id} encerrada após {duration:.1f}s. "
                       f"Transcrições: {self.metrics[session_id]['transcription_count']}, "
                       f"Sínteses: {self.metrics[session_id]['synthesis_count']}")
            del self.metrics[session_id]
    
    def set_speaking(self, session_id: str, is_speaking: bool):
        """Marca uma sessão como falando ou não."""
        if is_speaking:
            self.speaking_sessions.add(session_id)
        elif session_id in self.speaking_sessions:
            self.speaking_sessions.remove(session_id)
    
    def set_transcribing(self, session_id: str, is_transcribing: bool):
        """Marca uma sessão como transcrevendo ou não."""
        if is_transcribing:
            self.transcribing_sessions.add(session_id)
        elif session_id in self.transcribing_sessions:
            self.transcribing_sessions.remove(session_id)
    
    async def acquire_transcription_lock(self, session_id: str):
        """
        Adquire um lock para transcrição, limitando o número de transcrições
        simultâneas para evitar sobrecarga de CPU/memória.
        """
        await self.transcription_semaphore.acquire()
        self.set_transcribing(session_id, True)
        return True
    
    def release_transcription_lock(self, session_id: str):
        """Libera um lock de transcrição."""
        self.set_transcribing(session_id, False)
        self.transcription_semaphore.release()
    
    async def acquire_synthesis_lock(self, session_id: str):
        """
        Adquire um lock para síntese de voz, limitando o número de sínteses
        simultâneas para evitar sobrecarga.
        """
        await self.synthesis_semaphore.acquire()
        return True
    
    def release_synthesis_lock(self, session_id: str):
        """Libera um lock de síntese."""
        self.synthesis_semaphore.release()
    
    def record_transcription(self, session_id: str, duration_ms: float):
        """Registra métricas de uma transcrição."""
        if session_id in self.metrics:
            self.metrics[session_id]['transcription_count'] += 1
            self.metrics[session_id]['transcription_time_ms'] += duration_ms
    
    def record_synthesis(self, session_id: str, duration_ms: float):
        """Registra métricas de uma síntese."""
        if session_id in self.metrics:
            self.metrics[session_id]['synthesis_count'] += 1
            self.metrics[session_id]['synthesis_time_ms'] += duration_ms
    
    def get_system_load(self):
        """Retorna informações sobre o carregamento atual do sistema."""
        try:
            cpu_percent = psutil.cpu_percent(interval=0.1)
            mem_percent = psutil.virtual_memory().percent
            return {
                'cpu_percent': cpu_percent,
                'memory_percent': mem_percent,
                'active_sessions': len(self.active_sessions),
                'speaking_sessions': len(self.speaking_sessions),
                'transcribing_sessions': len(self.transcribing_sessions)
            }
        except Exception as e:
            logger.error(f"Erro ao obter carga do sistema: {e}")
            return {
                'error': str(e)
            }
            
    def should_throttle_audio(self):
        """
        Determina se a transmissão de áudio deve ser limitada com base na carga do sistema.
        Retorna True se o sistema estiver sobrecarregado.
        """
        system_load = self.get_system_load()
        cpu_percent = system_load.get('cpu_percent', 0)
        active_sessions = system_load.get('active_sessions', 0)
        
        # Se temos muitas sessões ativas E a CPU está alta, ativamos throttling
        return active_sessions > 3 and cpu_percent > 85
        
    def register_connection(self, call_id: str, role: str, reader, writer):
        """
        Registra uma conexão de socket ativa para permitir envio do KIND_HANGUP.
        
        Args:
            call_id: ID da chamada
            role: 'visitor' ou 'resident'
            reader: StreamReader da conexão
            writer: StreamWriter da conexão
        """
        if call_id not in self.active_connections:
            self.active_connections[call_id] = {}
            
        self.active_connections[call_id][role] = {
            'reader': reader,
            'writer': writer,
            'timestamp': time.time()
        }
        logger.debug(f"Conexão registrada para {call_id} ({role})")
        
    def unregister_connection(self, call_id: str, role: str):
        """
        Remove uma conexão quando ela é encerrada.
        """
        if call_id in self.active_connections and role in self.active_connections[call_id]:
            del self.active_connections[call_id][role]
            logger.debug(f"Conexão removida para {call_id} ({role})")
            
            # Se não há mais conexões para esta chamada, limpar
            if not self.active_connections[call_id]:
                del self.active_connections[call_id]
                
    def get_active_connection(self, call_id: str, role: str):
        """
        Retorna informações sobre uma conexão ativa.
        
        Args:
            call_id: ID da chamada
            role: 'visitor' ou 'resident'
            
        Returns:
            Dict com reader e writer, ou None se não existir
        """
        if call_id in self.active_connections and role in self.active_connections[call_id]:
            return self.active_connections[call_id][role]
        return None

# Instância global
resource_manager = ResourceManager()

extensions/__init__.py:


extensions/server_manager.py:
import asyncio
import logging
import socket
from typing import Dict, List, Tuple, Any

# Importar handlers do audiosocket dinamicamente
from audiosocket_handler import iniciar_servidor_audiosocket_visitante, iniciar_servidor_audiosocket_morador

logger = logging.getLogger(__name__)

class ServerManager:
    """
    Classe responsável por gerenciar os servidores socket para ramais de IA.
    Permite iniciar, parar e reiniciar servidores dinamicamente.
    """
    
    def __init__(self):
        # Dicionário para armazenar servidores ativos
        # {extension_id: {'ia_server': obj, 'retorno_server': obj, 'config': dict}}
        self.servers: Dict[int, Dict[str, Any]] = {}
        
        # Mapeamento de porta para extension_id para identificação rápida
        # {porta: extension_id}
        self.port_to_extension: Dict[int, int] = {}
        
        # Mapeamento de ramal para extension_id
        # {ramal: extension_id}
        self.extension_to_id: Dict[str, int] = {}
        
        # Mapeamento reverso de porta de retorno para porta de IA
        # {porta_retorno: porta_ia}
        self.return_to_ia_port: Dict[int, int] = {}
    
    def is_port_available(self, ip: str, port: int) -> bool:
        """
        Verifica se uma porta está disponível para uso.
        
        Args:
            ip: Endereço IP para verificar
            port: Número da porta para verificar
            
        Returns:
            bool: True se a porta estiver disponível, False caso contrário
        """
        sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)
        try:
            sock.bind((ip, port))
            result = True
        except:
            result = False
        finally:
            sock.close()
        return result
    
    async def start_server(self, config: Dict[str, Any]) -> Tuple[asyncio.Server, asyncio.Server]:
        """
        Inicia servidores socket para um ramal específico.
        
        Args:
            config: Dicionário com configuração do ramal
            
        Returns:
            Tuple contendo os servidores de IA e retorno, ou levanta exceção se não for possível iniciar
        """
        extension_id = config['id']
        ramal_ia = config['ramal_ia']
        porta_ia = config['porta_ia']
        porta_retorno = config['porta_retorno']
        ip_registro = config['ip_servidor']  # IP para registro/Asterisk
        
        # Sempre usar 0.0.0.0 para binding do socket
        binding_ip = '0.0.0.0'
        
        # Verificar se as portas estão disponíveis no IP de binding
        if not self.is_port_available(binding_ip, porta_ia):
            err_msg = f"Porta {porta_ia} não está disponível para ramal IA {ramal_ia}. Ramal não será iniciado."
            logger.error(err_msg)
            raise RuntimeError(err_msg)
        
        if not self.is_port_available(binding_ip, porta_retorno):
            err_msg = f"Porta {porta_retorno} não está disponível para ramal retorno {config['ramal_retorno']}. Ramal não será iniciado."
            logger.error(err_msg)
            raise RuntimeError(err_msg)
        
        # Criar servidores assíncronos
        try:
            # Servidor para visitante (IA) - com parâmetros para melhor qualidade de áudio
            ia_server = await asyncio.start_server(
                iniciar_servidor_audiosocket_visitante,
                binding_ip,  # Use 0.0.0.0 para binding
                porta_ia,
                # Manter apenas parâmetros essenciais e alguns importantes para qualidade
                limit=1024*1024,  # 1MB buffer
                start_serving=True
            )
            
            # Servidor para morador (retorno) - com parâmetros para melhor qualidade de áudio
            retorno_server = await asyncio.start_server(
                iniciar_servidor_audiosocket_morador,
                binding_ip,  # Use 0.0.0.0 para binding
                porta_retorno,
                # Manter apenas parâmetros essenciais e alguns importantes para qualidade
                limit=1024*1024,  # 1MB buffer
                start_serving=True
            )
            
            # Criar uma cópia da configuração e adicionar detalhes de binding
            config_copy = config.copy()
            config_copy['binding_ip'] = binding_ip
            
            # Armazenar servidores e configuração sem criar tasks
            # Voltando à configuração original que funcionava
            self.servers[extension_id] = {
                'ia_server': ia_server,
                'retorno_server': retorno_server,
                'config': config_copy
            }
            
            # Atualizar mapeamentos
            self.port_to_extension[porta_ia] = extension_id
            self.port_to_extension[porta_retorno] = extension_id
            self.extension_to_id[ramal_ia] = extension_id
            self.return_to_ia_port[porta_retorno] = porta_ia
            
            logger.info(f"Iniciados servidores para ramal {ramal_ia}: "
                       f"IA: Socket em {binding_ip}:{porta_ia}, Registro para Asterisk: {ip_registro}:{porta_ia}, "
                       f"Retorno: Socket em {binding_ip}:{porta_retorno}, Registro para Asterisk: {ip_registro}:{porta_retorno}")
            
            return ia_server, retorno_server
        
        except Exception as e:
            logger.error(f"Erro ao iniciar servidores para ramal {ramal_ia}: {e}")
            raise
    
    async def stop_server(self, extension_id: int) -> bool:
        """
        Para os servidores de um ramal específico.
        
        Args:
            extension_id: ID do ramal para parar
            
        Returns:
            bool: True se os servidores foram parados com sucesso
        """
        if extension_id not in self.servers:
            logger.warning(f"Tentativa de parar servidores para ramal inexistente: {extension_id}")
            return False
        
        try:
            servers = self.servers[extension_id]
            config = servers['config']
            
            # Removendo código de cancelamento de tasks
            # para voltar à configuração original que funcionava
            
            # Fechar servidores
            servers['ia_server'].close()
            servers['retorno_server'].close()
            
            # Aguardar fechamento completo
            await servers['ia_server'].wait_closed()
            await servers['retorno_server'].wait_closed()
            
            # Remover mapeamentos
            porta_ia = config['porta_ia']
            porta_retorno = config['porta_retorno']
            ramal_ia = config['ramal_ia']
            
            if porta_ia in self.port_to_extension:
                del self.port_to_extension[porta_ia]
            
            if porta_retorno in self.port_to_extension:
                del self.port_to_extension[porta_retorno]
            
            if ramal_ia in self.extension_to_id:
                del self.extension_to_id[ramal_ia]
            
            if porta_retorno in self.return_to_ia_port:
                del self.return_to_ia_port[porta_retorno]
            
            # Remover da lista de servidores
            del self.servers[extension_id]
            
            logger.info(f"Servidores para ramal {ramal_ia} parados com sucesso")
            return True
        
        except Exception as e:
            logger.error(f"Erro ao parar servidores para ramal {extension_id}: {e}")
            return False
    
    async def start_all_servers(self, configs: List[Dict[str, Any]]) -> int:
        """
        Inicia todos os servidores com base nas configurações fornecidas.
        
        Args:
            configs: Lista de configurações de ramais
            
        Returns:
            int: Número de servidores iniciados com sucesso
        """
        success_count = 0
        tasks = []
        
        # Iniciar todos os servidores em paralelo para melhorar performance
        for config in configs:
            task = asyncio.create_task(self._safe_start_server(config))
            tasks.append(task)
        
        # Esperar que todos os servidores sejam iniciados
        results = await asyncio.gather(*tasks, return_exceptions=True)
        
        # Contar os servidores iniciados com sucesso
        for result in results:
            if isinstance(result, Exception):
                # Um erro ocorreu durante a inicialização
                logger.error(f"Erro ao iniciar servidor: {result}")
            elif result:
                # Servidor iniciado com sucesso
                success_count += 1
        
        return success_count
    
    async def _safe_start_server(self, config: Dict[str, Any]) -> bool:
        """
        Método auxiliar para iniciar um servidor com tratamento de exceções.
        
        Args:
            config: Configuração do ramal
            
        Returns:
            bool: True se o servidor foi iniciado com sucesso
        """
        try:
            await self.start_server(config)
            return True
        except Exception as e:
            logger.error(f"Falha ao iniciar servidor para ramal {config['ramal_ia']}: {e}")
            return False
    
    async def restart_servers(self, new_configs: List[Dict[str, Any]]) -> Tuple[int, int, int]:
        """
        Reinicia os servidores com novas configurações. 
        Para servidores que não existem mais, inicia novos e atualiza os existentes.
        
        Args:
            new_configs: Lista com novas configurações de ramais
            
        Returns:
            Tuple[int, int, int]: Contadores de (removidos, atualizados, adicionados)
        """
        removed_count = 0
        updated_count = 0
        added_count = 0
        
        # Mapear novos configs por ID para fácil acesso
        new_configs_map = {config['id']: config for config in new_configs}
        
        # Remover servidores que não estão mais nas configurações
        current_ids = set(self.servers.keys())
        new_ids = set(new_configs_map.keys())
        
        # IDs a serem removidos
        for extension_id in current_ids - new_ids:
            success = await self.stop_server(extension_id)
            if success:
                removed_count += 1
        
        # Atualizar servidores existentes ou adicionar novos
        for config in new_configs:
            extension_id = config['id']
            
            if extension_id in self.servers:
                # Verificar se configuração mudou
                old_config = self.servers[extension_id]['config']
                
                if self._config_changed(old_config, config):
                    # Se configuração mudou, reiniciar servidor
                    await self.stop_server(extension_id)
                    try:
                        await self.start_server(config)
                        updated_count += 1
                        logger.info(f"Servidor para ramal {config['ramal_ia']} atualizado com sucesso")
                    except Exception as e:
                        logger.error(f"Erro ao atualizar servidor para ramal {config['ramal_ia']}: {e}")
            else:
                # Iniciar novo servidor
                try:
                    await self.start_server(config)
                    added_count += 1
                    logger.info(f"Novo servidor para ramal {config['ramal_ia']} iniciado com sucesso")
                except Exception as e:
                    logger.error(f"Erro ao iniciar novo servidor para ramal {config['ramal_ia']}: {e}")
        
        return removed_count, updated_count, added_count
    
    def _config_changed(self, old_config: Dict[str, Any], new_config: Dict[str, Any]) -> bool:
        """
        Verifica se a configuração de um ramal mudou.
        
        Args:
            old_config: Configuração antiga
            new_config: Nova configuração
            
        Returns:
            bool: True se a configuração mudou
        """
        # Campos relevantes para verificar mudanças
        keys = ['ip_servidor', 'porta_ia', 'porta_retorno', 'ramal_ia', 'ramal_retorno']
        
        for key in keys:
            if old_config.get(key) != new_config.get(key):
                return True
        
        return False
    
    def get_extension_info(self, call_id: str = None, porta: int = None, ramal: str = None) -> Dict[str, Any]:
        """
        Recupera informações de um ramal com base em call_id, porta ou número do ramal.
        
        Args:
            call_id: ID da chamada (UUID)
            porta: Número da porta
            ramal: Número do ramal
            
        Returns:
            Dict: Informações do ramal ou dicionário vazio se não encontrado
        """
        # Primeiro tenta pela porta (mais rápido)
        if porta and porta in self.port_to_extension:
            extension_id = self.port_to_extension[porta]
            if extension_id in self.servers:
                return self.servers[extension_id]['config']
        
        # Depois tenta pelo ramal
        if ramal and ramal in self.extension_to_id:
            extension_id = self.extension_to_id[ramal]
            if extension_id in self.servers:
                return self.servers[extension_id]['config']
        
        return {}
    
    def get_all_extensions(self) -> List[Dict[str, Any]]:
        """
        Retorna todas as configurações de ramais ativos.
        
        Returns:
            List[Dict]: Lista de configurações de ramais
        """
        return [server['config'] for server in self.servers.values()]
    
    def get_return_info(self, porta_ia: int) -> Dict[str, Any]:
        """
        Obtém informações de retorno com base na porta da IA.
        
        Args:
            porta_ia: Porta da IA
            
        Returns:
            Dict: Informações do retorno ou dicionário vazio se não encontrado
        """
        if porta_ia in self.port_to_extension:
            extension_id = self.port_to_extension[porta_ia]
            if extension_id in self.servers:
                return self.servers[extension_id]['config']
        
        return {}
    
    def get_ia_info_from_return(self, porta_retorno: int) -> Dict[str, Any]:
        """
        Obtém informações da IA com base na porta de retorno.
        
        Args:
            porta_retorno: Porta de retorno
            
        Returns:
            Dict: Informações da IA ou dicionário vazio se não encontrado
        """
        if porta_retorno in self.return_to_ia_port:
            porta_ia = self.return_to_ia_port[porta_retorno]
            return self.get_extension_info(porta=porta_ia)
        
        return {}

extensions/api_server.py:
import logging
import json
from aiohttp import web
import asyncio
from typing import Dict, Any

from .server_manager import ServerManager
from .config_persistence import ConfigPersistence
from .db_connector import DBConnector

logger = logging.getLogger(__name__)

class APIServer:
    """
    Servidor HTTP simples para gerenciar os ramais de IA remotamente.
    Fornece endpoints para status, atualização de configurações, etc.
    """
    
    def __init__(self, server_manager: ServerManager, config_persistence: ConfigPersistence, db_connector: DBConnector):
        self.server_manager = server_manager
        self.config_persistence = config_persistence
        self.db_connector = db_connector
        self.app = web.Application()
        self.setup_routes()
    
    def setup_routes(self):
        """Configura as rotas da API."""
        self.app.router.add_get('/api/status', self.get_status)
        self.app.router.add_post('/api/refresh', self.refresh_config)
        self.app.router.add_get('/api/extensions', self.get_extensions)
        self.app.router.add_post('/api/restart', self.restart_extension)
        self.app.router.add_post('/api/hangup', self.hangup_call)
    
    async def get_status(self, request: web.Request) -> web.Response:
        """
        Retorna o status de todos os servidores de ramais ativos.
        
        URL: GET /api/status
        """
        extensions = []
        
        for extension_id, server_data in self.server_manager.servers.items():
            config = server_data['config']
            extensions.append({
                "id": extension_id,
                "ramal_ia": config['ramal_ia'],
                "ramal_retorno": config['ramal_retorno'],
                "ip": config['ip_servidor'],
                "porta_ia": config['porta_ia'],
                "porta_retorno": config['porta_retorno'],
                "condominio_id": config['condominio_id'],
                "status": "ativo"
            })
        
        return web.json_response({
            "status": "success",
            "total_extensions": len(extensions),
            "extensions": extensions
        })
    
    async def refresh_config(self, request: web.Request) -> web.Response:
        """
        Atualiza as configurações de ramais a partir do banco de dados.
        
        URL: POST /api/refresh
        """
        try:
            # Obter novas configurações do banco
            db_configs = self.db_connector.get_extensions()
            
            if not db_configs:
                return web.json_response({
                    "status": "error",
                    "message": "Não foi possível obter configurações do banco de dados"
                }, status=500)
            
            # Atualizar servidores com novas configurações
            removed, updated, added = await self.server_manager.restart_servers(db_configs)
            
            # Persistir configurações localmente
            self.config_persistence.save_configs(db_configs)
            
            return web.json_response({
                "status": "success",
                "message": "Configurações atualizadas com sucesso",
                "stats": {
                    "removed": removed,
                    "updated": updated,
                    "added": added,
                    "total_active": len(self.server_manager.servers)
                }
            })
        
        except Exception as e:
            logger.error(f"Erro ao atualizar configurações: {e}")
            return web.json_response({
                "status": "error",
                "message": f"Erro ao atualizar configurações: {str(e)}"
            }, status=500)
    
    async def get_extensions(self, request: web.Request) -> web.Response:
        """
        Retorna todas as configurações de ramais do banco de dados.
        
        URL: GET /api/extensions
        """
        try:
            db_configs = self.db_connector.get_extensions()
            return web.json_response({
                "status": "success",
                "total": len(db_configs),
                "extensions": db_configs
            })
        except Exception as e:
            logger.error(f"Erro ao obter extensões: {e}")
            return web.json_response({
                "status": "error",
                "message": f"Erro ao obter extensões: {str(e)}"
            }, status=500)
    
    async def restart_extension(self, request: web.Request) -> web.Response:
        """
        Reinicia um ramal específico.
        
        URL: POST /api/restart
        Body: {"extension_id": 123} ou {"ramal": "1001"}
        """
        try:
            data = await request.json()
            
            if 'extension_id' in data:
                extension_id = int(data['extension_id'])
                if extension_id in self.server_manager.servers:
                    config = self.server_manager.servers[extension_id]['config']
                    await self.server_manager.stop_server(extension_id)
                    await self.server_manager.start_server(config)
                    return web.json_response({
                        "status": "success",
                        "message": f"Ramal ID {extension_id} reiniciado com sucesso"
                    })
                else:
                    return web.json_response({
                        "status": "error",
                        "message": f"Ramal ID {extension_id} não encontrado"
                    }, status=404)
            
            elif 'ramal' in data:
                ramal = data['ramal']
                if ramal in self.server_manager.extension_to_id:
                    extension_id = self.server_manager.extension_to_id[ramal]
                    config = self.server_manager.servers[extension_id]['config']
                    await self.server_manager.stop_server(extension_id)
                    await self.server_manager.start_server(config)
                    return web.json_response({
                        "status": "success",
                        "message": f"Ramal {ramal} reiniciado com sucesso"
                    })
                else:
                    return web.json_response({
                        "status": "error",
                        "message": f"Ramal {ramal} não encontrado"
                    }, status=404)
            
            else:
                return web.json_response({
                    "status": "error",
                    "message": "É necessário fornecer extension_id ou ramal"
                }, status=400)
        
        except Exception as e:
            logger.error(f"Erro ao reiniciar ramal: {e}")
            return web.json_response({
                "status": "error",
                "message": f"Erro ao reiniciar ramal: {str(e)}"
            }, status=500)
    
    async def hangup_call(self, request: web.Request) -> web.Response:
        """
        Envia sinal de hangup (KIND_HANGUP, 0x00) para uma chamada ativa.
        
        URL: POST /api/hangup
        Body: {"call_id": "uuid-da-chamada", "role": "visitor|resident"}
        """
        try:
            from audiosocket_handler import session_manager
            import struct
            
            data = await request.json()
            
            if 'call_id' not in data:
                return web.json_response({
                    "status": "error",
                    "message": "call_id é obrigatório"
                }, status=400)
                
            call_id = data['call_id']
            role = data.get('role', 'visitor')  # Padrão é visitante
            
            # Validar role
            if role not in ['visitor', 'resident']:
                return web.json_response({
                    "status": "error",
                    "message": "role deve ser 'visitor' ou 'resident'"
                }, status=400)
            
            # Verificar se a sessão existe
            session = session_manager.get_session(call_id)
            if not session:
                return web.json_response({
                    "status": "error",
                    "message": f"Sessão {call_id} não encontrada"
                }, status=404)
            
            # Obter a conexão ativa da sessão através do ResourceManager
            from extensions.resource_manager import resource_manager
            
            connection = resource_manager.get_active_connection(call_id, role)
            if not connection:
                return web.json_response({
                    "status": "error",
                    "message": f"Conexão ativa não encontrada para {call_id} ({role})"
                }, status=404)
            
            # Enviar KIND_HANGUP (0x00) com payload length 0
            writer = connection.get('writer')
            if not writer:
                return web.json_response({
                    "status": "error",
                    "message": f"Writer não disponível para {call_id} ({role})"
                }, status=500)
            
            # Enviar KIND_HANGUP (0x00) com tratamento de erro
            try:
                writer.write(struct.pack('>B H', 0x00, 0))
                await writer.drain()
            except ConnectionResetError:
                logger.info(f"Conexão já foi resetada durante envio de KIND_HANGUP para {call_id} ({role}) - comportamento normal")
            except Exception as e:
                logger.error(f"Erro ao enviar KIND_HANGUP para {call_id} ({role}): {e}")
                return web.json_response({
                    "status": "error",
                    "message": f"Erro ao enviar KIND_HANGUP: {str(e)}"
                }, status=500)
            
            # Definir flag para indicar teste de hangup na sessão
            session.intent_data["test_hangup"] = True
            
            # Aguardar um momento e então encerrar a sessão completamente
            asyncio.create_task(self._cleanup_session_after_delay(call_id, session_manager))
            
            logger.info(f"KIND_HANGUP enviado com sucesso para {call_id} ({role})")
            return web.json_response({
                "status": "success",
                "message": f"KIND_HANGUP enviado com sucesso para {call_id} ({role})"
            })
            
        except Exception as e:
            logger.error(f"Erro ao enviar KIND_HANGUP: {e}", exc_info=True)
            return web.json_response({
                "status": "error",
                "message": f"Erro ao enviar KIND_HANGUP: {str(e)}"
            }, status=500)
    
    async def _cleanup_session_after_delay(self, call_id, session_manager, delay=3.0):
        """Aguarda um delay e então limpa a sessão completamente."""
        await asyncio.sleep(delay)
        session = session_manager.get_session(call_id)
        if session:
            # Sinalizar encerramento e depois forçar remoção
            session_manager.end_session(call_id)
            await asyncio.sleep(1.0)
            session_manager._complete_session_termination(call_id)
            logger.info(f"Sessão {call_id} encerrada após KIND_HANGUP")
    
    async def start(self, host: str = '0.0.0.0', port: int = 8082):
        """
        Inicia o servidor API.
        
        Args:
            host: Endereço IP para bindar o servidor
            port: Porta para o servidor API
        """
        runner = web.AppRunner(self.app)
        await runner.setup()
        site = web.TCPSite(runner, host, port)
        await site.start()
        
        logger.info(f"Servidor API iniciado em http://{host}:{port}")
        
        return runner, site

extensions/config_persistence.py:
import json
import os
import logging

logger = logging.getLogger(__name__)

class ConfigPersistence:
    """
    Classe responsável por persistir configurações de ramais localmente,
    permitindo que o sistema reinicie com as mesmas configurações mesmo
    sem acesso ao banco de dados.
    """
    
    def __init__(self, config_path="./data/ramais_config.json"):
        self.config_path = config_path
        # Garante que o diretório existe
        os.makedirs(os.path.dirname(self.config_path), exist_ok=True)
    
    def save_configs(self, configs):
        """
        Salva as configurações de ramais em um arquivo JSON local.
        
        Args:
            configs (list): Lista de dicionários com configurações de ramais
        """
        try:
            with open(self.config_path, 'w') as f:
                json.dump({'ramais': configs}, f, indent=2)
            logger.info(f"Configurações de {len(configs)} ramais salvas em {self.config_path}")
            return True
        except Exception as e:
            logger.error(f"Erro ao salvar configurações localmente: {e}")
            return False
    
    def load_configs(self):
        """
        Carrega configurações de ramais a partir do arquivo JSON local.
        
        Returns:
            list: Lista de dicionários com configurações de ramais
        """
        if not os.path.exists(self.config_path):
            logger.warning(f"Arquivo de configuração {self.config_path} não encontrado.")
            return []
        
        try:
            with open(self.config_path, 'r') as f:
                data = json.load(f)
                configs = data.get('ramais', [])
                logger.info(f"Carregadas {len(configs)} configurações de ramais do arquivo local")
                return configs
        except Exception as e:
            logger.error(f"Erro ao carregar configurações locais: {e}")
            return []

extensions/db_listener.py:
import asyncio
import json
import logging
import psycopg2
import psycopg2.extensions
import select
from typing import Callable, Dict, Any
from dotenv import load_dotenv
import os

load_dotenv()
logger = logging.getLogger(__name__)

class PostgresListener:
    """
    Classe que implementa um listener assíncrono para notificações do PostgreSQL.
    Usa asyncio para não bloquear a aplicação principal.
    """
    
    def __init__(self, callback: Callable[[dict], None], channel: str = "change_record_extension_ia"):
        """
        Inicializa o listener do PostgreSQL.
        
        Args:
            callback: Função que será chamada quando uma notificação for recebida
            channel: Canal do PostgreSQL para escutar notificações
        """
        self.callback = callback
        self.channel = channel
        self.conn = None
        self.running = False
        self.task = None
        self.db_config = {
            'dbname': os.getenv('DB_NAME', 'postgres'),
            'user': os.getenv('DB_USER', 'admincd'),
            'password': os.getenv('DB_PASSWORD', 'Isabela@2022!!'),
            'host': os.getenv('DB_HOST', 'dev-postgres-cd.postgres.database.azure.com'),
            'port': os.getenv('DB_PORT', '5432'),
        }
    
    async def connect(self) -> bool:
        """
        Estabelece conexão com o banco de dados PostgreSQL.
        
        Returns:
            bool: True se a conexão foi estabelecida com sucesso
        """
        try:
            # Use psycopg2 diretamente, já que não tem versão assíncrona nativa
            self.conn = psycopg2.connect(**self.db_config)
            self.conn.set_isolation_level(psycopg2.extensions.ISOLATION_LEVEL_AUTOCOMMIT)
            
            # Registrar no canal
            cursor = self.conn.cursor()
            cursor.execute(f"LISTEN {self.channel};")
            cursor.close()
            
            logger.info(f"Listener conectado ao banco de dados e escutando no canal '{self.channel}'")
            return True
        except Exception as e:
            logger.error(f"Erro ao conectar listener ao banco de dados: {e}")
            return False
    
    async def listen(self):
        """
        Inicia o loop de escuta de notificações de forma não-bloqueante.
        """
        if not self.conn:
            success = await self.connect()
            if not success:
                logger.error("Não foi possível iniciar o listener devido a falha na conexão")
                return
        
        self.running = True
        logger.info(f"Listener iniciado no canal '{self.channel}'")
        
        while self.running:
            try:
                # Verifica se há notificações disponíveis, com timeout para não bloquear
                if select.select([self.conn], [], [], 1) == ([self.conn], [], []):
                    self.conn.poll()
                    
                    # Processa todas as notificações pendentes
                    while self.conn.notifies:
                        notify = self.conn.notifies.pop(0)
                        try:
                            payload = json.loads(notify.payload)
                            logger.info(f"Notificação recebida: {payload['action']} na extensão")
                            logger.debug(f"Payload completo: {payload}")
                            
                            # Chama o callback com os dados recebidos
                            await self.callback(payload)
                        except json.JSONDecodeError:
                            logger.error(f"Payload inválido recebido: {notify.payload}")
                        except Exception as e:
                            logger.error(f"Erro ao processar notificação: {e}", exc_info=True)
                
                # Dá chance para outras tarefas assíncronas executarem
                await asyncio.sleep(0.1)
                
            except psycopg2.OperationalError:
                logger.error("Conexão com o banco de dados perdida. Tentando reconectar...")
                self.conn.close()
                self.conn = None
                
                # Tenta reconectar
                await asyncio.sleep(5)  # Espera 5 segundos antes de tentar novamente
                success = await self.connect()
                if not success:
                    logger.error("Falha ao reconectar. Tentando novamente em 10 segundos...")
                    await asyncio.sleep(10)
            
            except Exception as e:
                logger.error(f"Erro no loop do listener: {e}")
                await asyncio.sleep(5)  # Pequena pausa para evitar loop de erro intensivo
    
    async def start(self):
        """
        Inicia o listener em uma tarefa assíncrona separada.
        """
        if self.task is not None:
            logger.warning("Listener já está em execução")
            return
        
        # Inicia o listener como uma tarefa assíncrona
        self.task = asyncio.create_task(self.listen())
        logger.info("Tarefa de listener iniciada")
        
    async def stop(self):
        """
        Para o listener e libera os recursos.
        """
        self.running = False
        
        if self.task:
            self.task.cancel()
            try:
                await self.task
            except asyncio.CancelledError:
                pass
            self.task = None
        
        if self.conn:
            self.conn.close()
            self.conn = None
        
        logger.info("Listener encerrado")

extensions/mock_db_connector.py:
import logging
import os
from typing import List, Dict, Any

logger = logging.getLogger(__name__)

class MockDBConnector:
    """
    Implementação de substituição para DBConnector quando o banco de dados não está disponível.
    Fornece as mesmas interfaces mas usa valores fixos ou configuração local.
    """
    
    def __init__(self):
        logger.info("Inicializando MockDBConnector para modo de compatibilidade")
        self.local_config = []
        
    def connect(self):
        """Simula conexão bem-sucedida."""
        logger.info("Simulando conexão bem-sucedida no modo de compatibilidade")
        return True
        
    def disconnect(self):
        """Simula desconexão."""
        logger.info("Simulando desconexão no modo de compatibilidade")
        
    def get_extensions(self) -> List[Dict[str, Any]]:
        """
        Retorna configurações de ramais padrão para modo de compatibilidade.
        Tenta carregar de ramais_config.json se disponível.
        
        Returns:
            List[Dict]: Lista de configurações de ramais
        """
        # Primeiro tenta carregar da configuração local
        try:
            from extensions.config_persistence import ConfigPersistence
            config_persistence = ConfigPersistence()
            saved_configs = config_persistence.load_configs()
            if saved_configs:
                logger.info(f"Usando {len(saved_configs)} configurações de ramais do arquivo local")
                self.local_config = saved_configs
                return saved_configs
        except Exception as e:
            logger.warning(f"Erro ao carregar configurações locais: {e}")
        
        # Se não tem configurações salvas, retorna configuração padrão
        # para compatibilidade com versões anteriores
        default_config = [
            {
                'id': 1,
                'ramal_ia': '1001',
                'ramal_retorno': '1002',
                'ip_servidor': '0.0.0.0',
                'porta_ia': 8080,
                'porta_retorno': 8081,
                'condominio_id': 1
            }
        ]
        
        logger.info("Usando configuração padrão para modo de compatibilidade")
        return default_config
        
    def test_connection(self):
        """Simula teste de conexão bem-sucedido."""
        return True

ai/state_manager.py:
import redis
import json

r = redis.Redis(host='localhost', port=6379, decode_responses=True)

def get_user_state(id: str):
    data = r.get(id)
    return json.loads(data) if data else {"intent": {}, "history": []}

def update_user_state(user_id: str, intent=None, message=None):
    state = get_user_state(user_id)
    if intent:
        state["intent"].update(intent)
    if message:
        state["history"].append(message)
    r.set(user_id, json.dumps(state))

def clear_user_state(user_id: str):
    r.delete(user_id)


ai/tasks.py:
from crewai import Task
from ai.agents import (create_conversation_coordinator_agent, create_conversation_monitor_agent,
                       identificator_person_agent, identificator_intent_agent, identificator_resident_apartment_agent)


def create_conversation_coordinator_task(user_message: str, conversation_history: str, intent: dict) -> Task:
    agent = create_conversation_coordinator_agent()
    monitor = create_conversation_monitor_agent()
    print(conversation_history)
    return Task(
        description=f"""
        Você é um concierge virtual em uma conversa com alguém no portão do condomínio.

        Histórico da conversa:
        {conversation_history}

        Mensagem nova:
        "{user_message}"

        Intenção acumulada até agora:
        {intent}

        Seu trabalho:
        - Verificar se todos os campos estão preenchidos:
          • intent_type
          • interlocutor_name
          • apartment_number
          • resident_name
        - Se faltar algo, pergunte o que falta.
        - Se estiver completo, apenas confirme.

        Regras:
        - NÃO repita o que já foi dito.
        - NÃO reinicie a conversa.
        - Não peça dados que já estão no intent.
        """,
        # expected_output="""
        # Mensagem para o usuário ou confirmação final.
        # Confirme a ação de forma educada, objetiva e breve.
        # Exemplos de formato desejado:
        # - "Entrega confirmada para Fulano no xxxx. Notificarei o morador."
        # - "Entendido, vou avisar Cicrano no apartamento bla bla bla."
        # Evite repetições desnecessárias, não deseje bom dia ou acrescente floreios.
        # """,
        expected_output=""""
        Responda em formato JSON com os seguintes campos:
        - mensagem: mensagem de resposta clara e objetiva para o usuário, podendo ser a confirmação final. 
        Confirme a ação de forma educada, objetiva e breve. Ex, "Entrega confirmada para Fulano no xxxx. Notificarei o 
        morador"., ou ainda, "Entendido, vou avisar Cicrano no apartamento bla bla bla."
        - dados: objeto com os campos intent_type, interlocutor_name, apartment_number e resident_name.
        
        Exemplo:
        {
          "mensagem": "Entrega confirmada para Fulano do XXXX.",
          "dados": {
            "intent_type": "entrega",
            "interlocutor_name": "João",
            "apartment_number": "XXXX",
            "resident_name": "Fulano"
          }
        }
        O campo message é sempre obrigatório, pois você sempre deve ter uma resposta, mesmo que seja uma pergunta para 
        quem está sendo atendido. Se não puder identificar os campos de dados, retorne as chaves com valores vazios. 
        Não use floreios, não repita informações já ditas, apenas informe ou confirme a ação.
        O campo intent_type dentro de dados deve ser preenchido com entrega, visita ou desconhecido, conforme o que 
        você identificar.
        
        Alguns exemplos de campos faltando e possíveis respostas:
        {
          "mensagem": "Informe o que deseja",
          "dados": {
            "intent_type": "",
            "interlocutor_name": "",
            "apartment_number": "",
            "resident_name": ""
          }
        }
        
        {
          "mensagem": "Por favor, me informe o nome do visitante, o apartamento e o nome do morador que autorizou",
          "dados": {
            "intent_type": "visita",
            "interlocutor_name": "",
            "apartment_number": "",
            "resident_name": ""
          }
        } 
        
        {
          "mensagem": "Por favor, me informe o seu nome, o apartamento e o nome do morador que vai receber a entrega",
          "dados": {
            "intent_type": "entrega",
            "interlocutor_name": "",
            "apartment_number": "",
            "resident_name": ""
          }
        }                 
                
        {
          "mensagem": "Por favor, me informe o nome do morador.",
          "dados": {
            "intent_type": "entrega",
            "interlocutor_name": "Carlos",
            "apartment_number": "301",
            "resident_name": ""
          }
        }
        
        {
          "mensagem": "Por favor, me informe sua intenção",
          "dados": {
            "intent_type": "",
            "interlocutor_name": "Carlos",
            "apartment_number": "301",
            "resident_name": "Fulano"
          }
        }
        
        {
          "mensagem": "Podes informar o seu nome?",
          "dados": {
            "intent_type": "visita",
            "interlocutor_name": "",
            "apartment_number": "301",
            "resident_name": "Fulano"
          }
        }      
        
        {
          "mensagem": "Podes informar o número do apartamento?",
          "dados": {
            "intent_type": "visita",
            "interlocutor_name": "Cicrano",
            "apartment_number": "",
            "resident_name": "Fulano"
          }
        }              
        
        
        """,
        agent=agent
    )


def conversation_extractor_name_task(user_message: str, conversation_history: str, intent: dict) -> Task:
    agent = identificator_person_agent()
    print(conversation_history)
    return Task(
        description=f"""
        Você é o concierge inicial virtual do condomínio em uma conversa com alguém no portão do condomínio.

        Histórico da conversa:
        {conversation_history}

        Mensagem nova:
        "{user_message}"

        Intenção acumulada até agora:
        {intent}

        Seu trabalho:
        - Verificar se o campo interlocutor_name está preenchido:
          • interlocutor_name
        - Se não souber o nome da pessoa, pergunte
        - Se estiver completo, com um nome legível e aceitável, apenas confirme.

        Regras:
        - NÃO repita o que já foi dito.
        - NÃO reinicie a conversa.
        - Não peça dados que já estão no identificados.
        """,
        expected_output=""""
        Responda em formato JSON onde o campo interlocutor_name deve estar preenchido com o nome do visitante. 
        O campo de intent_type também deve estar preenchido pois já perguntamos isso ao interlocutor. Os outros
        campos (apartment_number e resident_name) dentro de dados serão preenchidos pelos outros concierges. 
        Exemplo de mensagem quando ainda não foi identificado o nome:
        {
          "mensagem": "Por favor, me informe o seu nome",
          "dados": {
            "intent_type": "",
            "interlocutor_name": "",
            "apartment_number": "",
            "resident_name": ""
          }
        }
        
        Exemplo de mensagem quando foi identificado o nome:
        {
          "mensagem": "Obrigado Fulano, aguarde um instante",
          "dados": {
            "intent_type": "",
            "interlocutor_name": "Fulano",
            "apartment_number": "",
            "resident_name": ""
          }
        }
        """,
        agent=agent
    )


def conversation_extractor_intent_task(user_message: str, conversation_history: str, intent: dict) -> Task:
    agent = identificator_intent_agent()
    print(conversation_history)
    return Task(
        description=f"""
        Você é o concierge virtual primário do condomínio em uma conversa com alguém no portão do condomínio.

        Histórico da conversa:
        {conversation_history}

        Mensagem nova:
        "{user_message}"

        Intenção acumulada até agora:
        {intent}

        Seu trabalho:
        - Verificar se o campo intent_type está preenchido:
          • intent_type
        - Se não souber a intenção da pessoa, pergunte.
        - Se estiver completo, com a intenção identificada entre entrega e visita, apenas confirme.

        Regras:
        - NÃO repita o que já foi dito.
        - NÃO reinicie a conversa.
        - Não peça dados que já estão identificados.
        """,
        expected_output=""""
        Responda em formato JSON onde o campo intent_type deve estar preenchido com a intenção do visitante. Os outros
        campos dentro de dados serão preenchidos pelos outros concierges.
        Mesmo que o usuário diga outras informações, ignore informações como o apartamento e o nome do morador, 
        apenas retorne a intenção (intent_type).

        Exemplo de mensagem quando ainda não foi identificadoa a intenção:
        {
          "mensagem": "Por favor, me informe sua intenção, se visita ou entrega",
          "dados": {
            "intent_type": "",
            "interlocutor_name": "Fulano",
            "apartment_number": "",
            "resident_name": ""
          }
        }

        Exemplo de mensagem quando foi identificada a intenção:
        {
          "mensagem": "Obrigado Fulano, aguarde um instante",
          "dados": {
            "intent_type": "visita",
            "interlocutor_name": "Fulano",
            "apartment_number": "",
            "resident_name": ""
          }
        }
        """,
        agent=agent
    )


def conversation_extractor_resident_apartment_task(user_message: str, conversation_history: str, intent: dict) -> Task:
    agent = identificator_resident_apartment_agent()
    print(conversation_history)
    return Task(
        description=f"""
        Você é o concierge virtual terciário em uma conversa com alguém no portão do condomínio.

        Histórico da conversa:
        {conversation_history}

        Mensagem nova:
        "{user_message}"

        Intenção acumulada até agora:
        {intent}

        Seu trabalho:
        - Verificar se os campos apartment_number e resident_name estão preenchidos:
          • apartment_number
          • resident_name
        - Se não souber o apartamento ou o morador, pergunte
        - Se estiver completo, com o apartamento e o nome do morador, apenas confirme.

        Regras:
        - NÃO repita o que já foi dito.
        - NÃO reinicie a conversa.
        - Não peça dados que já estão no identificados.
        """,
        expected_output=""""
        Responda em formato JSON onde os campos apartment_number e resident_name devem estar preenchidos com o
        número do apartamento e nome do morador. Os outros campos serão preenchidos pelos outros concierges.

        Exemplo de mensagem quando ainda não foi identificado o apartamento ou o morador:
        {
          "mensagem": "Por favor, me informe sua para qual apartamento e o nome do morador",
          "dados": {
            "intent_type": "visita",
            "interlocutor_name": "Fulano",
            "apartment_number": "",
            "resident_name": ""
          }
        }

        Exemplo de mensagem quando foi identificado o apartamento e o morador:
        {
          "mensagem": "Obrigado Fulano, aguarde um instante",
          "dados": {
            "intent_type": "visita",
            "interlocutor_name": "Fulano",
            "apartment_number": "501",
            "resident_name": "Cicrano"
          }
        }
        """,
        agent=agent
    )


ai/tools.py:
from rapidfuzz import fuzz
import json
from pathlib import Path
from typing import Dict
from ai.models.intent import IntentData
from crewai.tools import tool

VALID_APT_PATH = Path("data/apartamentos.json")

@tool("SendMessageTool")
def identify_user_intent(message: str) -> str:
    """
    Extrai intenção do usuário com base na mensagem utilizando um modelo LLM.
    Retorna um JSON no formato esperado por UserIntent.
    """
    return message


# @tool("ValidarIntentComFuzzyTool")
def validar_intent_com_fuzzy(intent: Dict) -> Dict:
    """
    Verifica se a combinação apartment_number e resident_name da intent
    corresponde (mesmo que parcialmente) a um morador real.

    Retorna um dicionário:
    {
      "status": "válido" ou "inválido",
      "match_name": "Fulano de Tal",
      "voip_number": "1003031"
    }
    """
    try:
        apt = intent.get("apartment_number", "").strip().lower()  # 'apartment_number
        resident_informado = intent.get("resident_name", "").strip().lower()

        # Regras de pré-processamento para melhorar matches
        # Normalizar nomes comuns
        residentes_alternativos = {
            "daner": ["daniel", "daner", "dani", "danir", "taner"],
            "renata": ["renata", "renato"]
        }
        
        # Log para debug
        print(f"Validando: Apt={apt}, Morador={resident_informado}")

        if not apt or not resident_informado:
            return {
                "status": "inválido",
                "reason": "Faltando número do apartamento ou nome do morador"
            }

        try:
            with open(VALID_APT_PATH, "r", encoding="utf-8") as f:
                apartamentos = json.load(f)
        except Exception as e:
            print(f"Erro ao ler arquivo de apartamentos: {e}")
            # Resposta de fallback para não interromper o fluxo
            return {
                "status": "erro",
                "message": f"Erro ao ler dados: {str(e)}"
            }

        # Primeiro tenta match exato de apartamento
        apt_matches = [a for a in apartamentos if a["apartment_number"] == apt]
        if not apt_matches:
            print(f"Nenhum apartamento {apt} encontrado")
            # Tentativa com fuzzy no apartamento (se for um erro de digitação)
            best_apt_score = 0
            best_apt_match = None
            for apartamento in apartamentos:
                apt_score = fuzz.ratio(apt, apartamento["apartment_number"])
                if apt_score > best_apt_score and apt_score >= 85:
                    best_apt_score = apt_score
                    best_apt_match = apartamento
            
            if best_apt_match:
                print(f"Encontrado apartamento próximo: {best_apt_match['apartment_number']} (score={best_apt_score})")
                apt_matches = [best_apt_match]

        # Se não encontrar o apartamento, retorna inválido
        if not apt_matches:
            return {
                "status": "inválido",
                "reason": f"Apartamento {apt} não encontrado"
            }
            
        # Procura melhor match de residente nos apartamentos encontrados
        best_match = None
        best_score = 0
        best_apt = None
        
        for apartamento in apt_matches:
            for residente in apartamento["residents"]:
                nome_residente = residente.strip().lower()
                
                # Pontuações para diferentes algoritmos de match
                scores = [
                    fuzz.ratio(resident_informado, nome_residente),  # Match completo
                    fuzz.partial_ratio(resident_informado, nome_residente),  # Match parcial
                    fuzz.token_sort_ratio(resident_informado, nome_residente)  # Ignora ordem das palavras
                ]
                
                # Verificar também nomes alternativos comuns
                for nome_base, alternativas in residentes_alternativos.items():
                    if nome_base in nome_residente:
                        for alt in alternativas:
                            if alt in resident_informado:
                                scores.append(95)  # Adiciona alta pontuação para alternativas conhecidas
                
                # Usar o melhor score entre os algoritmos
                score = max(scores)
                print(f"Comparando '{resident_informado}' com '{nome_residente}': score={score}")
                
                if score > best_score:
                    best_score = score
                    best_match = residente
                    best_apt = apartamento

        # Umbral mais baixo para melhorar a taxa de aceitação
        if best_score >= 75:
            print(f"Match encontrado: {best_match} no apt {best_apt['apartment_number']} (score={best_score})")
            return {
                "status": "válido",
                "match_name": best_match,
                "voip_number": best_apt["voip_number"],
                "match_score": best_score,
                "apartment_number": best_apt["apartment_number"]
            }
        else:
            print(f"Melhor match encontrado: {best_match} (score={best_score}), mas abaixo do umbral")

        return {
            "status": "inválido",
            "reason": "Morador não encontrado neste apartamento",
            "best_match": best_match,
            "best_score": best_score
        }
    except Exception as e:
        import logging
        logger = logging.getLogger(__name__)
        logger.error(f"Erro na validação fuzzy: {e}", exc_info=True)
        print(f"Erro na validação fuzzy: {e}")
        return {
            "status": "erro",
            "message": str(e)
        }


ai/__init__.py:


ai/agents.py:
import os
from crewai import Agent, LLM

# llm = LLM(
#     model="groq/meta-llama/llama-4-maverick-17b-128e-instruct",
#     temperature=0.7
# )

def identificator_person_agent() -> Agent:
    """
    Cria e retorna um agente responsável por conversar com o usuário,
    para identificar quem está falando.
    """
    return Agent(
        role="Primeiro Concierge Virtual do Condomínio",
        goal="Atender o usuário com empatia e eficiência, visando identificar quem esta falando",
        backstory="""
        Você é o primeiro concierge virtual treinado para conversar com pessoas no interfone da portaria de um condomínio.

        Seu trabalho é:
        - Solicitar a identificação da pessoa que está falando
        - Coletar :
          - Quem está no portão (interlocutor_name) (obrigatório)
          - Solicitar que a pessoa identifique-se para que o atendimento continue
          - Analise se o nome informado realmente é um nome aceitável, o usuário pode informar coisas como:
            - "Olá, tudo bem?" ou ainda
            - "Boa tarde", isso não é um nome válido
          - Você precisa interagir até que o nome seja obtido
        - Sua função não é identificar outras informações, apenas obter o nome e a identificação de quem está falando.
        """,
        verbose=False,
        allow_delegation=False,
        # llm=llm,
    )

def identificator_intent_agent() -> Agent:
    """
    Cria e retorna um agente responsável por conversar com o usuário,
    para identificar qual a intenção do usuário.
    """
    return Agent(
        role="Segundo Concierge Virtual do Condomínio",
        goal="Atender o usuário com empatia e eficiência, visando identificar qual a intenção do usuário",
        backstory="""
        Você é o segundo concierge virtual treinado para conversar com pessoas no interfone da portaria de um condomínio.

        Seu trabalho é:
        - Solicitar a qual a intenção da pessoa que está falando
        - Coletar :
          - Qual a intenção da pessoa (intent_type)
          - Por hora, aceitamos apenas duas intenções, entrega ou visita. Precisamos identificar se a intenção 
          é uma dessas duas.
        - Sua função não é identificar outras informações, apenas obter a intenção de quem está falando.
        """,
        verbose=False,
        allow_delegation=False,
        # llm=llm,
    )

def identificator_resident_apartment_agent() -> Agent:
    """
    Cria e retorna um agente responsável por conversar com o usuário,
    para identificar quam o nome do morador e apartamento.
    """
    return Agent(
        role="Terceiro Concierge Virtual do Condomínio",
        goal="Atender o usuário com empatia e eficiência, visando identificar qual a apartamento do morador e o nome "
             "do morador",
        backstory="""
        Você é o terceiro concierge virtual treinado para conversar com pessoas no interfone da portaria de um condomínio.

        Seu trabalho é:
        - Solicitar a qual a nome do morador e qual o apartamento
        - Coletar :
          - Qual o nome do morador (resident_name)
          - Qual o apartamento (apartment_number)
        - Sua função não é identificar outras informações, apenas obter o nome do morador e o apartamento.
        """,
        verbose=False,
        allow_delegation=False,
        # llm=llm,
    )

def create_conversation_coordinator_agent() -> Agent:
    """
    Cria e retorna um agente responsável por conversar com o usuário,
    entender sua intenção e coletar todas as informações necessárias.
    """
    return Agent(
        role="Concierge Virtual do Condomínio",
        goal="Atender o usuário com empatia e eficiência, entendendo e completando sua solicitação",
        backstory="""
        Você é um concierge virtual treinado que conversa com pessoas no interfone da portaria de um condomínio.

        Seu trabalho é:
        - Entender se a pessoa deseja fazer uma entrega ou visita (obrigatório)
        - Coletar todas as informações necessárias:
          • Quem está no portão (interlocutor_name) (obrigatório)
          • Número do apartamento de destino (apartment_number) (obrigatório)
          • Nome do morador (resident_name) (obrigatório)
        - Confirmar quando tudo estiver preenchido
        - Jamais perguntar algo que o usuário já informou
        - Nunca delegar sua responsabilidade de entender a solicitação
        - Nunca confunda entrega com visita, se o morador diz que vai ou deseja ir em um apartamento, provável visita

        Quando tiver todas as informações, apenas finalize a conversa com uma resposta simpática e diga que irá notificar o morador.
        """,
        verbose=False,
        allow_delegation=False,
        # llm=llm,
    )


def create_conversation_monitor_agent() -> Agent:
    """
    Cria e retorna um agente responsável por conversar com o usuário,
    entender sua intenção e coletar todas as informações necessárias.
    """
    return Agent(
        role="Gerente do Concierge Virtual do Condomínio",
        goal="Supervisionar o concierge virtual e garantir que ele vai extrair todos os dados que precisamos do usuário",
        backstory="""
        Você é um gerente senior e sua função é garantir que os etendentes da portaria remota (concierge virtual) 
        extraiam as informações corretas do usuário. Há relatos de que os atendentes estão chamando o morador, sem saber 
        qual a intenção ou sequer perguntar o nome de quem está falando ou se deseja uma visita ou entrega. 

        Seu trabalho é:
        - Identificar primeiro a pessoa que está falando com o concierge virtual e qual sua intenção
        - Somente pergunte o nome do morador e o apartamento se já houver identificado a interação e o nome do vistnate ou entregador
        - Fiscalizar o trabalho do concierge virtual
        - Garantir que ele vau coletar todas as informações necessárias:
          • Identificar a intenção do usuário (intent_type) obrigatório: se é visita ou entrega
          • Quem está no portão (interlocutor_name) (obrigatório)
          • Número do apartamento de destino (apartment_number) (obrigatório)
          • Nome do morador (resident_name) (obrigatório)
        - Nunca finalize a conversa se qualquer campo estiver vazio.
        - Se o nome do morador estiver vazio, você deve perguntar isso.

        Fiscalize as respostas antes de serem enviadas ao usuário garantindo tudo que foi solicitado.
        """,
        verbose=False,
        allow_delegation=False,
        # llm=llm,
    )


ai/crew.py:
from crewai import Crew
import time

from ai.state_manager import get_user_state, update_user_state, clear_user_state
from ai.tasks import create_conversation_coordinator_task, conversation_extractor_name_task, \
    conversation_extractor_intent_task, conversation_extractor_resident_apartment_task
from ai.tools import validar_intent_com_fuzzy
from ai.utils.intent_extractor import extract_intent_from_response
from ai.models.intent import IntentData
import json
from utils.call_logger import CallLoggerManager

def process_user_message_with_coordinator(id: str, message: str) -> dict:
    # Obter logger para esta chamada
    call_logger = CallLoggerManager.get_logger(id)
    
    # Marcar início do processamento pela IA
    call_logger.log_ai_processing_start(message)
    total_start_time = time.time()
    
    state = get_user_state(id)
    history = "\n".join(state["history"])
    partial_intent = state["intent"]
    dados_estruturados = None

    # A partir do nome obtido, verifica a intenção do usuário
    if not state["intent"] or state["intent"]["intent_type"] == "":
        call_logger.log_event("INTENT_EXTRACTION_START", {
            "stage": "intent_type",
            "current_intent": str(partial_intent)
        })
        
        intent_start_time = time.time()
        task = conversation_extractor_intent_task(
            user_message=message,
            conversation_history=history,
            intent=partial_intent
        )
        crew = Crew(tasks=[task], verbose=True)
        result = str(crew.kickoff())
        intent_duration = (time.time() - intent_start_time) * 1000
        
        try:
            dados_estruturados = extract_intent_from_response(result)
            update_user_state(id, intent=dados_estruturados.get("dados"),
                              message=f"Usuário: {message}\nResposta LLM: {dados_estruturados.get('mensagem')}")
            
            call_logger.log_event("INTENT_EXTRACTION_COMPLETE", {
                "stage": "intent_type",
                "result": dados_estruturados["dados"]["intent_type"],
                "duration_ms": round(intent_duration, 2)
            })
        except Exception as e:
            update_user_state(id, message=f"Usuário: {message}")
            call_logger.log_error("INTENT_EXTRACTION_ERROR", 
                                str(e), 
                                {"stage": "intent_type"})
            
            # Criar um dados_estruturados default para evitar erros
            dados_estruturados = {
                "mensagem": "Desculpe, não consegui entender. Poderia repetir?",
                "dados": {"intent_type": "", "interlocutor_name": "", "apartment_number": "", "resident_name": ""},
                "valid_for_action": False
            }

        if dados_estruturados["dados"]["intent_type"] == "" or dados_estruturados["dados"]["intent_type"] == "desconhecido":
            ai_duration = (time.time() - total_start_time) * 1000
            call_logger.log_ai_processing_complete(dados_estruturados, ai_duration)
            return dados_estruturados

    state = get_user_state(id)
    history = "\n".join(state["history"])
    partial_intent = state["intent"]

    # Verifica se o nome foi obtido, caso contrário interage com o usuário para extrair essa informação
    if state["intent"] and state["intent"]["interlocutor_name"] == "":
        call_logger.log_event("INTENT_EXTRACTION_START", {
            "stage": "interlocutor_name",
            "current_intent": str(partial_intent)
        })
        
        name_start_time = time.time()
        task = conversation_extractor_name_task(
            user_message=message,
            conversation_history=history,
            intent=partial_intent
        )
        crew = Crew(tasks=[task], verbose=True)
        result = str(crew.kickoff())
        name_duration = (time.time() - name_start_time) * 1000
        
        try:
            dados_estruturados = extract_intent_from_response(result)
            update_user_state(id, intent=dados_estruturados.get("dados"),
                              message=f"Usuário: {message}\nResposta LLM: {dados_estruturados.get('mensagem')}")
            
            call_logger.log_event("INTENT_EXTRACTION_COMPLETE", {
                "stage": "interlocutor_name",
                "result": dados_estruturados["dados"]["interlocutor_name"],
                "duration_ms": round(name_duration, 2)
            })
        except Exception as e:
            update_user_state(id, message=f"Usuário: {message}")
            call_logger.log_error("INTENT_EXTRACTION_ERROR", 
                                str(e), 
                                {"stage": "interlocutor_name"})
            
            # Criar um dados_estruturados default
            dados_estruturados = {
                "mensagem": "Desculpe, não consegui entender seu nome. Poderia repetir?",
                "dados": partial_intent,
                "valid_for_action": False
            }

        if dados_estruturados["dados"]["interlocutor_name"] == "":
            ai_duration = (time.time() - total_start_time) * 1000
            call_logger.log_ai_processing_complete(dados_estruturados, ai_duration)
            return dados_estruturados

    state = get_user_state(id)
    history = "\n".join(state["history"])
    partial_intent = state["intent"]

    if state["intent"] and state["intent"]["apartment_number"] == "" or state["intent"]["resident_name"] == "":
        call_logger.log_event("INTENT_EXTRACTION_START", {
            "stage": "apartment_and_resident",
            "current_intent": str(partial_intent)
        })
        
        apt_start_time = time.time()
        task = conversation_extractor_resident_apartment_task(
            user_message=message,
            conversation_history=history,
            intent=partial_intent
        )
        crew = Crew(tasks=[task], verbose=True)
        result = str(crew.kickoff())
        apt_duration = (time.time() - apt_start_time) * 1000
        
        try:
            dados_estruturados = extract_intent_from_response(result)
            update_user_state(id, intent=dados_estruturados.get("dados"),
                              message=f"Usuário: {message}\nResposta LLM: {dados_estruturados.get('mensagem')}")
            
            call_logger.log_event("INTENT_EXTRACTION_COMPLETE", {
                "stage": "apartment_and_resident",
                "result": f"apt: {dados_estruturados['dados']['apartment_number']}, resident: {dados_estruturados['dados']['resident_name']}",
                "duration_ms": round(apt_duration, 2)
            })
        except Exception as e:
            update_user_state(id, message=f"Usuário: {message}")
            call_logger.log_error("INTENT_EXTRACTION_ERROR", 
                                str(e), 
                                {"stage": "apartment_and_resident"})
            
            # Criar um dados_estruturados default
            dados_estruturados = {
                "mensagem": "Desculpe, não consegui entender as informações do apartamento/morador. Poderia repetir?",
                "dados": partial_intent,
                "valid_for_action": False
            }

        if dados_estruturados["dados"]["apartment_number"] == "" or dados_estruturados["dados"]["resident_name"] == "":
            ai_duration = (time.time() - total_start_time) * 1000
            call_logger.log_ai_processing_complete(dados_estruturados, ai_duration)
            return dados_estruturados

        # É preciso retornar os dados para poder deixar a chamada em waiting enquanto processamos a intenção do usuário
        state = get_user_state(id)
        partial_intent = state["intent"]
        
        # Medição de tempo para validação fuzzy
        fuzzy_start_time = time.time()
        call_logger.log_event("FUZZY_VALIDATION_START", {"intent": str(partial_intent)})
        resultado = validar_intent_com_fuzzy(partial_intent)
        fuzzy_duration = (time.time() - fuzzy_start_time) * 1000
        
        call_logger.log_event("FUZZY_VALIDATION_COMPLETE", {
            "status": resultado["status"],
            "reason": resultado.get("reason", ""),
            "match_name": resultado.get("match_name", ""),
            "voip_number": resultado.get("voip_number", ""),
            "duration_ms": round(fuzzy_duration, 2)
        })

        if resultado["status"] == "inválido":
            # Zera apt/resident
            partial_intent["apartment_number"] = ""
            partial_intent["resident_name"] = ""

            invalid_response = {
                "mensagem": "Não encontrei esse apartamento/morador. Poderia repetir?",
                "dados": partial_intent,
                "valid_for_action": False
            }
            
            ai_duration = (time.time() - total_start_time) * 1000
            call_logger.log_ai_processing_complete(invalid_response, ai_duration)
            return invalid_response

        # Adicionar valid_for_action e outros metadados
        dados_estruturados["valid_for_action"] = True

    # Tempo total de processamento
    ai_duration = (time.time() - total_start_time) * 1000
    call_logger.log_ai_processing_complete(dados_estruturados, ai_duration)
    return dados_estruturados









    # # Cria a Task com histórico e dados parciais
    # task = create_conversation_coordinator_task(
    #     user_message=message,
    #     conversation_history=history,
    #     intent=partial_intent
    # )
    #
    # crew = Crew(tasks=[task], verbose=True)
    # result = str(crew.kickoff())
    #
    # # Tenta extrair novos dados (você pode usar OpenAI function calling ou Regex/JSON)
    # try:
    #     dados_estruturados = extract_intent_from_response(result)
    #     update_user_state(id, intent=dados_estruturados.get("dados"), message=f"Usuário: {message}\nResposta LLM: {dados_estruturados.get('mensagem')}")
    # except Exception:
    #     update_user_state(id, message=f"Usuário: {dados_estruturados.get('mensagem')}")
    #
    # return dados_estruturados


ai/main.py:
from ai.crew import process_user_message_with_coordinator



def process_message(content: str, id: str = "anon") -> dict:
    try:
        return process_user_message_with_coordinator(id=id, message=content)
            
    except Exception as e:
        print(f"Erro ao chamar CrewAI: {e}")
        return "Estou enfrentando alguns problemas técnicos. Por favor, tente novamente mais tarde."


ai/utils/__init__.py:


ai/utils/intent_extractor.py:
import json

from guardrails import Guard
from typing import Dict

from humanfriendly.terminal import message

from ai.models.intent import IntentData, IntentType, FullIntentResponse

# =======================
# 🛡️ RAILS SCHEMA (sem valid_for_action)
# =======================

INTENT_SCHEMA = """
<rail version="0.1">
<output>
  <object>
    <string name="mensagem" description="Mensagem a ser enviada ao usuário, clara e objetiva"/>
    <object name="dados">
      <string name="intent_type" format="enum" enum-values="visita,entrega,desconhecido"/>
      <string name="interlocutor_name" on-fail-soft="noop"/>
      <string name="apartment_number" on-fail-soft="noop"/>
      <string name="resident_name" on-fail-soft="noop"/>
    </object>
  </object>
</output>
</rail>
"""

guard = Guard.for_rail_string(INTENT_SCHEMA)

# =======================
# 🔍 EXTRACTOR
# =======================

def extract_intent_from_response(response: str) -> Dict:
    """
    Valida e extrai os dados de intenção usando Guardrails e Pydantic.
    Calcula valid_for_action com base nos campos preenchidos.
    """
    try:
        validated  = guard.parse(response)
        raw = validated.validated_output

        if not all(k in raw for k in ["mensagem", "dados"]):
            raise ValueError("Resposta fora do padrão esperado")

        if raw["dados"].get("intent_type") == "":
            raw["dados"]["intent_type"] = IntentType.DESCONHECIDO

        intent = IntentData(**raw["dados"])

        # Cálculo de completude
        campos_preenchidos = all([
            intent.intent_type != IntentType.DESCONHECIDO,
            intent.interlocutor_name.strip(),
            intent.apartment_number.strip(),
            intent.resident_name.strip()
        ])

        call_status = "USER_TURN"
        if campos_preenchidos:
            call_status = "WAITING"

        result = FullIntentResponse(
            mensagem=raw["mensagem"],
            dados=intent,
            valid_for_action=campos_preenchidos,
            set_call_status=call_status
        )

        return result.model_dump()
    except Exception as e:
        # Log do erro para diagnóstico (idealmente seria para um sistema de log)
        print(f"Erro ao extrair intenção: {str(e)}")
        
        # Tenta entender se a mensagem já é um JSON
        if isinstance(response, str) and response.strip().startswith('{') and response.strip().endswith('}'):
            try:
                # Tenta extrair JSON diretamente da resposta
                message_of_non_understanding = json.loads(response)
                if "mensagem" in message_of_non_understanding and "dados" in message_of_non_understanding:
                    # Garante que todos os campos necessários estejam presentes
                    if "intent_type" not in message_of_non_understanding["dados"]:
                        message_of_non_understanding["dados"]["intent_type"] = "desconhecido"
                    if "interlocutor_name" not in message_of_non_understanding["dados"]:
                        message_of_non_understanding["dados"]["interlocutor_name"] = ""
                    if "apartment_number" not in message_of_non_understanding["dados"]:
                        message_of_non_understanding["dados"]["apartment_number"] = ""
                    if "resident_name" not in message_of_non_understanding["dados"]:
                        message_of_non_understanding["dados"]["resident_name"] = ""
                    
                    message_of_non_understanding["valid_for_action"] = False
                    message_of_non_understanding["set_call_status"] = "USER_TURN"
                    return message_of_non_understanding
            except:
                # Se falhar na análise JSON, continua para a resposta padrão
                pass
                
        # Resposta padrão para casos de erro
        message_of_non_understanding = {
            "mensagem": "Desculpe, não consegui entender. Por favor, informe novamente o que deseja.",
            "dados": {
                "intent_type": "desconhecido",
                "interlocutor_name": "",
                "apartment_number": "",
                "resident_name": ""
            },
            "valid_for_action": False,
            "set_call_status": "USER_TURN"
        }
        
        return message_of_non_understanding


ai/models/intent.py:
from enum import Enum
from typing import Optional

from pydantic import BaseModel, Field

# =======================
# 🧠 MODELO DE INTENÇÃO
# =======================

class IntentType(str, Enum):
    VISITA = "visita"
    ENTREGA = "entrega"
    DESCONHECIDO = "desconhecido"

class IntentData(BaseModel):
    intent_type: IntentType = Field(..., description="Tipo de intenção identificada")
    interlocutor_name: str = Field("", description="Nome da pessoa no portão")
    apartment_number: str = Field("", description="Número do apartamento de destino")
    resident_name: str = Field("", description="Nome do morador/destinatário")

class FullIntentResponse(BaseModel):
    mensagem: str
    dados: IntentData
    valid_for_action: bool
    set_call_status: Optional[str] = "USER_TURN"


ai/models/__init__.py:


data/apartamentos.json:
[
  {
    "apartment_number": "101",
    "residents": ["Ana Beatriz Silva", "João Pedro Silva"],
    "voip_number": "sip:101@condominio.local"
  },
  {
    "apartment_number": "102",
    "residents": ["Carlos Oliveira"],
    "voip_number": "sip:102@condominio.local"
  },
  {
    "apartment_number": "201",
    "residents": ["Fernanda Souza", "Tiago Souza"],
    "voip_number": "sip:201@condominio.local"
  },
  {
    "apartment_number": "202",
    "residents": ["Mariana Costa"],
    "voip_number": "sip:202@condominio.local"
  },
  {
    "apartment_number": "301",
    "residents": ["Rodrigo Lima", "Letícia Lima"],
    "voip_number": "sip:301@condominio.local"
  },
  {
    "apartment_number": "302",
    "residents": ["Lucas Martins"],
    "voip_number": "sip:302@condominio.local"
  },
  {
    "apartment_number": "401",
    "residents": ["Paula Fernandes"],
    "voip_number": "sip:401@condominio.local"
  },
  {
    "apartment_number": "402",
    "residents": ["Eduardo Almeida", "Bruna Almeida"],
    "voip_number": "sip:402@condominio.local"
  },
  {
    "apartment_number": "501",
    "residents": ["Renata Oliveira", "Daner dos Reis"],
    "voip_number": "1003030"
  },
  {
    "apartment_number": "502",
    "residents": ["Rogério Paranhos"],
    "voip_number": "1003025"
  }
]


data/ramais_config.json:
{
  "ramais": [
    {
      "id": 10,
      "ramal_ia": "2018519",
      "ramal_retorno": "2018520",
      "ip_servidor": "18.231.197.181",
      "porta_ia": 8101,
      "porta_retorno": 8201,
      "condominio_id": 15
    }
  ]
}

services/__init__.py:


services/amqp_service.py:

import pika
import json
import logging
import time

def enviar_msg_autorizacao_morador(payload):
    """
    Envia uma mensagem para a fila de autorização via AMQP.
    
    Esta função é chamada quando um morador autoriza ou nega a entrada
    de um visitante e precisa comunicar essa decisão ao sistema de controle
    de acesso físico.
    """
    try:
        connection = pika.BlockingConnection(pika.ConnectionParameters('localhost'))
        channel = connection.channel()

        channel.queue_declare(queue='fila_autorizacao')

        # Adicionar timestamp para rastreabilidade
        payload['timestamp'] = int(time.time())
        
        channel.basic_publish(
            exchange='',
            routing_key='fila_autorizacao',
            body=json.dumps(payload)
        )

        logging.info(f"Mensagem AMQP enviada para fila_autorizacao: {payload}")
        connection.close()
        return True
    except Exception as e:
        logging.error(f"Erro ao enviar mensagem AMQP: {e}")
        return False

